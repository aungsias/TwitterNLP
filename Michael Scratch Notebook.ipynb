{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Preprocessing Tweets](#preprocessing-tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/michaelromanski/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/michaelromanski/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/michaelromanski/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, silhouette_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unprocessed_tweet</th>\n",
       "      <th>product</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   unprocessed_tweet             product  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...              iPhone   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...  iPad or iPhone App   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...                iPad   \n",
       "\n",
       "            emotion  \n",
       "0  Negative emotion  \n",
       "1  Positive emotion  \n",
       "2  Positive emotion  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/tweets.csv\", encoding='ISO-8859-1')\n",
    "\n",
    "df.columns = [\"unprocessed_tweet\", \"product\", \"emotion\"]\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       .@wesley83 I have a 3G iPhone. After 3 hrs twe...\n",
       "1       @jessedee Know about @fludapp ? Awesome iPad/i...\n",
       "2       @swonderlin Can not wait for #iPad 2 also. The...\n",
       "3       @sxsw I hope this year's festival isn't as cra...\n",
       "4       @sxtxstate great stuff on Fri #SXSW: Marissa M...\n",
       "                              ...                        \n",
       "9088                        Ipad everywhere. #SXSW {link}\n",
       "9089    Wave, buzz... RT @mention We interrupt your re...\n",
       "9090    Google's Zeiger, a physician never reported po...\n",
       "9091    Some Verizon iPhone customers complained their...\n",
       "9092    Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT @...\n",
       "Name: unprocessed_tweet, Length: 9093, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = df[\"unprocessed_tweet\"]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stop_words = [i.replace(\"'\", '') for i in stop_words]\n",
    "\n",
    "stop_words[-5:]\n",
    "top_words = ['sxsw', 'mention', 'link', 'rt']\n",
    "stop_words = stop_words + top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       wesley i have a g iphone after  hrs tweeting a...\n",
       "1       jessedee know about fludapp  awesome ipadiphon...\n",
       "2       swonderlin can not wait for ipad  also they sh...\n",
       "3       sxsw i hope this years festival isnt as crashy...\n",
       "4       sxtxstate great stuff on fri sxsw marissa maye...\n",
       "                              ...                        \n",
       "9088                            ipad everywhere sxsw link\n",
       "9089    wave buzz rt mention we interrupt your regular...\n",
       "9090    googles zeiger a physician never reported pote...\n",
       "9091    some verizon iphone customers complained their...\n",
       "9092    rt mention google tests checkin offers at sxsw...\n",
       "Name: unprocessed_tweet, Length: 9093, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = \"[^a-zA-Z\\s]\"\n",
    "\n",
    "text = text.str.replace(ex, \"\", regex=True)\n",
    "text = text.str.lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [wesley, i, have, a, g, iphone, after, hrs, tw...\n",
       "1       [jessedee, know, about, fludapp, awesome, ipad...\n",
       "2       [swonderlin, can, not, wait, for, ipad, also, ...\n",
       "3       [sxsw, i, hope, this, years, festival, isnt, a...\n",
       "4       [sxtxstate, great, stuff, on, fri, sxsw, maris...\n",
       "                              ...                        \n",
       "9088                       [ipad, everywhere, sxsw, link]\n",
       "9089    [wave, buzz, rt, mention, we, interrupt, your,...\n",
       "9090    [googles, zeiger, a, physician, never, reporte...\n",
       "9091    [some, verizon, iphone, customers, complained,...\n",
       "9092    [rt, mention, google, tests, checkin, offers, ...\n",
       "Name: unprocessed_tweet, Length: 9093, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized_text = text.apply(lambda t: word_tokenize(str(t)))\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [wesley, iphone, hrs, tweeting, riseaustin, de...\n",
       "1       [jessedee, know, fludapp, awesome, ipadiphone,...\n",
       "2                    [swonderlin, wait, ipad, also, sale]\n",
       "3       [hope, years, festival, crashy, years, iphone,...\n",
       "4       [sxtxstate, great, stuff, fri, marissa, mayer,...\n",
       "                              ...                        \n",
       "9088                                   [ipad, everywhere]\n",
       "9089    [wave, buzz, interrupt, regularly, scheduled, ...\n",
       "9090    [googles, zeiger, physician, never, reported, ...\n",
       "9091    [verizon, iphone, customers, complained, time,...\n",
       "9092                     [google, tests, checkin, offers]\n",
       "Name: unprocessed_tweet, Length: 9093, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_text = tokenized_text.apply(lambda x: [word for word in x if word not in stop_words])\n",
    "filtered_text = filtered_text.apply(lambda x: [word for word in x if len(word) > 1])\n",
    "filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [(wesley, NN), (iphone, NN), (hrs, NN), (tweet...\n",
       "1       [(jessedee, NN), (know, VBP), (fludapp, VBZ), ...\n",
       "2       [(swonderlin, NN), (wait, NN), (ipad, NN), (al...\n",
       "3       [(hope, NN), (years, NNS), (festival, JJ), (cr...\n",
       "4       [(sxtxstate, NN), (great, JJ), (stuff, NN), (f...\n",
       "                              ...                        \n",
       "9088                       [(ipad, NN), (everywhere, RB)]\n",
       "9089    [(wave, NN), (buzz, NN), (interrupt, VBP), (re...\n",
       "9090    [(googles, NNS), (zeiger, RBR), (physician, JJ...\n",
       "9091    [(verizon, NN), (iphone, NN), (customers, NNS)...\n",
       "9092    [(google, NN), (tests, NNS), (checkin, VBP), (...\n",
       "Name: unprocessed_tweet, Length: 9093, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_text = filtered_text.apply(lambda x: pos_tag(x))\n",
    "tagged_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       wesley iphone hr tweet riseaustin dead need up...\n",
       "1       jessedee know fludapp awesome ipadiphone app l...\n",
       "2                          swonderlin wait ipad also sale\n",
       "3               hope year festival crashy year iphone app\n",
       "4       sxtxstate great stuff fri marissa mayer google...\n",
       "                              ...                        \n",
       "9088                                      ipad everywhere\n",
       "9089    wave buzz interrupt regularly schedule geek pr...\n",
       "9090    google zeiger physician never report potential...\n",
       "9091    verizon iphone customer complain time fell bac...\n",
       "9092                            google test checkin offer\n",
       "Name: unprocessed_tweet, Length: 9093, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "lemmatized_text = tagged_text.apply(\n",
    "    lambda x: [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in x]\n",
    ")\n",
    "\n",
    "lemmatized_str = lemmatized_text.apply(lambda x: ' '.join(x))\n",
    "\n",
    "lemmatized_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [wesley, iphone, hr, tweet, riseaustin, dead, ...\n",
       "1       [jessedee, know, fludapp, awesome, ipadiphone,...\n",
       "2                    [swonderlin, wait, ipad, also, sale]\n",
       "3       [hope, year, festival, crashy, year, iphone, app]\n",
       "4       [sxtxstate, great, stuff, fri, marissa, mayer,...\n",
       "                              ...                        \n",
       "9088                                   [ipad, everywhere]\n",
       "9089    [wave, buzz, interrupt, regularly, schedule, g...\n",
       "9090    [google, zeiger, physician, never, report, pot...\n",
       "9091    [verizon, iphone, customer, complain, time, fe...\n",
       "9092                       [google, test, checkin, offer]\n",
       "Name: unprocessed_tweet, Length: 9093, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unprocessed_tweet</th>\n",
       "      <th>product</th>\n",
       "      <th>emotion</th>\n",
       "      <th>processed_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>wesley iphone hr tweet riseaustin dead need up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>jessedee know fludapp awesome ipadiphone app l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>swonderlin wait ipad also sale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>hope year festival crashy year iphone app</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>sxtxstate great stuff fri marissa mayer google...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   unprocessed_tweet             product  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...              iPhone   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...  iPad or iPhone App   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...                iPad   \n",
       "3  @sxsw I hope this year's festival isn't as cra...  iPad or iPhone App   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...              Google   \n",
       "\n",
       "            emotion                                    processed_tweet  \n",
       "0  Negative emotion  wesley iphone hr tweet riseaustin dead need up...  \n",
       "1  Positive emotion  jessedee know fludapp awesome ipadiphone app l...  \n",
       "2  Positive emotion                     swonderlin wait ipad also sale  \n",
       "3  Negative emotion          hope year festival crashy year iphone app  \n",
       "4  Positive emotion  sxtxstate great stuff fri marissa mayer google...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"processed_tweet\"] = lemmatized_str\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iPad                               946\n",
       "Apple                              661\n",
       "iPad or iPhone App                 470\n",
       "Google                             430\n",
       "iPhone                             297\n",
       "Other Google product or service    293\n",
       "Android App                         81\n",
       "Android                             78\n",
       "Other Apple product or service      35\n",
       "Name: product, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['product'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/processed_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binary = df.copy()\n",
    "df_binary = df_binary[df_binary[\"emotion\"].isin([\"Negative emotion\", \"Positive emotion\"])]\n",
    "\n",
    "df_binary[\"emotion_encoded\"] = df_binary[\"emotion\"].replace(\"Negative emotion\", 0).replace(\"Positive emotion\", 1)\n",
    "df_binary.to_csv(\"data/processed_tweets_binary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unprocessed_tweet</th>\n",
       "      <th>product</th>\n",
       "      <th>emotion</th>\n",
       "      <th>processed_tweet</th>\n",
       "      <th>emotion_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>wesley iphone hr tweet riseaustin dead need up...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>jessedee know fludapp awesome ipadiphone app l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>swonderlin wait ipad also sale</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>hope year festival crashy year iphone app</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>sxtxstate great stuff fri marissa mayer google...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9077</th>\n",
       "      <td>@mention your PR guy just convinced me to swit...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>pr guy convince switch back iphone great cover...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9079</th>\n",
       "      <td>&amp;quot;papyrus...sort of like the ipad&amp;quot; - ...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>quotpapyrussort like ipadquot nice lol lavelle</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9080</th>\n",
       "      <td>Diller says Google TV &amp;quot;might be run over ...</td>\n",
       "      <td>Other Google product or service</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>diller say google tv quotmight run playstation...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9085</th>\n",
       "      <td>I've always used Camera+ for my iPhone b/c it ...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>ive always use camera iphone bc image stabiliz...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9088</th>\n",
       "      <td>Ipad everywhere. #SXSW {link}</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>ipad everywhere</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3548 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      unprocessed_tweet  \\\n",
       "0     .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1     @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2     @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3     @sxsw I hope this year's festival isn't as cra...   \n",
       "4     @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "...                                                 ...   \n",
       "9077  @mention your PR guy just convinced me to swit...   \n",
       "9079  &quot;papyrus...sort of like the ipad&quot; - ...   \n",
       "9080  Diller says Google TV &quot;might be run over ...   \n",
       "9085  I've always used Camera+ for my iPhone b/c it ...   \n",
       "9088                      Ipad everywhere. #SXSW {link}   \n",
       "\n",
       "                              product           emotion  \\\n",
       "0                              iPhone  Negative emotion   \n",
       "1                  iPad or iPhone App  Positive emotion   \n",
       "2                                iPad  Positive emotion   \n",
       "3                  iPad or iPhone App  Negative emotion   \n",
       "4                              Google  Positive emotion   \n",
       "...                               ...               ...   \n",
       "9077                           iPhone  Positive emotion   \n",
       "9079                             iPad  Positive emotion   \n",
       "9080  Other Google product or service  Negative emotion   \n",
       "9085               iPad or iPhone App  Positive emotion   \n",
       "9088                             iPad  Positive emotion   \n",
       "\n",
       "                                        processed_tweet  emotion_encoded  \n",
       "0     wesley iphone hr tweet riseaustin dead need up...                0  \n",
       "1     jessedee know fludapp awesome ipadiphone app l...                1  \n",
       "2                        swonderlin wait ipad also sale                1  \n",
       "3             hope year festival crashy year iphone app                0  \n",
       "4     sxtxstate great stuff fri marissa mayer google...                1  \n",
       "...                                                 ...              ...  \n",
       "9077  pr guy convince switch back iphone great cover...                1  \n",
       "9079     quotpapyrussort like ipadquot nice lol lavelle                1  \n",
       "9080  diller say google tv quotmight run playstation...                0  \n",
       "9085  ive always use camera iphone bc image stabiliz...                1  \n",
       "9088                                    ipad everywhere                1  \n",
       "\n",
       "[3548 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2978\n",
       "0     570\n",
       "Name: emotion_encoded, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_binary['emotion_encoded'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_binary['processed_tweet']\n",
    "y = df_binary['emotion_encoded']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_forest_pipe = Pipeline([\n",
    "    ('tf', TfidfVectorizer(min_df=.01, max_df=.9, stop_words=stop_words)),\n",
    "    ('rf', RandomForestClassifier(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8577464788732394"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_forest_pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred_forest = bin_forest_pipe.predict(X_test)\n",
    "forest_test_acc = accuracy_score(y_test, y_pred_forest)\n",
    "forest_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f8048418280>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cfm_forest = confusion_matrix(y_test, y_pred_forest)\n",
    "ConfusionMatrixDisplay(cfm_forest).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "685     1\n",
       "6393    1\n",
       "7851    0\n",
       "5813    0\n",
       "2944    1\n",
       "       ..\n",
       "2707    1\n",
       "2770    1\n",
       "6653    1\n",
       "2127    1\n",
       "5565    0\n",
       "Name: emotion_encoded, Length: 710, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.85211268, 0.84859155, 0.8556338 , 0.86419753, 0.85361552])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_score = cross_val_score(bin_forest_pipe, X_train, y_train, cv=5)\n",
    "forest_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_forest_pipe_grid = Pipeline([\n",
    "    ('tf', TfidfVectorizer(stop_words=stop_words)),\n",
    "    ('rf', RandomForestClassifier(random_state=42))\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_param_grid = {\n",
    "    'tf__min_df': [.01, .02, .03],\n",
    "    'tf__max_df': [.88, .90, .92],\n",
    "    'tf__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "    'rf__n_estimators': [86, 88, 90],\n",
    "    'rf__min_samples_split': [10, 20, 30],\n",
    "    'rf__class_weight': [None, 'balanced', 'balanced_subsample']\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_param_grid = {\n",
    "    'tf__min_df': [.01, .02, .03],\n",
    "    'tf__max_df': [.88, .90, .92],\n",
    "    'tf__ngram_range': [(1,1), (2,2), (3,3)],\n",
    "    'rf__n_estimators': [86, 88, 90],\n",
    "    'rf__min_samples_split': [10, 20, 30],\n",
    "    'rf__class_weight': [None, 'balanced', 'balanced_subsample']\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 729 candidates, totalling 3645 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
      "405 fits failed out of a total of 3645.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "405 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\", line 416, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\", line 370, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/joblib/memory.py\", line 353, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\", line 950, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 2126, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 1396, in fit_transform\n",
      "    X, self.stop_words_ = self._limit_features(\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 1248, in _limit_features\n",
      "    raise ValueError(\n",
      "ValueError: After pruning, no terms remain. Try a lower min_df or a higher max_df.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:976: UserWarning: One or more of the test scores are non-finite: [0.85201331 0.83826726 0.83897024 0.84179211 0.83826664 0.83967571\n",
      " 0.83403756 0.83967571        nan 0.85201331 0.83826726 0.83897024\n",
      " 0.84179211 0.83826664 0.83967571 0.83403756 0.83967571        nan\n",
      " 0.85201331 0.83826726 0.83897024 0.84179211 0.83826664 0.83967571\n",
      " 0.83403756 0.83967571        nan 0.85095511 0.83861937 0.83897024\n",
      " 0.84249634 0.83826664 0.83967571 0.83333271 0.83967571        nan\n",
      " 0.85095511 0.83861937 0.83897024 0.84249634 0.83826664 0.83967571\n",
      " 0.83333271 0.83967571        nan 0.85095511 0.83861937 0.83897024\n",
      " 0.84249634 0.83826664 0.83967571 0.83333271 0.83967571        nan\n",
      " 0.85060238 0.83826726 0.83897024 0.84284907 0.83826664 0.83967571\n",
      " 0.83333271 0.83967571        nan 0.85060238 0.83826726 0.83897024\n",
      " 0.84284907 0.83826664 0.83967571 0.83333271 0.83967571        nan\n",
      " 0.85060238 0.83826726 0.83897024 0.84284907 0.83826664 0.83967571\n",
      " 0.83333271 0.83967571        nan 0.84919082 0.83826726 0.83897024\n",
      " 0.84214298 0.83826664 0.83967571 0.83509514 0.83967571        nan\n",
      " 0.84919082 0.83826726 0.83897024 0.84214298 0.83826664 0.83967571\n",
      " 0.83509514 0.83967571        nan 0.84919082 0.83826726 0.83897024\n",
      " 0.84214298 0.83826664 0.83967571 0.83509514 0.83967571        nan\n",
      " 0.84954294 0.83826726 0.83897024 0.84073329 0.83826664 0.83967571\n",
      " 0.83509514 0.83967571        nan 0.84954294 0.83826726 0.83897024\n",
      " 0.84073329 0.83826664 0.83967571 0.83509514 0.83967571        nan\n",
      " 0.84954294 0.83826726 0.83897024 0.84073329 0.83826664 0.83967571\n",
      " 0.83509514 0.83967571        nan 0.84989505 0.83826726 0.83897024\n",
      " 0.84143813 0.83826664 0.83967571 0.83509514 0.83967571        nan\n",
      " 0.84989505 0.83826726 0.83897024 0.84143813 0.83826664 0.83967571\n",
      " 0.83509514 0.83967571        nan 0.84989505 0.83826726 0.83897024\n",
      " 0.84143813 0.83826664 0.83967571 0.83509514 0.83967571        nan\n",
      " 0.84601933 0.83826726 0.83897024 0.84038118 0.83826664 0.83967571\n",
      " 0.83298122 0.83967571        nan 0.84601933 0.83826726 0.83897024\n",
      " 0.84038118 0.83826664 0.83967571 0.83298122 0.83967571        nan\n",
      " 0.84601933 0.83826726 0.83897024 0.84038118 0.83826664 0.83967571\n",
      " 0.83298122 0.83967571        nan 0.84672417 0.83826726 0.83967571\n",
      " 0.84038118 0.83826664 0.83967571 0.83298122 0.83967571        nan\n",
      " 0.84672417 0.83826726 0.83967571 0.84038118 0.83826664 0.83967571\n",
      " 0.83298122 0.83967571        nan 0.84672417 0.83826726 0.83967571\n",
      " 0.84038118 0.83826664 0.83967571 0.83298122 0.83967571        nan\n",
      " 0.84637268 0.83826726 0.83897024 0.83967633 0.83826664 0.83967571\n",
      " 0.83438967 0.83967571        nan 0.84637268 0.83826726 0.83897024\n",
      " 0.83967633 0.83826664 0.83967571 0.83438967 0.83967571        nan\n",
      " 0.84637268 0.83826726 0.83897024 0.83967633 0.83826664 0.83967571\n",
      " 0.83438967 0.83967571        nan 0.83016432 0.44010793 0.23925342\n",
      " 0.75932509 0.35236294 0.31104094 0.70823397 0.29774946        nan\n",
      " 0.83016432 0.44010793 0.23925342 0.75932509 0.35236294 0.31104094\n",
      " 0.70823397 0.29774946        nan 0.83016432 0.44010793 0.23925342\n",
      " 0.75932509 0.35236294 0.31104094 0.70823397 0.29774946        nan\n",
      " 0.83051581 0.44010793 0.23925342 0.75932447 0.35236294 0.31104094\n",
      " 0.70788248 0.29774946        nan 0.83051581 0.44010793 0.23925342\n",
      " 0.75932447 0.35236294 0.31104094 0.70788248 0.29774946        nan\n",
      " 0.83051581 0.44010793 0.23925342 0.75932447 0.35236294 0.31104094\n",
      " 0.70788248 0.29774946        nan 0.83368731 0.44010793 0.23925342\n",
      " 0.75932509 0.35236294 0.31104094 0.70964366 0.29774946        nan\n",
      " 0.83368731 0.44010793 0.23925342 0.75932509 0.35236294 0.31104094\n",
      " 0.70964366 0.29774946        nan 0.83368731 0.44010793 0.23925342\n",
      " 0.75932509 0.35236294 0.31104094 0.70964366 0.29774946        nan\n",
      " 0.82311772 0.44116613 0.23925342 0.754747   0.35306841 0.31104094\n",
      " 0.70823521 0.29774946        nan 0.82311772 0.44116613 0.23925342\n",
      " 0.754747   0.35306841 0.31104094 0.70823521 0.29774946        nan\n",
      " 0.82311772 0.44116613 0.23925342 0.754747   0.35306841 0.31104094\n",
      " 0.70823521 0.29774946        nan 0.82382194 0.44116613 0.23925342\n",
      " 0.7558052  0.35377388 0.31104094 0.70823521 0.29774946        nan\n",
      " 0.82382194 0.44116613 0.23925342 0.7558052  0.35377388 0.31104094\n",
      " 0.70823521 0.29774946        nan 0.82382194 0.44116613 0.23925342\n",
      " 0.7558052  0.35377388 0.31104094 0.70823521 0.29774946        nan\n",
      " 0.82311772 0.44327881 0.23925342 0.75721489 0.35306841 0.31104094\n",
      " 0.70823459 0.29774946        nan 0.82311772 0.44327881 0.23925342\n",
      " 0.75721489 0.35306841 0.31104094 0.70823459 0.29774946        nan\n",
      " 0.82311772 0.44327881 0.23925342 0.75721489 0.35306841 0.31104094\n",
      " 0.70823459 0.29774946        nan 0.82135902 0.44327881 0.23925342\n",
      " 0.75368818 0.35377388 0.31104094 0.70894379 0.29774946        nan\n",
      " 0.82135902 0.44327881 0.23925342 0.75368818 0.35377388 0.31104094\n",
      " 0.70894379 0.29774946        nan 0.82135902 0.44327881 0.23925342\n",
      " 0.75368818 0.35377388 0.31104094 0.70894379 0.29774946        nan\n",
      " 0.82065417 0.44327881 0.23925342 0.75474514 0.35377388 0.31104094\n",
      " 0.71000137 0.29774946        nan 0.82065417 0.44327881 0.23925342\n",
      " 0.75474514 0.35377388 0.31104094 0.71000137 0.29774946        nan\n",
      " 0.82065417 0.44327881 0.23925342 0.75474514 0.35377388 0.31104094\n",
      " 0.71000137 0.29774946        nan 0.82241722 0.44327881 0.23925342\n",
      " 0.75263246 0.35377388 0.31104094 0.70894379 0.29774946        nan\n",
      " 0.82241722 0.44327881 0.23925342 0.75263246 0.35377388 0.31104094\n",
      " 0.70894379 0.29774946        nan 0.82241722 0.44327881 0.23925342\n",
      " 0.75263246 0.35377388 0.31104094 0.70894379 0.29774946        nan\n",
      " 0.82734369 0.4397552  0.23925342 0.75967906 0.35236294 0.1793508\n",
      " 0.7075285  0.29774946        nan 0.82734369 0.4397552  0.23925342\n",
      " 0.75967906 0.35236294 0.1793508  0.7075285  0.29774946        nan\n",
      " 0.82734369 0.4397552  0.23925342 0.75967906 0.35236294 0.1793508\n",
      " 0.7075285  0.29774946        nan 0.82769518 0.4397552  0.23925342\n",
      " 0.76003055 0.35236294 0.1793508  0.7047116  0.29774946        nan\n",
      " 0.82769518 0.4397552  0.23925342 0.76003055 0.35236294 0.1793508\n",
      " 0.7047116  0.29774946        nan 0.82769518 0.4397552  0.23925342\n",
      " 0.76003055 0.35236294 0.1793508  0.7047116  0.29774946        nan\n",
      " 0.82839941 0.43940246 0.23925342 0.76143963 0.35236294 0.1793508\n",
      " 0.7061213  0.29774946        nan 0.82839941 0.43940246 0.23925342\n",
      " 0.76143963 0.35236294 0.1793508  0.7061213  0.29774946        nan\n",
      " 0.82839941 0.43940246 0.23925342 0.76143963 0.35236294 0.1793508\n",
      " 0.7061213  0.29774946        nan 0.82135653 0.44151825 0.23925342\n",
      " 0.75404464 0.35306841 0.1793508  0.70541831 0.29774946        nan\n",
      " 0.82135653 0.44151825 0.23925342 0.75404464 0.35306841 0.1793508\n",
      " 0.70541831 0.29774946        nan 0.82135653 0.44151825 0.23925342\n",
      " 0.75404464 0.35306841 0.1793508  0.70541831 0.29774946        nan\n",
      " 0.8213584  0.44151825 0.23925342 0.75404402 0.35306841 0.1793508\n",
      " 0.70541893 0.29774946        nan 0.8213584  0.44151825 0.23925342\n",
      " 0.75404402 0.35306841 0.1793508  0.70541893 0.29774946        nan\n",
      " 0.8213584  0.44151825 0.23925342 0.75404402 0.35306841 0.1793508\n",
      " 0.70541893 0.29774946        nan 0.81959659 0.44151825 0.23925342\n",
      " 0.75686464 0.35306841 0.1793508  0.70577043 0.29774946        nan\n",
      " 0.81959659 0.44151825 0.23925342 0.75686464 0.35306841 0.1793508\n",
      " 0.70577043 0.29774946        nan 0.81959659 0.44151825 0.23925342\n",
      " 0.75686464 0.35306841 0.1793508  0.70577043 0.29774946        nan\n",
      " 0.81854025 0.44327881 0.23925342 0.75227973 0.35306841 0.1793508\n",
      " 0.70718198 0.29774946        nan 0.81854025 0.44327881 0.23925342\n",
      " 0.75227973 0.35306841 0.1793508  0.70718198 0.29774946        nan\n",
      " 0.81854025 0.44327881 0.23925342 0.75227973 0.35306841 0.1793508\n",
      " 0.70718198 0.29774946        nan 0.81959783 0.44327881 0.23925342\n",
      " 0.75192762 0.35377388 0.1793508  0.70718198 0.29774946        nan\n",
      " 0.81959783 0.44327881 0.23925342 0.75192762 0.35377388 0.1793508\n",
      " 0.70718198 0.29774946        nan 0.81959783 0.44327881 0.23925342\n",
      " 0.75192762 0.35377388 0.1793508  0.70718198 0.29774946        nan\n",
      " 0.81924448 0.44327881 0.23925342 0.75192824 0.35306841 0.1793508\n",
      " 0.70718198 0.29774946        nan 0.81924448 0.44327881 0.23925342\n",
      " 0.75192824 0.35306841 0.1793508  0.70718198 0.29774946        nan\n",
      " 0.81924448 0.44327881 0.23925342 0.75192824 0.35306841 0.1793508\n",
      " 0.70718198 0.29774946        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;tf&#x27;,\n",
       "                                        TfidfVectorizer(stop_words=[&#x27;i&#x27;, &#x27;me&#x27;,\n",
       "                                                                    &#x27;my&#x27;,\n",
       "                                                                    &#x27;myself&#x27;,\n",
       "                                                                    &#x27;we&#x27;, &#x27;our&#x27;,\n",
       "                                                                    &#x27;ours&#x27;,\n",
       "                                                                    &#x27;ourselves&#x27;,\n",
       "                                                                    &#x27;you&#x27;,\n",
       "                                                                    &#x27;youre&#x27;,\n",
       "                                                                    &#x27;youve&#x27;,\n",
       "                                                                    &#x27;youll&#x27;,\n",
       "                                                                    &#x27;youd&#x27;,\n",
       "                                                                    &#x27;your&#x27;,\n",
       "                                                                    &#x27;yours&#x27;,\n",
       "                                                                    &#x27;yourself&#x27;,\n",
       "                                                                    &#x27;yourselves&#x27;,\n",
       "                                                                    &#x27;he&#x27;, &#x27;him&#x27;,\n",
       "                                                                    &#x27;his&#x27;,\n",
       "                                                                    &#x27;himself&#x27;,\n",
       "                                                                    &#x27;she&#x27;,\n",
       "                                                                    &#x27;shes&#x27;,\n",
       "                                                                    &#x27;her&#x27;,\n",
       "                                                                    &#x27;hers&#x27;,\n",
       "                                                                    &#x27;herself&#x27;,\n",
       "                                                                    &#x27;it&#x27;, &#x27;its&#x27;,\n",
       "                                                                    &#x27;its&#x27;,\n",
       "                                                                    &#x27;itself&#x27;, ...])),\n",
       "                                       (&#x27;rf&#x27;,\n",
       "                                        RandomForestClassifier(random_state=42))]),\n",
       "             n_jobs=-2,\n",
       "             param_grid={&#x27;rf__class_weight&#x27;: [None, &#x27;balanced&#x27;,\n",
       "                                              &#x27;balanced_subsample&#x27;],\n",
       "                         &#x27;rf__min_samples_split&#x27;: [10, 20, 30],\n",
       "                         &#x27;rf__n_estimators&#x27;: [86, 88, 90],\n",
       "                         &#x27;tf__max_df&#x27;: [0.88, 0.9, 0.92],\n",
       "                         &#x27;tf__min_df&#x27;: [0.01, 0.02, 0.03],\n",
       "                         &#x27;tf__ngram_range&#x27;: [(1, 1), (2, 2), (3, 3)]},\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;tf&#x27;,\n",
       "                                        TfidfVectorizer(stop_words=[&#x27;i&#x27;, &#x27;me&#x27;,\n",
       "                                                                    &#x27;my&#x27;,\n",
       "                                                                    &#x27;myself&#x27;,\n",
       "                                                                    &#x27;we&#x27;, &#x27;our&#x27;,\n",
       "                                                                    &#x27;ours&#x27;,\n",
       "                                                                    &#x27;ourselves&#x27;,\n",
       "                                                                    &#x27;you&#x27;,\n",
       "                                                                    &#x27;youre&#x27;,\n",
       "                                                                    &#x27;youve&#x27;,\n",
       "                                                                    &#x27;youll&#x27;,\n",
       "                                                                    &#x27;youd&#x27;,\n",
       "                                                                    &#x27;your&#x27;,\n",
       "                                                                    &#x27;yours&#x27;,\n",
       "                                                                    &#x27;yourself&#x27;,\n",
       "                                                                    &#x27;yourselves&#x27;,\n",
       "                                                                    &#x27;he&#x27;, &#x27;him&#x27;,\n",
       "                                                                    &#x27;his&#x27;,\n",
       "                                                                    &#x27;himself&#x27;,\n",
       "                                                                    &#x27;she&#x27;,\n",
       "                                                                    &#x27;shes&#x27;,\n",
       "                                                                    &#x27;her&#x27;,\n",
       "                                                                    &#x27;hers&#x27;,\n",
       "                                                                    &#x27;herself&#x27;,\n",
       "                                                                    &#x27;it&#x27;, &#x27;its&#x27;,\n",
       "                                                                    &#x27;its&#x27;,\n",
       "                                                                    &#x27;itself&#x27;, ...])),\n",
       "                                       (&#x27;rf&#x27;,\n",
       "                                        RandomForestClassifier(random_state=42))]),\n",
       "             n_jobs=-2,\n",
       "             param_grid={&#x27;rf__class_weight&#x27;: [None, &#x27;balanced&#x27;,\n",
       "                                              &#x27;balanced_subsample&#x27;],\n",
       "                         &#x27;rf__min_samples_split&#x27;: [10, 20, 30],\n",
       "                         &#x27;rf__n_estimators&#x27;: [86, 88, 90],\n",
       "                         &#x27;tf__max_df&#x27;: [0.88, 0.9, 0.92],\n",
       "                         &#x27;tf__min_df&#x27;: [0.01, 0.02, 0.03],\n",
       "                         &#x27;tf__ngram_range&#x27;: [(1, 1), (2, 2), (3, 3)]},\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tf&#x27;,\n",
       "                 TfidfVectorizer(stop_words=[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;,\n",
       "                                             &#x27;our&#x27;, &#x27;ours&#x27;, &#x27;ourselves&#x27;, &#x27;you&#x27;,\n",
       "                                             &#x27;youre&#x27;, &#x27;youve&#x27;, &#x27;youll&#x27;, &#x27;youd&#x27;,\n",
       "                                             &#x27;your&#x27;, &#x27;yours&#x27;, &#x27;yourself&#x27;,\n",
       "                                             &#x27;yourselves&#x27;, &#x27;he&#x27;, &#x27;him&#x27;, &#x27;his&#x27;,\n",
       "                                             &#x27;himself&#x27;, &#x27;she&#x27;, &#x27;shes&#x27;, &#x27;her&#x27;,\n",
       "                                             &#x27;hers&#x27;, &#x27;herself&#x27;, &#x27;it&#x27;, &#x27;its&#x27;,\n",
       "                                             &#x27;its&#x27;, &#x27;itself&#x27;, ...])),\n",
       "                (&#x27;rf&#x27;, RandomForestClassifier(random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(stop_words=[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;, &#x27;our&#x27;, &#x27;ours&#x27;,\n",
       "                            &#x27;ourselves&#x27;, &#x27;you&#x27;, &#x27;youre&#x27;, &#x27;youve&#x27;, &#x27;youll&#x27;,\n",
       "                            &#x27;youd&#x27;, &#x27;your&#x27;, &#x27;yours&#x27;, &#x27;yourself&#x27;, &#x27;yourselves&#x27;,\n",
       "                            &#x27;he&#x27;, &#x27;him&#x27;, &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;, &#x27;shes&#x27;, &#x27;her&#x27;,\n",
       "                            &#x27;hers&#x27;, &#x27;herself&#x27;, &#x27;it&#x27;, &#x27;its&#x27;, &#x27;its&#x27;, &#x27;itself&#x27;, ...])</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tf',\n",
       "                                        TfidfVectorizer(stop_words=['i', 'me',\n",
       "                                                                    'my',\n",
       "                                                                    'myself',\n",
       "                                                                    'we', 'our',\n",
       "                                                                    'ours',\n",
       "                                                                    'ourselves',\n",
       "                                                                    'you',\n",
       "                                                                    'youre',\n",
       "                                                                    'youve',\n",
       "                                                                    'youll',\n",
       "                                                                    'youd',\n",
       "                                                                    'your',\n",
       "                                                                    'yours',\n",
       "                                                                    'yourself',\n",
       "                                                                    'yourselves',\n",
       "                                                                    'he', 'him',\n",
       "                                                                    'his',\n",
       "                                                                    'himself',\n",
       "                                                                    'she',\n",
       "                                                                    'shes',\n",
       "                                                                    'her',\n",
       "                                                                    'hers',\n",
       "                                                                    'herself',\n",
       "                                                                    'it', 'its',\n",
       "                                                                    'its',\n",
       "                                                                    'itself', ...])),\n",
       "                                       ('rf',\n",
       "                                        RandomForestClassifier(random_state=42))]),\n",
       "             n_jobs=-2,\n",
       "             param_grid={'rf__class_weight': [None, 'balanced',\n",
       "                                              'balanced_subsample'],\n",
       "                         'rf__min_samples_split': [10, 20, 30],\n",
       "                         'rf__n_estimators': [86, 88, 90],\n",
       "                         'tf__max_df': [0.88, 0.9, 0.92],\n",
       "                         'tf__min_df': [0.01, 0.02, 0.03],\n",
       "                         'tf__ngram_range': [(1, 1), (2, 2), (3, 3)]},\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_forest_grid = GridSearchCV(bin_forest_pipe_grid, forest_param_grid, cv=5, n_jobs=-2, verbose=1, scoring='accuracy')\n",
    "bin_forest_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('tf',\n",
      "                 TfidfVectorizer(max_df=0.88, min_df=0.01,\n",
      "                                 stop_words=['i', 'me', 'my', 'myself', 'we',\n",
      "                                             'our', 'ours', 'ourselves', 'you',\n",
      "                                             'youre', 'youve', 'youll', 'youd',\n",
      "                                             'your', 'yours', 'yourself',\n",
      "                                             'yourselves', 'he', 'him', 'his',\n",
      "                                             'himself', 'she', 'shes', 'her',\n",
      "                                             'hers', 'herself', 'it', 'its',\n",
      "                                             'its', 'itself', ...])),\n",
      "                ('rf',\n",
      "                 RandomForestClassifier(min_samples_split=10, n_estimators=86,\n",
      "                                        random_state=42))])\n",
      "{'rf__class_weight': None, 'rf__min_samples_split': 10, 'rf__n_estimators': 86, 'tf__max_df': 0.88, 'tf__min_df': 0.01, 'tf__ngram_range': (1, 1)}\n",
      "0.8520133144546289\n"
     ]
    }
   ],
   "source": [
    "print(bin_forest_grid.best_estimator_)\n",
    "print(bin_forest_grid.best_params_)\n",
    "print(bin_forest_grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Feature importances')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract the TfidfVectorizer from the pipeline\n",
    "tf_vec = bin_forest_pipe_grid.named_steps['tf']\n",
    "\n",
    "# Fit the TfidfVectorizer on the training data\n",
    "tf_vec.fit(X_train)\n",
    "\n",
    "# Transform the training data using the fitted TfidfVectorizer\n",
    "X_train_tf = tf_vec.transform(X_train)\n",
    "\n",
    "# Get the feature names from the TfidfVectorizer\n",
    "feature_names = tf_vec.get_feature_names_out()\n",
    "\n",
    "# Extract the RandomForestClassifier from the pipeline\n",
    "rf = bin_forest_pipe_grid.named_steps['rf']\n",
    "\n",
    "# Fit the RandomForestClassifier on the transformed training data\n",
    "rf.fit(X_train_tf, y_train)\n",
    "\n",
    "# Get the feature importances from the fitted RandomForestClassifier\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# Create a DataFrame with the feature names and their corresponding importances\n",
    "importances_df = pd.DataFrame({\"feature_names\": feature_names, \"importances\": feature_importances})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_importances = importances_df.sort_values(by='importances', ascending=False)\n",
    "sorted_importances = sorted_importances.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_importances.to_csv('data/importances.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion_encoded</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>iPhone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>iPad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9077</th>\n",
       "      <td>1</td>\n",
       "      <td>iPhone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9079</th>\n",
       "      <td>1</td>\n",
       "      <td>iPad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9080</th>\n",
       "      <td>0</td>\n",
       "      <td>Other Google product or service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9085</th>\n",
       "      <td>1</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9088</th>\n",
       "      <td>1</td>\n",
       "      <td>iPad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3548 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      emotion_encoded                          product\n",
       "0                   0                           iPhone\n",
       "1                   1               iPad or iPhone App\n",
       "2                   1                             iPad\n",
       "3                   0               iPad or iPhone App\n",
       "4                   1                           Google\n",
       "...               ...                              ...\n",
       "9077                1                           iPhone\n",
       "9079                1                             iPad\n",
       "9080                0  Other Google product or service\n",
       "9085                1               iPad or iPhone App\n",
       "9088                1                             iPad\n",
       "\n",
       "[3548 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment = df_binary[['emotion_encoded', 'product']]\n",
    "sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion_encoded  product                        \n",
       "1                iPad                               793\n",
       "                 Apple                              543\n",
       "                 iPad or iPhone App                 397\n",
       "                 Google                             346\n",
       "                 Other Google product or service    236\n",
       "                 iPhone                             184\n",
       "                 Android App                         72\n",
       "                 Android                             69\n",
       "                 Other Apple product or service      32\n",
       "dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment[sentiment['emotion_encoded'] == 1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1949\n",
       "0     388\n",
       "Name: emotion_encoded, dtype: int64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple_prod = ['iPad', 'Apple', 'iPad or iPhone App', 'iPhone', 'Other Apple product or service']\n",
    "mask = sentiment[sentiment['product'].isin(apple_prod)]\n",
    "mask['emotion_encoded'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8339751818570817 0.16602481814291825\n"
     ]
    }
   ],
   "source": [
    "tot_apple_review = len(mask)\n",
    "pos_apple_review = 1949/len(mask)\n",
    "neg_apple_review = 1 - pos_apple_review\n",
    "print(pos_apple_review, neg_apple_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data = {\"Positive Review Percentage\": [pos_apple_review, pos_google_review],\n",
    "               \"Negative Review Percentage\": [neg_apple_review, neg_google_review]\n",
    "              }\n",
    "review_df = pd.DataFrame(review_data)\n",
    "review_df['Brand'] = ['Apple', 'Google']\n",
    "review_df.to_csv('data/review.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    723\n",
       "0    131\n",
       "Name: emotion_encoded, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_prod = ['Google', 'Other Google product or service', 'Android App', 'Android']\n",
    "g_mask = sentiment[sentiment['product'].isin(google_prod)]\n",
    "g_mask['emotion_encoded'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8466042154566745 0.15339578454332548\n"
     ]
    }
   ],
   "source": [
    "tot_google_review = len(g_mask)\n",
    "pos_google_review = 723/tot_google_review\n",
    "neg_google_review = 1 - pos_google_review\n",
    "print(pos_google_review, neg_google_review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Lengths must match to compare', (3548,), (5,))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-9c6360382181>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m tot_apple_products= sentiment.loc[sentiment['product'] == ['iPad', 'Apple', 'iPad or iPhone App', 'iPhone', \n\u001b[0m\u001b[1;32m      2\u001b[0m                                                           'Other Apple product or service']]\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/ops/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0mrvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomparison_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;31m#  The ambiguous case is object-dtype.  See GH#27803\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    225\u001b[0m                 \u001b[0;34m\"Lengths must match to compare\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: ('Lengths must match to compare', (3548,), (5,))"
     ]
    }
   ],
   "source": [
    "tot_apple_products=sentiment.loc[sentiment['product'] == ['iPad', 'Apple', 'iPad or iPhone App', 'iPhone', \n",
    "                                                          'Other Apple product or service']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wesley iphone hr tweet riseaustin dead need upgrade plugin station: 0.01\n",
      "jessedee know fludapp awesome ipadiphone app likely appreciate design also theyre give free t: 0.00\n",
      "swonderlin wait ipad also sale: 0.01\n",
      "hope year festival crashy year iphone app: 0.01\n",
      "sxtxstate great stuff fri marissa mayer google tim oreilly tech booksconferences amp matt mullenweg wordpress: 0.02\n",
      "start ctia around corner googleio hop skip jump good time android fan: 0.01\n",
      "beautifully smart simple idea madebymany thenextweb write hollergram ipad app httpbitlyieavob: 0.00\n",
      "counting day plus strong canadian dollar mean stock apple gear: 0.01\n",
      "excited meet samsungmobileus show sprint galaxy still run android fail: 0.01\n",
      "find amp start impromptu party hurricaneparty httpbitlygvlrin cant wait til android app come: 0.00\n",
      "foursquare ups game time httpjmpgrnpk still prefer gowalla far best look android app date: 0.01\n",
      "get ta love google calendar feature top party show case check hamsandwich via ischafer gthttpbitlyaxzwxb: 0.00\n",
      "great ipad app madebymany httptinyurlcomnqvl: 0.00\n",
      "haha awesomely rad ipad app madebymany httpbitlyhtdfim hollergram: 0.00\n",
      "noticed dst come weekend many iphone user hour late come sunday morning iphone: 0.00\n",
      "added flight planely match people planesairports also download klm iphone app nicely do: 0.01\n",
      "must app malbonster lovely review forbes ipad app holler gram httptcoggzypv: 0.01\n",
      "need buy ipad im austin sure ill need austin apple store: 0.00\n",
      "oh god app ipad pure unadulterated awesome easy browse event ipad website: 0.00\n",
      "okay really yay new foursquare android app kthxbai: 0.01\n",
      "photo instal iphone app really nice httptumblrcomxtpiav: 0.00\n",
      "really enjoy change gowalla android look forward see else amp foursquare sleeve: 0.00\n",
      "laurieshook im look forward smcdallas pre party wed hop ill win ipad result shameless promotion chevysmc: 0.01\n",
      "haha awesomely rad ipad app madebymany httpbitlyhtdfim hollergram via michaelpiliero: 0.01\n",
      "someone start austin partnerhub group google group presxsw great idea: 0.02\n",
      "new sq look like go rock update iphone android push tonight httpbitlyetsbzk keepaustinweird: 0.00\n",
      "right gowalla app android sweeeeet nice job team: 0.00\n",
      "smart madebymany hollergram ipad app httptcoaxvwc may leave vuvuzela home: 0.00\n",
      "must app ipad go httpitunesapplecomusapphollergramidmt hollergram: 0.01\n",
      "best ha first line ipad quotpopupquot apple store event planner eventprofs pcma engage: 0.00\n",
      "false alarm google circle come nowand probably ever google circle social: 0.01\n",
      "great weather greet still need sweater nightapple put quotflash storequot downtown sell ipad: 0.00\n",
      "ipad smartcover open instant access wait get one apple: 0.01\n",
      "handheld hobo drafthouse launch hobo shotgun iphone app: 0.00\n",
      "hooray apple open popup store austin: 0.00\n",
      "wooooo apple store downtown austin open til midnight: 0.00\n",
      "talk google effort allow user open system bettercloud: 0.00\n",
      "st stop chaos amp hunt austin java get spy game chance win ipad: 0.01\n",
      "omfg heard apple popup store downtown austin pic already gowalla ipad: 0.01\n",
      "line apple store insane: 0.01\n",
      "attend ipad design headache: 0.01\n",
      "boooo flipboard develop iphone version android say: 0.04\n",
      "check amp line ipad austin power sxswi: 0.00\n",
      "come party google tonight band food art ice cream nifty interactive map: 0.01\n",
      "holla google party best ever get butt: 0.01\n",
      "love iphone case cant get phone fail: 0.00\n",
      "new post iphone app make easy connect social network people meet: 0.00\n",
      "nice iphone app behave today crash yesterday ridiculous: 0.00\n",
      "nice hey apple fan get peek space thats slat popup apple store tomorrow: 0.00\n",
      "one thing great get great earth face google company love sxwsi: 0.01\n",
      "thanks new speech ipad apps showcased conf sxswh sxsh: 0.00\n",
      "provide iphone charger ive change mind go next year: 0.03\n",
      "xmas shiny new apps new garyvee book popup ipad store christmas nerd: 0.00\n",
      "yai new ubersocial iphone app store include uberguide sponsor cont: 0.03\n",
      "fast fun amp future google present search local mobile: 0.01\n",
      "headline quotipad musthave gadget sxswquot hmm could see one come gadget: 0.00\n",
      "know quotdatavizquot translates quotsatanicquot iphone im sayin: 0.00\n",
      "quotgoogle launch checkins month agoquot check in ok check out future bizzy: 0.01\n",
      "quotgoogle tweetquot new quotthink speakquot mark belinsky tweet panel: 0.01\n",
      "kawasaki quotnot c lewis level reason apple continue existence evidence existence godquot bawling: 0.01\n",
      "kawasaki quotpagemaker save applequot oh day jwtatl enchantment via: 0.02\n",
      "spark android teamandroid award read: 0.01\n",
      "apple ipad great thought japan apac region deal earthquake amp tsunami trauma sxswi: 0.01\n",
      "apple school marketing expert: 0.02\n",
      "temporary apple store def tent powerhouse gym: 0.02\n",
      "temporary apple store th congress along happy hipster: 0.00\n",
      "apple win open temporary store downtown austin support ipad launch good: 0.00\n",
      "ipad austin trend today fun nerdy nerd: 0.01\n",
      "christian ipad iphone devs want talk maybe wk together cool app: 0.00\n",
      "apple ipad take austin storm excited part mobile: 0.00\n",
      "haz ipad ifrom gr: 0.00\n",
      "stack ipads wait buy got mine hassle apple handle perfectly: 0.00\n",
      "smallbiz need review play google placeswe get app thatlink seo: 0.00\n",
      "take major south korean director get make movie entirely iphone: 0.00\n",
      "beautiful apple store pic: 0.02\n",
      "must app lovely review forbes ipad app holler gram: 0.02\n",
      "temporary apple store apple sneaky usual: 0.00\n",
      "beta test interactive book ipad app moonbot studio louisiana cool app: 0.01\n",
      "apple day one see ton ipad: 0.01\n",
      "ipad store sell everything except gig wifi white manage get: 0.00\n",
      "ipad store sell everything except gig wifi white also know white jean configuration: 0.00\n",
      "offer ipad promo ninjafinder user fan suck: 0.01\n",
      "poursite learn lifechanging impact ipad real people actual life bravo: 0.01\n",
      "lonelyplanet austin guide iphone free limited time lp travel: 0.01\n",
      "apple store still ipad short line: 0.01\n",
      "leave tradeshow demo google theatre ok get see presenter use: 0.02\n",
      "anyone want make quick hundred dollar new ipad ad hoc apple store get hundred plus cost: 0.01\n",
      "monday barry diller new york time congress lunch hotel party google party six dirty martini monday: 0.00\n",
      "seriously test mobile apps constant ipad crash cause lose schedule sync wp: 0.00\n",
      "ready iphone apps make ur blogging easy sxswi: 0.01\n",
      "ipad sxswa conflagration doofusness: 0.00\n",
      "attention sxswers rumor popup temporary apple store ipad launch need: 0.01\n",
      "go sxswi lousy ipad: 0.02\n",
      "win picture android google: 0.01\n",
      "spent come already use ipad wait couple city block ipad: 0.00\n",
      "behind email give line iphone compose reply protip: 0.00\n",
      "like pm night line around block popup apple store sell ipads: 0.00\n",
      "im ipad see wild people say fast still pic terrible: 0.00\n",
      "ipad take video: 0.02\n",
      "many ipad snap away keynote slide: 0.01\n",
      "new post iphone apps well use south southwest interactive sxswi: 0.00\n",
      "sweet new google map demo go ballroom: 0.00\n",
      "million mile per day drive google map navigation: 0.00\n",
      "day apple store there still lineand grow: 0.00\n",
      "band want share track audience stage use frostwire android theres wifi available: 0.00\n",
      "iphone need caruse zimride etc share rid shareable: 0.00\n",
      "pick mophie battery case iphone prep lug around laptop amp use phone huge win last year: 0.01\n",
      "rumor monger good read google circle preview via socbiz fb: 0.00\n",
      "hobo shotgun video game come ipadiphone user yes please: 0.01\n",
      "build store less hour one big launch companys history get apple ipad: 0.01\n",
      "read groupme sxswsounds like incredible app available android phone yet: 0.01\n",
      "iphone alarm botch timechange many sxswers freak late flight miss panel behind bloody mary: 0.01\n",
      "shouts lady hold ipad free show take photo sip free beer cc: 0.00\n",
      "meant also wish dyac stupid iphone: 0.00\n",
      "rumor apple store open th amp congress sign point yes: 0.01\n",
      "pick ipad pop apple store minute wait: 0.00\n",
      "brilliant enlighten game mechanic presentation google: 0.00\n",
      "marketing sale genius apple: 0.00\n",
      "need ipad iphone laptop dictaphone vidcamera wow love meet real cerebellum charge people: 0.00\n",
      "app iphone ridiculous personal planner next day: 0.00\n",
      "find app kyping iphones geolocation amp release background need patch batterykiller: 0.02\n",
      "app iphone live rsvp event phone amp check sundayswaggereventbritecom scoremore: 0.01\n",
      "love app awesome must see android app quotbizzyquot: 0.00\n",
      "course apple build temp store austin texas understand concept corral cattle pickmeupanipad: 0.01\n",
      "love apple let something like retail store near keep away open popup store: 0.00\n",
      "surprise apple open popup store austin nerd town get new cnet: 0.01\n",
      "surprise apple open popup store austin nerd town get new ipads cnet: 0.01\n",
      "surprise apple open popup store austin nerd town get new ipads: 0.01\n",
      "apple great example retail store layed future register checkout line: 0.00\n",
      "apple open temporary store downtown austin ipad launch gswsxsw: 0.01\n",
      "apple open temporary store downtown austin ipad launchquot oh yay traffic: 0.01\n",
      "apple popup store open noon fresh shipment ipad im pretty sure im go get one finger cross: 0.01\n",
      "technews apple save set open popup store technews apple tech: 0.00\n",
      "apple school marketing expert: 0.01\n",
      "apple school marketing expert via: 0.00\n",
      "apple school marketing expert via ltlt true love loathe one marketing like: 0.01\n",
      "apple school marketing expert cnet: 0.00\n",
      "apple school marketing expert cnet blog austin atx retail: 0.01\n",
      "apple store downtown austin open til midnight there still time: 0.00\n",
      "quotthe apple store mall sunday crowd line fake need fuck donglequot genius let: 0.01\n",
      "makeshift apple store th congress kid amaze apple: 0.01\n"
     ]
    }
   ],
   "source": [
    "for feature, importance in zip(df_binary.processed_tweet, importances):\n",
    "    print(f\"{feature}: {importance:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fd738cf3ac0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred_forest_grid = bin_forest_grid.predict(X_test)\n",
    "cfm_forest = confusion_matrix(y_test, y_pred_forest_grid)\n",
    "ConfusionMatrixDisplay(cfm_forest).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.852112676056338"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "forest_grid_test_acc = accuracy_score(y_test, y_pred_forest_grid)\n",
    "forest_grid_test_acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 729 candidates, totalling 3645 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
      "486 fits failed out of a total of 3645.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "486 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\", line 416, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\", line 370, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/joblib/memory.py\", line 353, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\", line 950, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 2126, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 1396, in fit_transform\n",
      "    X, self.stop_words_ = self._limit_features(\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 1248, in _limit_features\n",
      "    raise ValueError(\n",
      "ValueError: After pruning, no terms remain. Try a lower min_df or a higher max_df.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:976: UserWarning: One or more of the test scores are non-finite: [0.85506608 0.84008811 0.83964758 0.84008811 0.84008811        nan\n",
      " 0.84096916 0.83964758        nan 0.85506608 0.84008811 0.83964758\n",
      " 0.84008811 0.84008811        nan 0.84096916 0.83964758        nan\n",
      " 0.85506608 0.84008811 0.83964758 0.84008811 0.84008811        nan\n",
      " 0.84096916 0.83964758        nan 0.85594714 0.83964758 0.83964758\n",
      " 0.84052863 0.84008811        nan 0.84140969 0.83964758        nan\n",
      " 0.85594714 0.83964758 0.83964758 0.84052863 0.84008811        nan\n",
      " 0.84140969 0.83964758        nan 0.85594714 0.83964758 0.83964758\n",
      " 0.84052863 0.84008811        nan 0.84140969 0.83964758        nan\n",
      " 0.85594714 0.83964758 0.83964758 0.84096916 0.84008811        nan\n",
      " 0.84096916 0.83964758        nan 0.85594714 0.83964758 0.83964758\n",
      " 0.84096916 0.84008811        nan 0.84096916 0.83964758        nan\n",
      " 0.85594714 0.83964758 0.83964758 0.84096916 0.84008811        nan\n",
      " 0.84096916 0.83964758        nan 0.84801762 0.83964758 0.83964758\n",
      " 0.84493392 0.84008811        nan 0.84008811 0.83964758        nan\n",
      " 0.84801762 0.83964758 0.83964758 0.84493392 0.84008811        nan\n",
      " 0.84008811 0.83964758        nan 0.84801762 0.83964758 0.83964758\n",
      " 0.84493392 0.84008811        nan 0.84008811 0.83964758        nan\n",
      " 0.84889868 0.83964758 0.83964758 0.84449339 0.84008811        nan\n",
      " 0.84052863 0.83964758        nan 0.84889868 0.83964758 0.83964758\n",
      " 0.84449339 0.84008811        nan 0.84052863 0.83964758        nan\n",
      " 0.84889868 0.83964758 0.83964758 0.84449339 0.84008811        nan\n",
      " 0.84052863 0.83964758        nan 0.84889868 0.83964758 0.83964758\n",
      " 0.84493392 0.84008811        nan 0.84052863 0.83964758        nan\n",
      " 0.84889868 0.83964758 0.83964758 0.84493392 0.84008811        nan\n",
      " 0.84052863 0.83964758        nan 0.84889868 0.83964758 0.83964758\n",
      " 0.84493392 0.84008811        nan 0.84052863 0.83964758        nan\n",
      " 0.84493392 0.83964758 0.83964758 0.84140969 0.84008811        nan\n",
      " 0.84096916 0.83964758        nan 0.84493392 0.83964758 0.83964758\n",
      " 0.84140969 0.84008811        nan 0.84096916 0.83964758        nan\n",
      " 0.84493392 0.83964758 0.83964758 0.84140969 0.84008811        nan\n",
      " 0.84096916 0.83964758        nan 0.84449339 0.83964758 0.83964758\n",
      " 0.84229075 0.84008811        nan 0.84052863 0.83964758        nan\n",
      " 0.84449339 0.83964758 0.83964758 0.84229075 0.84008811        nan\n",
      " 0.84052863 0.83964758        nan 0.84449339 0.83964758 0.83964758\n",
      " 0.84229075 0.84008811        nan 0.84052863 0.83964758        nan\n",
      " 0.84493392 0.83964758 0.83964758 0.84140969 0.84008811        nan\n",
      " 0.84052863 0.83964758        nan 0.84493392 0.83964758 0.83964758\n",
      " 0.84140969 0.84008811        nan 0.84052863 0.83964758        nan\n",
      " 0.84493392 0.83964758 0.83964758 0.84140969 0.84008811        nan\n",
      " 0.84052863 0.83964758        nan 0.82555066 0.43436123 0.24052863\n",
      " 0.77136564 0.36343612        nan 0.71277533 0.30528634        nan\n",
      " 0.82555066 0.43436123 0.24052863 0.77136564 0.36343612        nan\n",
      " 0.71277533 0.30528634        nan 0.82555066 0.43436123 0.24052863\n",
      " 0.77136564 0.36343612        nan 0.71277533 0.30528634        nan\n",
      " 0.82643172 0.4339207  0.24052863 0.77092511 0.36343612        nan\n",
      " 0.71277533 0.30528634        nan 0.82643172 0.4339207  0.24052863\n",
      " 0.77092511 0.36343612        nan 0.71277533 0.30528634        nan\n",
      " 0.82643172 0.4339207  0.24052863 0.77092511 0.36343612        nan\n",
      " 0.71277533 0.30528634        nan 0.82599119 0.4339207  0.24052863\n",
      " 0.77048458 0.36343612        nan 0.7123348  0.30528634        nan\n",
      " 0.82599119 0.4339207  0.24052863 0.77048458 0.36343612        nan\n",
      " 0.7123348  0.30528634        nan 0.82599119 0.4339207  0.24052863\n",
      " 0.77048458 0.36343612        nan 0.7123348  0.30528634        nan\n",
      " 0.81453744 0.43612335 0.24052863 0.76828194 0.36343612        nan\n",
      " 0.71365639 0.30528634        nan 0.81453744 0.43612335 0.24052863\n",
      " 0.76828194 0.36343612        nan 0.71365639 0.30528634        nan\n",
      " 0.81453744 0.43612335 0.24052863 0.76828194 0.36343612        nan\n",
      " 0.71365639 0.30528634        nan 0.81497797 0.43612335 0.24052863\n",
      " 0.76784141 0.36343612        nan 0.71409692 0.30528634        nan\n",
      " 0.81497797 0.43612335 0.24052863 0.76784141 0.36343612        nan\n",
      " 0.71409692 0.30528634        nan 0.81497797 0.43612335 0.24052863\n",
      " 0.76784141 0.36343612        nan 0.71409692 0.30528634        nan\n",
      " 0.81497797 0.43612335 0.24052863 0.76784141 0.36343612        nan\n",
      " 0.71585903 0.30528634        nan 0.81497797 0.43612335 0.24052863\n",
      " 0.76784141 0.36343612        nan 0.71585903 0.30528634        nan\n",
      " 0.81497797 0.43612335 0.24052863 0.76784141 0.36343612        nan\n",
      " 0.71585903 0.30528634        nan 0.8061674  0.43920705 0.24052863\n",
      " 0.76519824 0.36431718        nan 0.71718062 0.30528634        nan\n",
      " 0.8061674  0.43920705 0.24052863 0.76519824 0.36431718        nan\n",
      " 0.71718062 0.30528634        nan 0.8061674  0.43920705 0.24052863\n",
      " 0.76519824 0.36431718        nan 0.71718062 0.30528634        nan\n",
      " 0.8061674  0.43700441 0.24052863 0.76519824 0.36343612        nan\n",
      " 0.71718062 0.30528634        nan 0.8061674  0.43700441 0.24052863\n",
      " 0.76519824 0.36343612        nan 0.71718062 0.30528634        nan\n",
      " 0.8061674  0.43700441 0.24052863 0.76519824 0.36343612        nan\n",
      " 0.71718062 0.30528634        nan 0.80660793 0.43612335 0.24052863\n",
      " 0.76387665 0.36431718        nan 0.71762115 0.30528634        nan\n",
      " 0.80660793 0.43612335 0.24052863 0.76387665 0.36431718        nan\n",
      " 0.71762115 0.30528634        nan 0.80660793 0.43612335 0.24052863\n",
      " 0.76387665 0.36431718        nan 0.71762115 0.30528634        nan\n",
      " 0.82599119 0.43656388 0.24273128 0.77048458 0.36343612        nan\n",
      " 0.71277533 0.30528634        nan 0.82599119 0.43656388 0.24273128\n",
      " 0.77048458 0.36343612        nan 0.71277533 0.30528634        nan\n",
      " 0.82599119 0.43656388 0.24273128 0.77048458 0.36343612        nan\n",
      " 0.71277533 0.30528634        nan 0.82643172 0.4339207  0.24052863\n",
      " 0.77180617 0.36343612        nan 0.71189427 0.30528634        nan\n",
      " 0.82643172 0.4339207  0.24052863 0.77180617 0.36343612        nan\n",
      " 0.71189427 0.30528634        nan 0.82643172 0.4339207  0.24052863\n",
      " 0.77180617 0.36343612        nan 0.71189427 0.30528634        nan\n",
      " 0.82907489 0.4339207  0.24052863 0.77180617 0.36343612        nan\n",
      " 0.71189427 0.30528634        nan 0.82907489 0.4339207  0.24052863\n",
      " 0.77180617 0.36343612        nan 0.71189427 0.30528634        nan\n",
      " 0.82907489 0.4339207  0.24052863 0.77180617 0.36343612        nan\n",
      " 0.71189427 0.30528634        nan 0.81894273 0.43612335 0.24052863\n",
      " 0.76431718 0.36343612        nan 0.71365639 0.30528634        nan\n",
      " 0.81894273 0.43612335 0.24052863 0.76431718 0.36343612        nan\n",
      " 0.71365639 0.30528634        nan 0.81894273 0.43612335 0.24052863\n",
      " 0.76431718 0.36343612        nan 0.71365639 0.30528634        nan\n",
      " 0.8185022  0.43612335 0.24052863 0.76079295 0.36343612        nan\n",
      " 0.71365639 0.30528634        nan 0.8185022  0.43612335 0.24052863\n",
      " 0.76079295 0.36343612        nan 0.71365639 0.30528634        nan\n",
      " 0.8185022  0.43612335 0.24052863 0.76079295 0.36343612        nan\n",
      " 0.71365639 0.30528634        nan 0.82114537 0.43612335 0.24052863\n",
      " 0.76167401 0.36343612        nan 0.71497797 0.30528634        nan\n",
      " 0.82114537 0.43612335 0.24052863 0.76167401 0.36343612        nan\n",
      " 0.71497797 0.30528634        nan 0.82114537 0.43612335 0.24052863\n",
      " 0.76167401 0.36343612        nan 0.71497797 0.30528634        nan\n",
      " 0.81057269 0.43920705 0.24052863 0.76563877 0.36431718        nan\n",
      " 0.7215859  0.30528634        nan 0.81057269 0.43920705 0.24052863\n",
      " 0.76563877 0.36431718        nan 0.7215859  0.30528634        nan\n",
      " 0.81057269 0.43920705 0.24052863 0.76563877 0.36431718        nan\n",
      " 0.7215859  0.30528634        nan 0.8092511  0.43832599 0.24052863\n",
      " 0.76784141 0.36343612        nan 0.71938326 0.30528634        nan\n",
      " 0.8092511  0.43832599 0.24052863 0.76784141 0.36343612        nan\n",
      " 0.71938326 0.30528634        nan 0.8092511  0.43832599 0.24052863\n",
      " 0.76784141 0.36343612        nan 0.71938326 0.30528634        nan\n",
      " 0.80837004 0.43832599 0.24052863 0.76696035 0.36431718        nan\n",
      " 0.72026432 0.30528634        nan 0.80837004 0.43832599 0.24052863\n",
      " 0.76696035 0.36431718        nan 0.72026432 0.30528634        nan\n",
      " 0.80837004 0.43832599 0.24052863 0.76696035 0.36431718        nan\n",
      " 0.72026432 0.30528634        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 729 candidates, totalling 3645 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
      "486 fits failed out of a total of 3645.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "486 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\", line 416, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\", line 370, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/joblib/memory.py\", line 353, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\", line 950, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 2126, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 1396, in fit_transform\n",
      "    X, self.stop_words_ = self._limit_features(\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 1248, in _limit_features\n",
      "    raise ValueError(\n",
      "ValueError: After pruning, no terms remain. Try a lower min_df or a higher max_df.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:976: UserWarning: One or more of the test scores are non-finite: [0.85110132 0.83788546 0.83964758 0.84493392 0.83832599        nan\n",
      " 0.83876652 0.83964758        nan 0.85110132 0.83788546 0.83964758\n",
      " 0.84493392 0.83832599        nan 0.83876652 0.83964758        nan\n",
      " 0.85110132 0.83788546 0.83964758 0.84493392 0.83832599        nan\n",
      " 0.83876652 0.83964758        nan 0.85198238 0.83788546 0.83964758\n",
      " 0.84493392 0.83832599        nan 0.83964758 0.83964758        nan\n",
      " 0.85198238 0.83788546 0.83964758 0.84493392 0.83832599        nan\n",
      " 0.83964758 0.83964758        nan 0.85198238 0.83788546 0.83964758\n",
      " 0.84493392 0.83832599        nan 0.83964758 0.83964758        nan\n",
      " 0.85242291 0.83788546 0.83964758 0.84625551 0.83832599        nan\n",
      " 0.83920705 0.83964758        nan 0.85242291 0.83788546 0.83964758\n",
      " 0.84625551 0.83832599        nan 0.83920705 0.83964758        nan\n",
      " 0.85242291 0.83788546 0.83964758 0.84625551 0.83832599        nan\n",
      " 0.83920705 0.83964758        nan 0.84493392 0.83744493 0.83964758\n",
      " 0.84361233 0.83832599        nan 0.83920705 0.83964758        nan\n",
      " 0.84493392 0.83744493 0.83964758 0.84361233 0.83832599        nan\n",
      " 0.83920705 0.83964758        nan 0.84493392 0.83744493 0.83964758\n",
      " 0.84361233 0.83832599        nan 0.83920705 0.83964758        nan\n",
      " 0.84449339 0.83744493 0.83964758 0.84317181 0.83832599        nan\n",
      " 0.83876652 0.83964758        nan 0.84449339 0.83744493 0.83964758\n",
      " 0.84317181 0.83832599        nan 0.83876652 0.83964758        nan\n",
      " 0.84449339 0.83744493 0.83964758 0.84317181 0.83832599        nan\n",
      " 0.83876652 0.83964758        nan 0.84537445 0.83744493 0.83964758\n",
      " 0.84405286 0.83832599        nan 0.83876652 0.83964758        nan\n",
      " 0.84537445 0.83744493 0.83964758 0.84405286 0.83832599        nan\n",
      " 0.83876652 0.83964758        nan 0.84537445 0.83744493 0.83964758\n",
      " 0.84405286 0.83832599        nan 0.83876652 0.83964758        nan\n",
      " 0.84493392 0.83744493 0.83964758 0.84185022 0.83832599        nan\n",
      " 0.83788546 0.83964758        nan 0.84493392 0.83744493 0.83964758\n",
      " 0.84185022 0.83832599        nan 0.83788546 0.83964758        nan\n",
      " 0.84493392 0.83744493 0.83964758 0.84185022 0.83832599        nan\n",
      " 0.83788546 0.83964758        nan 0.84581498 0.83744493 0.83964758\n",
      " 0.84185022 0.83832599        nan 0.83788546 0.83964758        nan\n",
      " 0.84581498 0.83744493 0.83964758 0.84185022 0.83832599        nan\n",
      " 0.83788546 0.83964758        nan 0.84581498 0.83744493 0.83964758\n",
      " 0.84185022 0.83832599        nan 0.83788546 0.83964758        nan\n",
      " 0.84493392 0.83744493 0.83964758 0.84229075 0.83832599        nan\n",
      " 0.83788546 0.83964758        nan 0.84493392 0.83744493 0.83964758\n",
      " 0.84229075 0.83832599        nan 0.83788546 0.83964758        nan\n",
      " 0.84493392 0.83744493 0.83964758 0.84229075 0.83832599        nan\n",
      " 0.83788546 0.83964758        nan 0.83436123 0.44405286 0.22995595\n",
      " 0.76079295 0.36696035        nan 0.70352423 0.3               nan\n",
      " 0.83436123 0.44405286 0.22995595 0.76079295 0.36696035        nan\n",
      " 0.70352423 0.3               nan 0.83436123 0.44405286 0.22995595\n",
      " 0.76079295 0.36696035        nan 0.70352423 0.3               nan\n",
      " 0.83215859 0.44361233 0.22995595 0.76035242 0.36696035        nan\n",
      " 0.70484581 0.3               nan 0.83215859 0.44361233 0.22995595\n",
      " 0.76035242 0.36696035        nan 0.70484581 0.3               nan\n",
      " 0.83215859 0.44361233 0.22995595 0.76035242 0.36696035        nan\n",
      " 0.70484581 0.3               nan 0.8339207  0.44361233 0.22995595\n",
      " 0.76035242 0.3660793         nan 0.70220264 0.3               nan\n",
      " 0.8339207  0.44361233 0.22995595 0.76035242 0.3660793         nan\n",
      " 0.70220264 0.3               nan 0.8339207  0.44361233 0.22995595\n",
      " 0.76035242 0.3660793         nan 0.70220264 0.3               nan\n",
      " 0.8215859  0.44361233 0.23127753 0.75682819 0.36828194        nan\n",
      " 0.70881057 0.3               nan 0.8215859  0.44361233 0.23127753\n",
      " 0.75682819 0.36828194        nan 0.70881057 0.3               nan\n",
      " 0.8215859  0.44361233 0.23127753 0.75682819 0.36828194        nan\n",
      " 0.70881057 0.3               nan 0.82070485 0.44361233 0.23127753\n",
      " 0.75638767 0.36740088        nan 0.7092511  0.3               nan\n",
      " 0.82070485 0.44361233 0.23127753 0.75638767 0.36740088        nan\n",
      " 0.7092511  0.3               nan 0.82070485 0.44361233 0.23127753\n",
      " 0.75638767 0.36740088        nan 0.7092511  0.3               nan\n",
      " 0.8215859  0.44361233 0.23127753 0.75770925 0.36740088        nan\n",
      " 0.70704846 0.3               nan 0.8215859  0.44361233 0.23127753\n",
      " 0.75770925 0.36740088        nan 0.70704846 0.3               nan\n",
      " 0.8215859  0.44361233 0.23127753 0.75770925 0.36740088        nan\n",
      " 0.70704846 0.3               nan 0.81453744 0.44405286 0.23127753\n",
      " 0.75594714 0.369163          nan 0.70748899 0.3               nan\n",
      " 0.81453744 0.44405286 0.23127753 0.75594714 0.369163          nan\n",
      " 0.70748899 0.3               nan 0.81453744 0.44405286 0.23127753\n",
      " 0.75594714 0.369163          nan 0.70748899 0.3               nan\n",
      " 0.81629956 0.44317181 0.23127753 0.75682819 0.369163          nan\n",
      " 0.70792952 0.3               nan 0.81629956 0.44317181 0.23127753\n",
      " 0.75682819 0.369163          nan 0.70792952 0.3               nan\n",
      " 0.81629956 0.44317181 0.23127753 0.75682819 0.369163          nan\n",
      " 0.70792952 0.3               nan 0.81718062 0.44405286 0.23127753\n",
      " 0.75506608 0.36828194        nan 0.70748899 0.3               nan\n",
      " 0.81718062 0.44405286 0.23127753 0.75506608 0.36828194        nan\n",
      " 0.70748899 0.3               nan 0.81718062 0.44405286 0.23127753\n",
      " 0.75506608 0.36828194        nan 0.70748899 0.3               nan\n",
      " 0.82819383 0.44405286 0.22995595 0.75859031 0.36563877        nan\n",
      " 0.70264317 0.3               nan 0.82819383 0.44405286 0.22995595\n",
      " 0.75859031 0.36563877        nan 0.70264317 0.3               nan\n",
      " 0.82819383 0.44405286 0.22995595 0.75859031 0.36563877        nan\n",
      " 0.70264317 0.3               nan 0.82643172 0.44405286 0.22995595\n",
      " 0.75903084 0.36696035        nan 0.70132159 0.3               nan\n",
      " 0.82643172 0.44405286 0.22995595 0.75903084 0.36696035        nan\n",
      " 0.70132159 0.3               nan 0.82643172 0.44405286 0.22995595\n",
      " 0.75903084 0.36696035        nan 0.70132159 0.3               nan\n",
      " 0.82731278 0.44405286 0.22995595 0.75947137 0.3660793         nan\n",
      " 0.70132159 0.3               nan 0.82731278 0.44405286 0.22995595\n",
      " 0.75947137 0.3660793         nan 0.70132159 0.3               nan\n",
      " 0.82731278 0.44405286 0.22995595 0.75947137 0.3660793         nan\n",
      " 0.70132159 0.3               nan 0.82290749 0.44361233 0.23127753\n",
      " 0.75770925 0.36784141        nan 0.70528634 0.3               nan\n",
      " 0.82290749 0.44361233 0.23127753 0.75770925 0.36784141        nan\n",
      " 0.70528634 0.3               nan 0.82290749 0.44361233 0.23127753\n",
      " 0.75770925 0.36784141        nan 0.70528634 0.3               nan\n",
      " 0.82334802 0.44361233 0.23127753 0.75859031 0.36740088        nan\n",
      " 0.70484581 0.3               nan 0.82334802 0.44361233 0.23127753\n",
      " 0.75859031 0.36740088        nan 0.70484581 0.3               nan\n",
      " 0.82334802 0.44361233 0.23127753 0.75859031 0.36740088        nan\n",
      " 0.70484581 0.3               nan 0.8246696  0.44361233 0.23127753\n",
      " 0.75638767 0.36740088        nan 0.7061674  0.3               nan\n",
      " 0.8246696  0.44361233 0.23127753 0.75638767 0.36740088        nan\n",
      " 0.7061674  0.3               nan 0.8246696  0.44361233 0.23127753\n",
      " 0.75638767 0.36740088        nan 0.7061674  0.3               nan\n",
      " 0.81982379 0.44405286 0.23127753 0.75682819 0.369163          nan\n",
      " 0.70881057 0.3               nan 0.81982379 0.44405286 0.23127753\n",
      " 0.75682819 0.369163          nan 0.70881057 0.3               nan\n",
      " 0.81982379 0.44405286 0.23127753 0.75682819 0.369163          nan\n",
      " 0.70881057 0.3               nan 0.82114537 0.44361233 0.23127753\n",
      " 0.75903084 0.369163          nan 0.70881057 0.3               nan\n",
      " 0.82114537 0.44361233 0.23127753 0.75903084 0.369163          nan\n",
      " 0.70881057 0.3               nan 0.82114537 0.44361233 0.23127753\n",
      " 0.75903084 0.369163          nan 0.70881057 0.3               nan\n",
      " 0.82070485 0.44361233 0.23127753 0.75859031 0.36828194        nan\n",
      " 0.70837004 0.3               nan 0.82070485 0.44361233 0.23127753\n",
      " 0.75859031 0.36828194        nan 0.70837004 0.3               nan\n",
      " 0.82070485 0.44361233 0.23127753 0.75859031 0.36828194        nan\n",
      " 0.70837004 0.3               nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 729 candidates, totalling 3645 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
      "486 fits failed out of a total of 3645.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "486 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\", line 416, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\", line 370, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/joblib/memory.py\", line 353, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\", line 950, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 2126, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 1396, in fit_transform\n",
      "    X, self.stop_words_ = self._limit_features(\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 1248, in _limit_features\n",
      "    raise ValueError(\n",
      "ValueError: After pruning, no terms remain. Try a lower min_df or a higher max_df.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:976: UserWarning: One or more of the test scores are non-finite: [0.84185022 0.83832599 0.83964758 0.84185022 0.83744493        nan\n",
      " 0.83744493 0.83964758        nan 0.84185022 0.83832599 0.83964758\n",
      " 0.84185022 0.83744493        nan 0.83744493 0.83964758        nan\n",
      " 0.84185022 0.83832599 0.83964758 0.84185022 0.83744493        nan\n",
      " 0.83744493 0.83964758        nan 0.84229075 0.83832599 0.83964758\n",
      " 0.84096916 0.83744493        nan 0.83832599 0.83964758        nan\n",
      " 0.84229075 0.83832599 0.83964758 0.84096916 0.83744493        nan\n",
      " 0.83832599 0.83964758        nan 0.84229075 0.83832599 0.83964758\n",
      " 0.84096916 0.83744493        nan 0.83832599 0.83964758        nan\n",
      " 0.84273128 0.83832599 0.83964758 0.84185022 0.83744493        nan\n",
      " 0.83656388 0.83964758        nan 0.84273128 0.83832599 0.83964758\n",
      " 0.84185022 0.83744493        nan 0.83656388 0.83964758        nan\n",
      " 0.84273128 0.83832599 0.83964758 0.84185022 0.83744493        nan\n",
      " 0.83656388 0.83964758        nan 0.84096916 0.83832599 0.83964758\n",
      " 0.83964758 0.83744493        nan 0.83480176 0.83964758        nan\n",
      " 0.84096916 0.83832599 0.83964758 0.83964758 0.83744493        nan\n",
      " 0.83480176 0.83964758        nan 0.84096916 0.83832599 0.83964758\n",
      " 0.83964758 0.83744493        nan 0.83480176 0.83964758        nan\n",
      " 0.84229075 0.83832599 0.83964758 0.83876652 0.83744493        nan\n",
      " 0.83524229 0.83964758        nan 0.84229075 0.83832599 0.83964758\n",
      " 0.83876652 0.83744493        nan 0.83524229 0.83964758        nan\n",
      " 0.84229075 0.83832599 0.83964758 0.83876652 0.83744493        nan\n",
      " 0.83524229 0.83964758        nan 0.84229075 0.83832599 0.83964758\n",
      " 0.84096916 0.83744493        nan 0.83436123 0.83964758        nan\n",
      " 0.84229075 0.83832599 0.83964758 0.84096916 0.83744493        nan\n",
      " 0.83436123 0.83964758        nan 0.84229075 0.83832599 0.83964758\n",
      " 0.84096916 0.83744493        nan 0.83436123 0.83964758        nan\n",
      " 0.84185022 0.83832599 0.83964758 0.83920705 0.83744493        nan\n",
      " 0.83568282 0.83964758        nan 0.84185022 0.83832599 0.83964758\n",
      " 0.83920705 0.83744493        nan 0.83568282 0.83964758        nan\n",
      " 0.84185022 0.83832599 0.83964758 0.83920705 0.83744493        nan\n",
      " 0.83568282 0.83964758        nan 0.84229075 0.83832599 0.83964758\n",
      " 0.83964758 0.83744493        nan 0.83568282 0.83964758        nan\n",
      " 0.84229075 0.83832599 0.83964758 0.83964758 0.83744493        nan\n",
      " 0.83568282 0.83964758        nan 0.84229075 0.83832599 0.83964758\n",
      " 0.83964758 0.83744493        nan 0.83568282 0.83964758        nan\n",
      " 0.84052863 0.83832599 0.83964758 0.83964758 0.83744493        nan\n",
      " 0.83568282 0.83964758        nan 0.84052863 0.83832599 0.83964758\n",
      " 0.83964758 0.83744493        nan 0.83568282 0.83964758        nan\n",
      " 0.84052863 0.83832599 0.83964758 0.83964758 0.83744493        nan\n",
      " 0.83568282 0.83964758        nan 0.81894273 0.43348018 0.23215859\n",
      " 0.76387665 0.34757709        nan 0.69955947 0.28678414        nan\n",
      " 0.81894273 0.43348018 0.23215859 0.76387665 0.34757709        nan\n",
      " 0.69955947 0.28678414        nan 0.81894273 0.43348018 0.23215859\n",
      " 0.76387665 0.34757709        nan 0.69955947 0.28678414        nan\n",
      " 0.81894273 0.43348018 0.23215859 0.76387665 0.34669604        nan\n",
      " 0.7        0.28678414        nan 0.81894273 0.43348018 0.23215859\n",
      " 0.76387665 0.34669604        nan 0.7        0.28678414        nan\n",
      " 0.81894273 0.43348018 0.23215859 0.76387665 0.34669604        nan\n",
      " 0.7        0.28678414        nan 0.81894273 0.43348018 0.23215859\n",
      " 0.76343612 0.34669604        nan 0.69867841 0.28678414        nan\n",
      " 0.81894273 0.43348018 0.23215859 0.76343612 0.34669604        nan\n",
      " 0.69867841 0.28678414        nan 0.81894273 0.43348018 0.23215859\n",
      " 0.76343612 0.34669604        nan 0.69867841 0.28678414        nan\n",
      " 0.8123348  0.43348018 0.23215859 0.75726872 0.34757709        nan\n",
      " 0.70792952 0.28678414        nan 0.8123348  0.43348018 0.23215859\n",
      " 0.75726872 0.34757709        nan 0.70792952 0.28678414        nan\n",
      " 0.8123348  0.43348018 0.23215859 0.75726872 0.34757709        nan\n",
      " 0.70792952 0.28678414        nan 0.81365639 0.43348018 0.23215859\n",
      " 0.75682819 0.34757709        nan 0.7092511  0.28678414        nan\n",
      " 0.81365639 0.43348018 0.23215859 0.75682819 0.34757709        nan\n",
      " 0.7092511  0.28678414        nan 0.81365639 0.43348018 0.23215859\n",
      " 0.75682819 0.34757709        nan 0.7092511  0.28678414        nan\n",
      " 0.81409692 0.43348018 0.23215859 0.75682819 0.34757709        nan\n",
      " 0.70837004 0.28678414        nan 0.81409692 0.43348018 0.23215859\n",
      " 0.75682819 0.34757709        nan 0.70837004 0.28678414        nan\n",
      " 0.81409692 0.43348018 0.23215859 0.75682819 0.34757709        nan\n",
      " 0.70837004 0.28678414        nan 0.80837004 0.43348018 0.23215859\n",
      " 0.75110132 0.34845815        nan 0.70881057 0.28678414        nan\n",
      " 0.80837004 0.43348018 0.23215859 0.75110132 0.34845815        nan\n",
      " 0.70881057 0.28678414        nan 0.80837004 0.43348018 0.23215859\n",
      " 0.75110132 0.34845815        nan 0.70881057 0.28678414        nan\n",
      " 0.80660793 0.43348018 0.23215859 0.74933921 0.34845815        nan\n",
      " 0.70881057 0.28678414        nan 0.80660793 0.43348018 0.23215859\n",
      " 0.74933921 0.34845815        nan 0.70881057 0.28678414        nan\n",
      " 0.80660793 0.43348018 0.23215859 0.74933921 0.34845815        nan\n",
      " 0.70881057 0.28678414        nan 0.80748899 0.43348018 0.23215859\n",
      " 0.75198238 0.34845815        nan 0.70660793 0.28678414        nan\n",
      " 0.80748899 0.43348018 0.23215859 0.75198238 0.34845815        nan\n",
      " 0.70660793 0.28678414        nan 0.80748899 0.43348018 0.23215859\n",
      " 0.75198238 0.34845815        nan 0.70660793 0.28678414        nan\n",
      " 0.8215859  0.43348018 0.23215859 0.76431718 0.34669604        nan\n",
      " 0.69867841 0.28678414        nan 0.8215859  0.43348018 0.23215859\n",
      " 0.76431718 0.34669604        nan 0.69867841 0.28678414        nan\n",
      " 0.8215859  0.43348018 0.23215859 0.76431718 0.34669604        nan\n",
      " 0.69867841 0.28678414        nan 0.82070485 0.43348018 0.23215859\n",
      " 0.76299559 0.34669604        nan 0.69955947 0.28678414        nan\n",
      " 0.82070485 0.43348018 0.23215859 0.76299559 0.34669604        nan\n",
      " 0.69955947 0.28678414        nan 0.82070485 0.43348018 0.23215859\n",
      " 0.76299559 0.34669604        nan 0.69955947 0.28678414        nan\n",
      " 0.81938326 0.43348018 0.23215859 0.76343612 0.34669604        nan\n",
      " 0.69911894 0.28678414        nan 0.81938326 0.43348018 0.23215859\n",
      " 0.76343612 0.34669604        nan 0.69911894 0.28678414        nan\n",
      " 0.81938326 0.43348018 0.23215859 0.76343612 0.34669604        nan\n",
      " 0.69911894 0.28678414        nan 0.81101322 0.43348018 0.23215859\n",
      " 0.75770925 0.34757709        nan 0.70484581 0.28678414        nan\n",
      " 0.81101322 0.43348018 0.23215859 0.75770925 0.34757709        nan\n",
      " 0.70484581 0.28678414        nan 0.81101322 0.43348018 0.23215859\n",
      " 0.75770925 0.34757709        nan 0.70484581 0.28678414        nan\n",
      " 0.81057269 0.43348018 0.23215859 0.75726872 0.34757709        nan\n",
      " 0.70572687 0.28678414        nan 0.81057269 0.43348018 0.23215859\n",
      " 0.75726872 0.34757709        nan 0.70572687 0.28678414        nan\n",
      " 0.81057269 0.43348018 0.23215859 0.75726872 0.34757709        nan\n",
      " 0.70572687 0.28678414        nan 0.81145374 0.43348018 0.23215859\n",
      " 0.75682819 0.34757709        nan 0.70396476 0.28678414        nan\n",
      " 0.81145374 0.43348018 0.23215859 0.75682819 0.34757709        nan\n",
      " 0.70396476 0.28678414        nan 0.81145374 0.43348018 0.23215859\n",
      " 0.75682819 0.34757709        nan 0.70396476 0.28678414        nan\n",
      " 0.80660793 0.43348018 0.23215859 0.75462555 0.34845815        nan\n",
      " 0.70352423 0.28678414        nan 0.80660793 0.43348018 0.23215859\n",
      " 0.75462555 0.34845815        nan 0.70352423 0.28678414        nan\n",
      " 0.80660793 0.43348018 0.23215859 0.75462555 0.34845815        nan\n",
      " 0.70352423 0.28678414        nan 0.80660793 0.43348018 0.23215859\n",
      " 0.75638767 0.34845815        nan 0.70528634 0.28678414        nan\n",
      " 0.80660793 0.43348018 0.23215859 0.75638767 0.34845815        nan\n",
      " 0.70528634 0.28678414        nan 0.80660793 0.43348018 0.23215859\n",
      " 0.75638767 0.34845815        nan 0.70528634 0.28678414        nan\n",
      " 0.80484581 0.43348018 0.23215859 0.75594714 0.34845815        nan\n",
      " 0.70484581 0.28678414        nan 0.80484581 0.43348018 0.23215859\n",
      " 0.75594714 0.34845815        nan 0.70484581 0.28678414        nan\n",
      " 0.80484581 0.43348018 0.23215859 0.75594714 0.34845815        nan\n",
      " 0.70484581 0.28678414        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 729 candidates, totalling 3645 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
      "567 fits failed out of a total of 3645.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "567 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\", line 416, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\", line 370, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/joblib/memory.py\", line 353, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\", line 950, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 2126, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 1396, in fit_transform\n",
      "    X, self.stop_words_ = self._limit_features(\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 1248, in _limit_features\n",
      "    raise ValueError(\n",
      "ValueError: After pruning, no terms remain. Try a lower min_df or a higher max_df.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:976: UserWarning: One or more of the test scores are non-finite: [0.84367914 0.83663746 0.8388372  0.8388401  0.83927869        nan\n",
      " 0.83443385 0.83971826        nan 0.84367914 0.83663746 0.8388372\n",
      " 0.8388401  0.83927869        nan 0.83443385 0.83971826        nan\n",
      " 0.84367914 0.83663746 0.8388372  0.8388401  0.83927869        nan\n",
      " 0.83443385 0.83971826        nan 0.84368011 0.83663746 0.8388372\n",
      " 0.84016072 0.83927869        nan 0.83487341 0.83971826        nan\n",
      " 0.84368011 0.83663746 0.8388372  0.84016072 0.83927869        nan\n",
      " 0.83487341 0.83971826        nan 0.84368011 0.83663746 0.8388372\n",
      " 0.84016072 0.83927869        nan 0.83487341 0.83971826        nan\n",
      " 0.84368011 0.83663746 0.8388372  0.83928063 0.83927869        nan\n",
      " 0.83487341 0.83971826        nan 0.84368011 0.83663746 0.8388372\n",
      " 0.83928063 0.83927869        nan 0.83487341 0.83971826        nan\n",
      " 0.84368011 0.83663746 0.8388372  0.83928063 0.83927869        nan\n",
      " 0.83487341 0.83971826        nan 0.83927676 0.83707799 0.8388372\n",
      " 0.83795905 0.83927869        nan 0.83355182 0.83971826        nan\n",
      " 0.83927676 0.83707799 0.8388372  0.83795905 0.83927869        nan\n",
      " 0.83355182 0.83971826        nan 0.83927676 0.83707799 0.8388372\n",
      " 0.83795905 0.83927869        nan 0.83355182 0.83971826        nan\n",
      " 0.83883623 0.83707799 0.8388372  0.83795905 0.83927869        nan\n",
      " 0.83267077 0.83971826        nan 0.83883623 0.83707799 0.8388372\n",
      " 0.83795905 0.83927869        nan 0.83267077 0.83971826        nan\n",
      " 0.83883623 0.83707799 0.8388372  0.83795905 0.83927869        nan\n",
      " 0.83267077 0.83971826        nan 0.83795517 0.83707799 0.8388372\n",
      " 0.83707799 0.83927869        nan 0.83399235 0.83971826        nan\n",
      " 0.83795517 0.83707799 0.8388372  0.83707799 0.83927869        nan\n",
      " 0.83399235 0.83971826        nan 0.83795517 0.83707799 0.8388372\n",
      " 0.83707799 0.83927869        nan 0.83399235 0.83971826        nan\n",
      " 0.84147843 0.83707799 0.8388372  0.83751755 0.83927869        nan\n",
      " 0.83355279 0.83971826        nan 0.84147843 0.83707799 0.8388372\n",
      " 0.83751755 0.83927869        nan 0.83355279 0.83971826        nan\n",
      " 0.84147843 0.83707799 0.8388372  0.83751755 0.83927869        nan\n",
      " 0.83355279 0.83971826        nan 0.84147843 0.83707799 0.8388372\n",
      " 0.83795711 0.83927869        nan 0.83443385 0.83971826        nan\n",
      " 0.84147843 0.83707799 0.8388372  0.83795711 0.83927869        nan\n",
      " 0.83443385 0.83971826        nan 0.84147843 0.83707799 0.8388372\n",
      " 0.83795711 0.83927869        nan 0.83443385 0.83971826        nan\n",
      " 0.84059738 0.83707799 0.8388372  0.83883817 0.83927869        nan\n",
      " 0.83443385 0.83971826        nan 0.84059738 0.83707799 0.8388372\n",
      " 0.83883817 0.83927869        nan 0.83443385 0.83971826        nan\n",
      " 0.84059738 0.83707799 0.8388372  0.83883817 0.83927869        nan\n",
      " 0.83443385 0.83971826        nan 0.814612   0.43858353 0.23161543\n",
      " 0.75428571 0.34875732        nan 0.70012683 0.29635087        nan\n",
      " 0.814612   0.43858353 0.23161543 0.75428571 0.34875732        nan\n",
      " 0.70012683 0.29635087        nan 0.814612   0.43858353 0.23161543\n",
      " 0.75428571 0.34875732        nan 0.70012683 0.29635087        nan\n",
      " 0.81549305 0.43858353 0.23161543 0.75296606 0.34875732        nan\n",
      " 0.70321053 0.29635087        nan 0.81549305 0.43858353 0.23161543\n",
      " 0.75296606 0.34875732        nan 0.70321053 0.29635087        nan\n",
      " 0.81549305 0.43858353 0.23161543 0.75296606 0.34875732        nan\n",
      " 0.70321053 0.29635087        nan 0.81637121 0.43902406 0.23161543\n",
      " 0.7525236  0.34875732        nan 0.70144842 0.29635087        nan\n",
      " 0.81637121 0.43902406 0.23161543 0.7525236  0.34875732        nan\n",
      " 0.70144842 0.29635087        nan 0.81637121 0.43902406 0.23161543\n",
      " 0.7525236  0.34875732        nan 0.70144842 0.29635087        nan\n",
      " 0.80756838 0.43858353 0.23161543 0.7520792  0.34875732        nan\n",
      " 0.70100886 0.29635087        nan 0.80756838 0.43858353 0.23161543\n",
      " 0.7520792  0.34875732        nan 0.70100886 0.29635087        nan\n",
      " 0.80756838 0.43858353 0.23161543 0.7520792  0.34875732        nan\n",
      " 0.70100886 0.29635087        nan 0.80888996 0.43902406 0.23161543\n",
      " 0.75119814 0.34875732        nan 0.70276904 0.29635087        nan\n",
      " 0.80888996 0.43902406 0.23161543 0.75119814 0.34875732        nan\n",
      " 0.70276904 0.29635087        nan 0.80888996 0.43902406 0.23161543\n",
      " 0.75119814 0.34875732        nan 0.70276904 0.29635087        nan\n",
      " 0.80888996 0.438143   0.23161543 0.7511962  0.34875732        nan\n",
      " 0.70321053 0.29635087        nan 0.80888996 0.438143   0.23161543\n",
      " 0.7511962  0.34875732        nan 0.70321053 0.29635087        nan\n",
      " 0.80888996 0.438143   0.23161543 0.7511962  0.34875732        nan\n",
      " 0.70321053 0.29635087        nan 0.80227913 0.43858353 0.23161543\n",
      " 0.75120008 0.34875732        nan 0.70453018 0.29635087        nan\n",
      " 0.80227913 0.43858353 0.23161543 0.75120008 0.34875732        nan\n",
      " 0.70453018 0.29635087        nan 0.80227913 0.43858353 0.23161543\n",
      " 0.75120008 0.34875732        nan 0.70453018 0.29635087        nan\n",
      " 0.8049223  0.43946459 0.23161543 0.75208113 0.34875732        nan\n",
      " 0.7032086  0.29635087        nan 0.8049223  0.43946459 0.23161543\n",
      " 0.75208113 0.34875732        nan 0.7032086  0.29635087        nan\n",
      " 0.8049223  0.43946459 0.23161543 0.75208113 0.34875732        nan\n",
      " 0.7032086  0.29635087        nan 0.80448177 0.43946459 0.23161543\n",
      " 0.74987752 0.34875732        nan 0.70188701 0.29635087        nan\n",
      " 0.80448177 0.43946459 0.23161543 0.74987752 0.34875732        nan\n",
      " 0.70188701 0.29635087        nan 0.80448177 0.43946459 0.23161543\n",
      " 0.74987752 0.34875732        nan 0.70188701 0.29635087        nan\n",
      " 0.81505543 0.43858353 0.23161543 0.75296606 0.34875732        nan\n",
      " 0.6996863  0.29635087        nan 0.81505543 0.43858353 0.23161543\n",
      " 0.75296606 0.34875732        nan 0.6996863  0.29635087        nan\n",
      " 0.81505543 0.43858353 0.23161543 0.75296606 0.34875732        nan\n",
      " 0.6996863  0.29635087        nan 0.81549596 0.43285666 0.23161543\n",
      " 0.75340563 0.34875732        nan 0.70012683 0.29635087        nan\n",
      " 0.81549596 0.43285666 0.23161543 0.75340563 0.34875732        nan\n",
      " 0.70012683 0.29635087        nan 0.81549596 0.43285666 0.23161543\n",
      " 0.75340563 0.34875732        nan 0.70012683 0.29635087        nan\n",
      " 0.8141734  0.43329719 0.23161543 0.75428475 0.34875732        nan\n",
      " 0.69924674 0.29635087        nan 0.8141734  0.43329719 0.23161543\n",
      " 0.75428475 0.34875732        nan 0.69924674 0.29635087        nan\n",
      " 0.8141734  0.43329719 0.23161543 0.75428475 0.34875732        nan\n",
      " 0.69924674 0.29635087        nan 0.80801278 0.43902406 0.23161543\n",
      " 0.75295929 0.34875732        nan 0.69924578 0.29635087        nan\n",
      " 0.80801278 0.43902406 0.23161543 0.75295929 0.34875732        nan\n",
      " 0.69924578 0.29635087        nan 0.80801278 0.43902406 0.23161543\n",
      " 0.75295929 0.34875732        nan 0.69924578 0.29635087        nan\n",
      " 0.80889287 0.4319756  0.23161543 0.75251876 0.34875732        nan\n",
      " 0.70056543 0.29635087        nan 0.80889287 0.4319756  0.23161543\n",
      " 0.75251876 0.34875732        nan 0.70056543 0.29635087        nan\n",
      " 0.80889287 0.4319756  0.23161543 0.75251876 0.34875732        nan\n",
      " 0.70056543 0.29635087        nan 0.80801084 0.43241613 0.23161543\n",
      " 0.75295929 0.34875732        nan 0.69968534 0.29635087        nan\n",
      " 0.80801084 0.43241613 0.23161543 0.75295929 0.34875732        nan\n",
      " 0.69968534 0.29635087        nan 0.80801084 0.43241613 0.23161543\n",
      " 0.75295929 0.34875732        nan 0.69968534 0.29635087        nan\n",
      " 0.79963886 0.43902406 0.23161543 0.74899259 0.34875732        nan\n",
      " 0.70320957 0.29635087        nan 0.79963886 0.43902406 0.23161543\n",
      " 0.74899259 0.34875732        nan 0.70320957 0.29635087        nan\n",
      " 0.79963886 0.43902406 0.23161543 0.74899259 0.34875732        nan\n",
      " 0.70320957 0.29635087        nan 0.80095948 0.43153507 0.23161543\n",
      " 0.74767391 0.34875732        nan 0.70100886 0.29635087        nan\n",
      " 0.80095948 0.43153507 0.23161543 0.74767391 0.34875732        nan\n",
      " 0.70100886 0.29635087        nan 0.80095948 0.43153507 0.23161543\n",
      " 0.74767391 0.34875732        nan 0.70100886 0.29635087        nan\n",
      " 0.80316116 0.43241613 0.23161543 0.74855594 0.34875732        nan\n",
      " 0.70144842 0.29635087        nan 0.80316116 0.43241613 0.23161543\n",
      " 0.74855594 0.34875732        nan 0.70144842 0.29635087        nan\n",
      " 0.80316116 0.43241613 0.23161543 0.74855594 0.34875732        nan\n",
      " 0.70144842 0.29635087        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 729 candidates, totalling 3645 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
      "405 fits failed out of a total of 3645.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "405 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\", line 416, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\", line 370, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/joblib/memory.py\", line 353, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\", line 950, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 2126, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 1396, in fit_transform\n",
      "    X, self.stop_words_ = self._limit_features(\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 1248, in _limit_features\n",
      "    raise ValueError(\n",
      "ValueError: After pruning, no terms remain. Try a lower min_df or a higher max_df.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:976: UserWarning: One or more of the test scores are non-finite: [0.84324055 0.83839861 0.83971826 0.83884107 0.83883913 0.83971826\n",
      " 0.83223024 0.83971826        nan 0.84324055 0.83839861 0.83971826\n",
      " 0.83884107 0.83883913 0.83971826 0.83223024 0.83971826        nan\n",
      " 0.84324055 0.83839861 0.83971826 0.83884107 0.83883913 0.83971826\n",
      " 0.83223024 0.83971826        nan 0.84236143 0.83839861 0.83971826\n",
      " 0.83840151 0.83883913 0.83971826 0.83267077 0.83971826        nan\n",
      " 0.84236143 0.83839861 0.83971826 0.83840151 0.83883913 0.83971826\n",
      " 0.83267077 0.83971826        nan 0.84236143 0.83839861 0.83971826\n",
      " 0.83840151 0.83883913 0.83971826 0.83267077 0.83971826        nan\n",
      " 0.84324152 0.83839861 0.83971826 0.83840151 0.83883913 0.83971826\n",
      " 0.83311129 0.83971826        nan 0.84324152 0.83839861 0.83971826\n",
      " 0.83840151 0.83883913 0.83971826 0.83311129 0.83971826        nan\n",
      " 0.84324152 0.83839861 0.83971826 0.83840151 0.83883913 0.83971826\n",
      " 0.83311129 0.83971826        nan 0.84191896 0.83839861 0.83971826\n",
      " 0.83531781 0.83883913 0.83971826 0.83355182 0.83971826        nan\n",
      " 0.84191896 0.83839861 0.83971826 0.83531781 0.83883913 0.83971826\n",
      " 0.83355182 0.83971826        nan 0.84191896 0.83839861 0.83971826\n",
      " 0.83531781 0.83883913 0.83971826 0.83355182 0.83971826        nan\n",
      " 0.84147843 0.83839861 0.83971826 0.83443675 0.83883913 0.83971826\n",
      " 0.83311129 0.83971826        nan 0.84147843 0.83839861 0.83971826\n",
      " 0.83443675 0.83883913 0.83971826 0.83311129 0.83971826        nan\n",
      " 0.84147843 0.83839861 0.83971826 0.83443675 0.83883913 0.83971826\n",
      " 0.83311129 0.83971826        nan 0.84103887 0.83839861 0.83971826\n",
      " 0.83443675 0.83883913 0.83971826 0.83399138 0.83971826        nan\n",
      " 0.84103887 0.83839861 0.83971826 0.83443675 0.83883913 0.83971826\n",
      " 0.83399138 0.83971826        nan 0.84103887 0.83839861 0.83971826\n",
      " 0.83443675 0.83883913 0.83971826 0.83399138 0.83971826        nan\n",
      " 0.84279711 0.83839861 0.83971826 0.83575543 0.83883913 0.83971826\n",
      " 0.83443094 0.83971826        nan 0.84279711 0.83839861 0.83971826\n",
      " 0.83575543 0.83883913 0.83971826 0.83443094 0.83971826        nan\n",
      " 0.84279711 0.83839861 0.83971826 0.83575543 0.83883913 0.83971826\n",
      " 0.83443094 0.83971826        nan 0.84103597 0.83839861 0.83971826\n",
      " 0.8357564  0.83883913 0.83971826 0.83443094 0.83971826        nan\n",
      " 0.84103597 0.83839861 0.83971826 0.8357564  0.83883913 0.83971826\n",
      " 0.83443094 0.83971826        nan 0.84103597 0.83839861 0.83971826\n",
      " 0.8357564  0.83883913 0.83971826 0.83443094 0.83971826        nan\n",
      " 0.84147747 0.83839861 0.83971826 0.83619693 0.83883913 0.83971826\n",
      " 0.83443094 0.83971826        nan 0.84147747 0.83839861 0.83971826\n",
      " 0.83619693 0.83883913 0.83971826 0.83443094 0.83971826        nan\n",
      " 0.84147747 0.83839861 0.83971826 0.83619693 0.83883913 0.83971826\n",
      " 0.83443094 0.83971826        nan 0.82165949 0.42758387 0.22897517\n",
      " 0.76441497 0.32937793 0.30917945 0.7080457  0.29061722        nan\n",
      " 0.82165949 0.42758387 0.22897517 0.76441497 0.32937793 0.30917945\n",
      " 0.7080457  0.29061722        nan 0.82165949 0.42758387 0.22897517\n",
      " 0.76441497 0.32937793 0.30917945 0.7080457  0.29061722        nan\n",
      " 0.8234216  0.42978264 0.22897517 0.76485743 0.32981846 0.30917945\n",
      " 0.70804473 0.29061722        nan 0.8234216  0.42978264 0.22897517\n",
      " 0.76485743 0.32981846 0.30917945 0.70804473 0.29061722        nan\n",
      " 0.8234216  0.42978264 0.22897517 0.76485743 0.32981846 0.30917945\n",
      " 0.70804473 0.29061722        nan 0.82210098 0.42978264 0.22897517\n",
      " 0.76617708 0.32981846 0.17702086 0.70848623 0.29061722        nan\n",
      " 0.82210098 0.42978264 0.22897517 0.76617708 0.32981846 0.17702086\n",
      " 0.70848623 0.29061722        nan 0.82210098 0.42978264 0.22897517\n",
      " 0.76617708 0.32981846 0.17702086 0.70848623 0.29061722        nan\n",
      " 0.81153023 0.42934211 0.23029675 0.75780994 0.33069662 0.30917945\n",
      " 0.70496587 0.29061722        nan 0.81153023 0.42934211 0.23029675\n",
      " 0.75780994 0.33069662 0.30917945 0.70496587 0.29061722        nan\n",
      " 0.81153023 0.42934211 0.23029675 0.75780994 0.33069662 0.30917945\n",
      " 0.70496587 0.29061722        nan 0.81064917 0.43022317 0.23117781\n",
      " 0.75693082 0.33113714 0.30917945 0.70716851 0.29061722        nan\n",
      " 0.81064917 0.43022317 0.23117781 0.75693082 0.33113714 0.30917945\n",
      " 0.70716851 0.29061722        nan 0.81064917 0.43022317 0.23117781\n",
      " 0.75693082 0.33113714 0.30917945 0.70716851 0.29061722        nan\n",
      " 0.81065014 0.43022317 0.23029675 0.75913347 0.33245873 0.17702086\n",
      " 0.70584693 0.29061722        nan 0.81065014 0.43022317 0.23029675\n",
      " 0.75913347 0.33245873 0.17702086 0.70584693 0.29061722        nan\n",
      " 0.81065014 0.43022317 0.23029675 0.75913347 0.33245873 0.17702086\n",
      " 0.70584693 0.29061722        nan 0.81417437 0.42890158 0.23029675\n",
      " 0.7525236  0.33245873 0.30917945 0.70320472 0.29061722        nan\n",
      " 0.81417437 0.42890158 0.23029675 0.7525236  0.33245873 0.30917945\n",
      " 0.70320472 0.29061722        nan 0.81417437 0.42890158 0.23029675\n",
      " 0.7525236  0.33245873 0.30917945 0.70320472 0.29061722        nan\n",
      " 0.81065014 0.42934211 0.23029675 0.75208501 0.33289926 0.30917945\n",
      " 0.70100305 0.29061722        nan 0.81065014 0.42934211 0.23029675\n",
      " 0.75208501 0.33289926 0.30917945 0.70100305 0.29061722        nan\n",
      " 0.81065014 0.42934211 0.23029675 0.75208501 0.33289926 0.30917945\n",
      " 0.70100305 0.29061722        nan 0.80977005 0.42934211 0.23029675\n",
      " 0.75120492 0.33289926 0.17702086 0.70188314 0.29061722        nan\n",
      " 0.80977005 0.42934211 0.23029675 0.75120492 0.33289926 0.17702086\n",
      " 0.70188314 0.29061722        nan 0.80977005 0.42934211 0.23029675\n",
      " 0.75120492 0.33289926 0.17702086 0.70188314 0.29061722        nan\n",
      " 0.8238631  0.42934211 0.22897517 0.76265673 0.32937793 0.17702086\n",
      " 0.71200368 0.29061722        nan 0.8238631  0.42934211 0.22897517\n",
      " 0.76265673 0.32937793 0.17702086 0.71200368 0.29061722        nan\n",
      " 0.8238631  0.42934211 0.22897517 0.76265673 0.32937793 0.17702086\n",
      " 0.71200368 0.29061722        nan 0.82518371 0.43022317 0.22897517\n",
      " 0.7622162  0.32981846 0.17702086 0.710685   0.29061722        nan\n",
      " 0.82518371 0.43022317 0.22897517 0.7622162  0.32981846 0.17702086\n",
      " 0.710685   0.29061722        nan 0.82518371 0.43022317 0.22897517\n",
      " 0.7622162  0.32981846 0.17702086 0.710685   0.29061722        nan\n",
      " 0.82474222 0.42978264 0.22897517 0.76397831 0.32981846 0.17702086\n",
      " 0.71112456 0.29061722        nan 0.82474222 0.42978264 0.22897517\n",
      " 0.76397831 0.32981846 0.17702086 0.71112456 0.29061722        nan\n",
      " 0.82474222 0.42978264 0.22897517 0.76397831 0.32981846 0.17702086\n",
      " 0.71112456 0.29061722        nan 0.81505543 0.42934211 0.23029675\n",
      " 0.75517064 0.33069662 0.17702086 0.7058479  0.29061722        nan\n",
      " 0.81505543 0.42934211 0.23029675 0.75517064 0.33069662 0.17702086\n",
      " 0.7058479  0.29061722        nan 0.81505543 0.42934211 0.23029675\n",
      " 0.75517064 0.33069662 0.17702086 0.7058479  0.29061722        nan\n",
      " 0.81373481 0.43022317 0.23117781 0.7564932  0.33113714 0.17702086\n",
      " 0.7058479  0.29061722        nan 0.81373481 0.43022317 0.23117781\n",
      " 0.7564932  0.33113714 0.17702086 0.7058479  0.29061722        nan\n",
      " 0.81373481 0.43022317 0.23117781 0.7564932  0.33113714 0.17702086\n",
      " 0.7058479  0.29061722        nan 0.81329525 0.43022317 0.23029675\n",
      " 0.75693276 0.33245873 0.17702086 0.70672895 0.29061722        nan\n",
      " 0.81329525 0.43022317 0.23029675 0.75693276 0.33245873 0.17702086\n",
      " 0.70672895 0.29061722        nan 0.81329525 0.43022317 0.23029675\n",
      " 0.75693276 0.33245873 0.17702086 0.70672895 0.29061722        nan\n",
      " 0.80977102 0.42890158 0.23029675 0.75208694 0.33245873 0.17702086\n",
      " 0.70408578 0.29061722        nan 0.80977102 0.42890158 0.23029675\n",
      " 0.75208694 0.33245873 0.17702086 0.70408578 0.29061722        nan\n",
      " 0.80977102 0.42890158 0.23029675 0.75208694 0.33245873 0.17702086\n",
      " 0.70408578 0.29061722        nan 0.81065208 0.42934211 0.23029675\n",
      " 0.75164545 0.33289926 0.17702086 0.7027642  0.29061722        nan\n",
      " 0.81065208 0.42934211 0.23029675 0.75164545 0.33289926 0.17702086\n",
      " 0.7027642  0.29061722        nan 0.81065208 0.42934211 0.23029675\n",
      " 0.75164545 0.33289926 0.17702086 0.7027642  0.29061722        nan\n",
      " 0.81021155 0.42934211 0.23029675 0.75208598 0.33289926 0.17702086\n",
      " 0.70364525 0.29061722        nan 0.81021155 0.42934211 0.23029675\n",
      " 0.75208598 0.33289926 0.17702086 0.70364525 0.29061722        nan\n",
      " 0.81021155 0.42934211 0.23029675 0.75208598 0.33289926 0.17702086\n",
      " 0.70364525 0.29061722        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.84859155, 0.84507042, 0.84859155, 0.85890653, 0.85185185])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_log_pipe = Pipeline([\n",
    "    ('tf', TfidfVectorizer(min_df=.01, max_df=.9, stop_words=stop_words)),\n",
    "    ('lr', LogisticRegression(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8352112676056338"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_log_pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred_log = bin_log_pipe.predict(X_test)\n",
    "log_test_acc = accuracy_score(y_test, y_pred_log)\n",
    "log_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fd7196c1880>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cfm_log = confusion_matrix(y_test, y_pred_log)\n",
    "ConfusionMatrixDisplay(cfm_log).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.83978873, 0.84683099, 0.83978873, 0.84479718, 0.84303351])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_score = cross_val_score(bin_log_pipe, X_train, y_train, cv=5)\n",
    "log_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_log_pipe_grid = Pipeline([\n",
    "    ('tf', TfidfVectorizer(stop_words=stop_words)),\n",
    "    ('lr', LogisticRegression(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_param_grid = {\n",
    "    'tf__min_df': [.01, .02, .03],\n",
    "    'tf__max_df': [.88, .90, .92],\n",
    "    'tf__ngram_range': [(1,1), (2,2), (3,3)],\n",
    "    'lr__penalty': [None, 'l2', 'l1'],\n",
    "    'lr__class_weight': [None, 'balanced'],\n",
    "    'lr__solver': ['lbfgs', 'liblinear'],\n",
    "    'lr__max_iter': [10, 15, 20]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 972 candidates, totalling 4860 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
      "1620 fits failed out of a total of 4860.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "810 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\", line 420, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1227, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py\", line 1221, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "810 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\", line 420, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:976: UserWarning: One or more of the test scores are non-finite: [0.83721651 0.83263656 0.83087538 0.8336842  0.83227575 0.83192364\n",
      " 0.83897148 0.83897148 0.83897148 0.83721651 0.83263656 0.83087538\n",
      " 0.8336842  0.83227575 0.83192364 0.83897148 0.83897148 0.83897148\n",
      " 0.83721651 0.83263656 0.83087538 0.8336842  0.83227575 0.83192364\n",
      " 0.83897148 0.83897148 0.83897148        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84073267 0.84038118 0.84038118 0.84002658 0.83967447 0.83967447\n",
      " 0.83967571 0.83967571 0.83967571 0.84073267 0.84038118 0.84038118\n",
      " 0.84002658 0.83967447 0.83967447 0.83967571 0.83967571 0.83967571\n",
      " 0.84073267 0.84038118 0.84038118 0.84002658 0.83967447 0.83967447\n",
      " 0.83967571 0.83967571 0.83967571 0.84319994 0.84249509 0.84178963\n",
      " 0.83967447 0.8400272  0.8400272  0.83967571 0.83967571 0.83967571\n",
      " 0.84319994 0.84249509 0.84178963 0.83967447 0.8400272  0.8400272\n",
      " 0.83967571 0.83967571 0.83967571 0.84319994 0.84249509 0.84178963\n",
      " 0.83967447 0.8400272  0.8400272  0.83967571 0.83967571 0.83967571\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.84425566 0.84249323 0.84249323\n",
      " 0.83932235 0.83932235 0.83932235 0.83967571 0.83967571 0.83967571\n",
      " 0.84425566 0.84249323 0.84249323 0.83932235 0.83932235 0.83932235\n",
      " 0.83967571 0.83967571 0.83967571 0.84425566 0.84249323 0.84249323\n",
      " 0.83932235 0.83932235 0.83932235 0.83967571 0.83967571 0.83967571\n",
      " 0.83404501 0.83369476 0.8326347  0.83297936 0.83227389 0.83227327\n",
      " 0.83826539 0.83861751 0.83861751 0.83404501 0.83369476 0.8326347\n",
      " 0.83297936 0.83227389 0.83227327 0.83826539 0.83861751 0.83861751\n",
      " 0.83404501 0.83369476 0.8326347  0.83297936 0.83227389 0.83227327\n",
      " 0.83826539 0.83861751 0.83861751        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84284845 0.84320056 0.84355267 0.83967447 0.83967447 0.83967447\n",
      " 0.83967571 0.83967571 0.83967571 0.84284845 0.84320056 0.84355267\n",
      " 0.83967447 0.83967447 0.83967447 0.83967571 0.83967571 0.83967571\n",
      " 0.84284845 0.84320056 0.84355267 0.83967447 0.83967447 0.83967447\n",
      " 0.83967571 0.83967571 0.83967571 0.84319994 0.84249509 0.84178963\n",
      " 0.83967447 0.8400272  0.8400272  0.83967571 0.83967571 0.83967571\n",
      " 0.84319994 0.84249509 0.84178963 0.83967447 0.8400272  0.8400272\n",
      " 0.83967571 0.83967571 0.83967571 0.84319994 0.84249509 0.84178963\n",
      " 0.83967447 0.8400272  0.8400272  0.83967571 0.83967571 0.83967571\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.84425566 0.84249323 0.84249323\n",
      " 0.83932235 0.83932235 0.83932235 0.83967571 0.83967571 0.83967571\n",
      " 0.84425566 0.84249323 0.84249323 0.83932235 0.83932235 0.83932235\n",
      " 0.83967571 0.83967571 0.83967571 0.84425566 0.84249323 0.84249323\n",
      " 0.83932235 0.83932235 0.83932235 0.83967571 0.83967571 0.83967571\n",
      " 0.82910984 0.82735052 0.83193233 0.83720844 0.83685446 0.83685384\n",
      " 0.83967509 0.83932297 0.83932297 0.82910984 0.82735052 0.83193233\n",
      " 0.83720844 0.83685446 0.83685384 0.83967509 0.83932297 0.83932297\n",
      " 0.82910984 0.82735052 0.83193233 0.83720844 0.83685446 0.83685384\n",
      " 0.83967509 0.83932297 0.83932297        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84319994 0.84214174 0.84214174 0.8400272  0.8400272  0.84073143\n",
      " 0.83967571 0.83967571 0.83967571 0.84319994 0.84214174 0.84214174\n",
      " 0.8400272  0.8400272  0.84073143 0.83967571 0.83967571 0.83967571\n",
      " 0.84319994 0.84214174 0.84214174 0.8400272  0.8400272  0.84073143\n",
      " 0.83967571 0.83967571 0.83967571 0.84319994 0.84249509 0.84178963\n",
      " 0.83967447 0.8400272  0.8400272  0.83967571 0.83967571 0.83967571\n",
      " 0.84319994 0.84249509 0.84178963 0.83967447 0.8400272  0.8400272\n",
      " 0.83967571 0.83967571 0.83967571 0.84319994 0.84249509 0.84178963\n",
      " 0.83967447 0.8400272  0.8400272  0.83967571 0.83967571 0.83967571\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.84425566 0.84249323 0.84249323\n",
      " 0.83932235 0.83932235 0.83932235 0.83967571 0.83967571 0.83967571\n",
      " 0.84425566 0.84249323 0.84249323 0.83932235 0.83932235 0.83932235\n",
      " 0.83967571 0.83967571 0.83967571 0.84425566 0.84249323 0.84249323\n",
      " 0.83932235 0.83932235 0.83932235 0.83967571 0.83967571 0.83967571\n",
      " 0.68745622 0.68851194 0.68780398 0.64869898 0.65292061 0.65432906\n",
      " 0.59019549 0.58455859 0.58455859 0.68745622 0.68851194 0.68780398\n",
      " 0.64869898 0.65292061 0.65432906 0.59019549 0.58455859 0.58455859\n",
      " 0.68745622 0.68851194 0.68780398 0.64869898 0.65292061 0.65432906\n",
      " 0.59019549 0.58455859 0.58455859        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.69380046 0.69767245 0.69732096 0.65397881 0.65327459 0.65362856\n",
      " 0.60324043 0.60006583 0.60006583 0.69380046 0.69767245 0.69732096\n",
      " 0.65397881 0.65327459 0.65362856 0.60324043 0.60006583 0.60006583\n",
      " 0.69380046 0.69767245 0.69732096 0.65397881 0.65327459 0.65362856\n",
      " 0.60324043 0.60006583 0.60006583 0.69661612 0.69802519 0.6976737\n",
      " 0.64904923 0.65221887 0.65327645 0.60218409 0.59830651 0.59830651\n",
      " 0.69661612 0.69802519 0.6976737  0.64904923 0.65221887 0.65327645\n",
      " 0.60218409 0.59830651 0.59830651 0.69661612 0.69802519 0.6976737\n",
      " 0.64904923 0.65221887 0.65327645 0.60218409 0.59830651 0.59830651\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.68076235 0.6821708  0.67970291\n",
      " 0.64199953 0.64305152 0.64305152 0.59337444 0.58421455 0.58421455\n",
      " 0.68076235 0.6821708  0.67970291 0.64199953 0.64305152 0.64305152\n",
      " 0.59337444 0.58421455 0.58421455 0.68076235 0.6821708  0.67970291\n",
      " 0.64199953 0.64305152 0.64305152 0.59337444 0.58421455 0.58421455\n",
      " 0.68921554 0.69133443 0.69274163 0.64693159 0.64763954 0.64728557\n",
      " 0.5983003  0.58878891 0.58878891 0.68921554 0.69133443 0.69274163\n",
      " 0.64693159 0.64763954 0.64728557 0.5983003  0.58878891 0.58878891\n",
      " 0.68921554 0.69133443 0.69274163 0.64693159 0.64763954 0.64728557\n",
      " 0.5983003  0.58878891 0.58878891        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.69415009 0.69767245 0.69837792 0.64834377 0.65116191 0.65186676\n",
      " 0.60077564 0.59689681 0.59689681 0.69415009 0.69767245 0.69837792\n",
      " 0.64834377 0.65116191 0.65186676 0.60077564 0.59689681 0.59689681\n",
      " 0.69415009 0.69767245 0.69837792 0.64834377 0.65116191 0.65186676\n",
      " 0.60077564 0.59689681 0.59689681 0.69661612 0.69802519 0.6976737\n",
      " 0.64904923 0.65221887 0.65327645 0.60218409 0.59830651 0.59830651\n",
      " 0.69661612 0.69802519 0.6976737  0.64904923 0.65221887 0.65327645\n",
      " 0.60218409 0.59830651 0.59830651 0.69661612 0.69802519 0.6976737\n",
      " 0.64904923 0.65221887 0.65327645 0.60218409 0.59830651 0.59830651\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.68040962 0.6821708  0.6793508\n",
      " 0.64235164 0.64340363 0.64269941 0.59337444 0.58351032 0.58351032\n",
      " 0.68040962 0.6821708  0.6793508  0.64235164 0.64340363 0.64269941\n",
      " 0.59337444 0.58351032 0.58351032 0.68040962 0.6821708  0.6793508\n",
      " 0.64235164 0.64340363 0.64269941 0.59337444 0.58351032 0.58351032\n",
      " 0.68393199 0.68005626 0.6821708  0.64834128 0.65327521 0.6543303\n",
      " 0.60112279 0.59019922 0.59019922 0.68393199 0.68005626 0.6821708\n",
      " 0.64834128 0.65327521 0.6543303  0.60112279 0.59019922 0.59019922\n",
      " 0.68393199 0.68005626 0.6821708  0.64834128 0.65327521 0.6543303\n",
      " 0.60112279 0.59019922 0.59019922        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.69626338 0.69802519 0.69837792 0.64975346 0.65257036 0.65292372\n",
      " 0.60147987 0.59725017 0.59725017 0.69626338 0.69802519 0.69837792\n",
      " 0.64975346 0.65257036 0.65292372 0.60147987 0.59725017 0.59725017\n",
      " 0.69626338 0.69802519 0.69837792 0.64975346 0.65257036 0.65292372\n",
      " 0.60147987 0.59725017 0.59725017 0.69661612 0.69802519 0.6976737\n",
      " 0.64904923 0.65221887 0.65327645 0.60218409 0.59830651 0.59830651\n",
      " 0.69661612 0.69802519 0.6976737  0.64904923 0.65221887 0.65327645\n",
      " 0.60218409 0.59830651 0.59830651 0.69661612 0.69802519 0.6976737\n",
      " 0.64904923 0.65221887 0.65327645 0.60218409 0.59830651 0.59830651\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.68040962 0.6821708  0.6793508\n",
      " 0.64235164 0.64340363 0.64269941 0.59337444 0.58351032 0.58351032\n",
      " 0.68040962 0.6821708  0.6793508  0.64235164 0.64340363 0.64269941\n",
      " 0.59337444 0.58351032 0.58351032 0.68040962 0.6821708  0.6793508\n",
      " 0.64235164 0.64340363 0.64269941 0.59337444 0.58351032 0.58351032]\n",
      "  warnings.warn(\n",
      "/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;tf&#x27;,\n",
       "                                        TfidfVectorizer(stop_words=[&#x27;i&#x27;, &#x27;me&#x27;,\n",
       "                                                                    &#x27;my&#x27;,\n",
       "                                                                    &#x27;myself&#x27;,\n",
       "                                                                    &#x27;we&#x27;, &#x27;our&#x27;,\n",
       "                                                                    &#x27;ours&#x27;,\n",
       "                                                                    &#x27;ourselves&#x27;,\n",
       "                                                                    &#x27;you&#x27;,\n",
       "                                                                    &#x27;youre&#x27;,\n",
       "                                                                    &#x27;youve&#x27;,\n",
       "                                                                    &#x27;youll&#x27;,\n",
       "                                                                    &#x27;youd&#x27;,\n",
       "                                                                    &#x27;your&#x27;,\n",
       "                                                                    &#x27;yours&#x27;,\n",
       "                                                                    &#x27;yourself&#x27;,\n",
       "                                                                    &#x27;yourselves&#x27;,\n",
       "                                                                    &#x27;he&#x27;, &#x27;him&#x27;,\n",
       "                                                                    &#x27;his&#x27;,\n",
       "                                                                    &#x27;himself&#x27;,\n",
       "                                                                    &#x27;she&#x27;,\n",
       "                                                                    &#x27;shes&#x27;,\n",
       "                                                                    &#x27;her&#x27;,\n",
       "                                                                    &#x27;hers&#x27;,\n",
       "                                                                    &#x27;herself&#x27;,\n",
       "                                                                    &#x27;it&#x27;, &#x27;its&#x27;,\n",
       "                                                                    &#x27;its&#x27;,\n",
       "                                                                    &#x27;itself&#x27;, ...])),\n",
       "                                       (&#x27;lr&#x27;,\n",
       "                                        LogisticRegression(random_state=42))]),\n",
       "             n_jobs=-2,\n",
       "             param_grid={&#x27;lr__class_weight&#x27;: [None, &#x27;balanced&#x27;],\n",
       "                         &#x27;lr__max_iter&#x27;: [10, 15, 20],\n",
       "                         &#x27;lr__penalty&#x27;: [None, &#x27;l2&#x27;, &#x27;l1&#x27;],\n",
       "                         &#x27;lr__solver&#x27;: [&#x27;lbfgs&#x27;, &#x27;liblinear&#x27;],\n",
       "                         &#x27;tf__max_df&#x27;: [0.88, 0.9, 0.92],\n",
       "                         &#x27;tf__min_df&#x27;: [0.01, 0.02, 0.03],\n",
       "                         &#x27;tf__ngram_range&#x27;: [(1, 1), (1, 2), (1, 3)]},\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;tf&#x27;,\n",
       "                                        TfidfVectorizer(stop_words=[&#x27;i&#x27;, &#x27;me&#x27;,\n",
       "                                                                    &#x27;my&#x27;,\n",
       "                                                                    &#x27;myself&#x27;,\n",
       "                                                                    &#x27;we&#x27;, &#x27;our&#x27;,\n",
       "                                                                    &#x27;ours&#x27;,\n",
       "                                                                    &#x27;ourselves&#x27;,\n",
       "                                                                    &#x27;you&#x27;,\n",
       "                                                                    &#x27;youre&#x27;,\n",
       "                                                                    &#x27;youve&#x27;,\n",
       "                                                                    &#x27;youll&#x27;,\n",
       "                                                                    &#x27;youd&#x27;,\n",
       "                                                                    &#x27;your&#x27;,\n",
       "                                                                    &#x27;yours&#x27;,\n",
       "                                                                    &#x27;yourself&#x27;,\n",
       "                                                                    &#x27;yourselves&#x27;,\n",
       "                                                                    &#x27;he&#x27;, &#x27;him&#x27;,\n",
       "                                                                    &#x27;his&#x27;,\n",
       "                                                                    &#x27;himself&#x27;,\n",
       "                                                                    &#x27;she&#x27;,\n",
       "                                                                    &#x27;shes&#x27;,\n",
       "                                                                    &#x27;her&#x27;,\n",
       "                                                                    &#x27;hers&#x27;,\n",
       "                                                                    &#x27;herself&#x27;,\n",
       "                                                                    &#x27;it&#x27;, &#x27;its&#x27;,\n",
       "                                                                    &#x27;its&#x27;,\n",
       "                                                                    &#x27;itself&#x27;, ...])),\n",
       "                                       (&#x27;lr&#x27;,\n",
       "                                        LogisticRegression(random_state=42))]),\n",
       "             n_jobs=-2,\n",
       "             param_grid={&#x27;lr__class_weight&#x27;: [None, &#x27;balanced&#x27;],\n",
       "                         &#x27;lr__max_iter&#x27;: [10, 15, 20],\n",
       "                         &#x27;lr__penalty&#x27;: [None, &#x27;l2&#x27;, &#x27;l1&#x27;],\n",
       "                         &#x27;lr__solver&#x27;: [&#x27;lbfgs&#x27;, &#x27;liblinear&#x27;],\n",
       "                         &#x27;tf__max_df&#x27;: [0.88, 0.9, 0.92],\n",
       "                         &#x27;tf__min_df&#x27;: [0.01, 0.02, 0.03],\n",
       "                         &#x27;tf__ngram_range&#x27;: [(1, 1), (1, 2), (1, 3)]},\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tf&#x27;,\n",
       "                 TfidfVectorizer(stop_words=[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;,\n",
       "                                             &#x27;our&#x27;, &#x27;ours&#x27;, &#x27;ourselves&#x27;, &#x27;you&#x27;,\n",
       "                                             &#x27;youre&#x27;, &#x27;youve&#x27;, &#x27;youll&#x27;, &#x27;youd&#x27;,\n",
       "                                             &#x27;your&#x27;, &#x27;yours&#x27;, &#x27;yourself&#x27;,\n",
       "                                             &#x27;yourselves&#x27;, &#x27;he&#x27;, &#x27;him&#x27;, &#x27;his&#x27;,\n",
       "                                             &#x27;himself&#x27;, &#x27;she&#x27;, &#x27;shes&#x27;, &#x27;her&#x27;,\n",
       "                                             &#x27;hers&#x27;, &#x27;herself&#x27;, &#x27;it&#x27;, &#x27;its&#x27;,\n",
       "                                             &#x27;its&#x27;, &#x27;itself&#x27;, ...])),\n",
       "                (&#x27;lr&#x27;, LogisticRegression(random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(stop_words=[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;, &#x27;our&#x27;, &#x27;ours&#x27;,\n",
       "                            &#x27;ourselves&#x27;, &#x27;you&#x27;, &#x27;youre&#x27;, &#x27;youve&#x27;, &#x27;youll&#x27;,\n",
       "                            &#x27;youd&#x27;, &#x27;your&#x27;, &#x27;yours&#x27;, &#x27;yourself&#x27;, &#x27;yourselves&#x27;,\n",
       "                            &#x27;he&#x27;, &#x27;him&#x27;, &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;, &#x27;shes&#x27;, &#x27;her&#x27;,\n",
       "                            &#x27;hers&#x27;, &#x27;herself&#x27;, &#x27;it&#x27;, &#x27;its&#x27;, &#x27;its&#x27;, &#x27;itself&#x27;, ...])</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tf',\n",
       "                                        TfidfVectorizer(stop_words=['i', 'me',\n",
       "                                                                    'my',\n",
       "                                                                    'myself',\n",
       "                                                                    'we', 'our',\n",
       "                                                                    'ours',\n",
       "                                                                    'ourselves',\n",
       "                                                                    'you',\n",
       "                                                                    'youre',\n",
       "                                                                    'youve',\n",
       "                                                                    'youll',\n",
       "                                                                    'youd',\n",
       "                                                                    'your',\n",
       "                                                                    'yours',\n",
       "                                                                    'yourself',\n",
       "                                                                    'yourselves',\n",
       "                                                                    'he', 'him',\n",
       "                                                                    'his',\n",
       "                                                                    'himself',\n",
       "                                                                    'she',\n",
       "                                                                    'shes',\n",
       "                                                                    'her',\n",
       "                                                                    'hers',\n",
       "                                                                    'herself',\n",
       "                                                                    'it', 'its',\n",
       "                                                                    'its',\n",
       "                                                                    'itself', ...])),\n",
       "                                       ('lr',\n",
       "                                        LogisticRegression(random_state=42))]),\n",
       "             n_jobs=-2,\n",
       "             param_grid={'lr__class_weight': [None, 'balanced'],\n",
       "                         'lr__max_iter': [10, 15, 20],\n",
       "                         'lr__penalty': [None, 'l2', 'l1'],\n",
       "                         'lr__solver': ['lbfgs', 'liblinear'],\n",
       "                         'tf__max_df': [0.88, 0.9, 0.92],\n",
       "                         'tf__min_df': [0.01, 0.02, 0.03],\n",
       "                         'tf__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_log_grid = GridSearchCV(bin_log_pipe_grid, log_param_grid, cv=5, n_jobs=-2, verbose=1, scoring='accuracy')\n",
    "bin_log_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr__class_weight': None, 'lr__max_iter': 10, 'lr__penalty': 'l1', 'lr__solver': 'liblinear', 'tf__max_df': 0.88, 'tf__min_df': 0.01, 'tf__ngram_range': (1, 1)}\n",
      "0.8442556574011973\n"
     ]
    }
   ],
   "source": [
    "print(bin_log_grid.best_params_)\n",
    "print(bin_log_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8408450704225352"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_log_grid = bin_log_grid.predict(X_test)\n",
    "log_test_acc = accuracy_score(y_test, y_pred_log_grid)\n",
    "log_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fd7195f7790>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cfm_log = confusion_matrix(y_test, y_pred_log_grid)\n",
    "ConfusionMatrixDisplay(cfm_log).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 972 candidates, totalling 4860 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
      "1620 fits failed out of a total of 4860.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "810 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\", line 420, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1227, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py\", line 1221, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "810 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\", line 420, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:976: UserWarning: One or more of the test scores are non-finite: [0.830837   0.82819383 0.82907489 0.83568282 0.83568282 0.83612335\n",
      " 0.84052863 0.84096916 0.84096916 0.830837   0.82819383 0.82907489\n",
      " 0.83568282 0.83568282 0.83612335 0.84052863 0.84096916 0.84096916\n",
      " 0.830837   0.82819383 0.82907489 0.83568282 0.83568282 0.83612335\n",
      " 0.84052863 0.84096916 0.84096916        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84185022 0.84229075 0.84096916 0.83920705 0.83964758 0.83964758\n",
      " 0.83964758 0.83964758 0.83964758 0.84185022 0.84229075 0.84096916\n",
      " 0.83920705 0.83964758 0.83964758 0.83964758 0.83964758 0.83964758\n",
      " 0.84185022 0.84229075 0.84096916 0.83920705 0.83964758 0.83964758\n",
      " 0.83964758 0.83964758 0.83964758 0.84052863 0.84140969 0.84140969\n",
      " 0.83964758 0.83964758 0.83964758 0.83964758 0.83964758 0.83964758\n",
      " 0.84052863 0.84140969 0.84140969 0.83964758 0.83964758 0.83964758\n",
      " 0.83964758 0.83964758 0.83964758 0.84052863 0.84140969 0.84140969\n",
      " 0.83964758 0.83964758 0.83964758 0.83964758 0.83964758 0.83964758\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.83964758 0.83920705 0.83876652\n",
      " 0.83964758 0.83964758 0.83964758 0.83964758 0.83964758 0.83964758\n",
      " 0.83964758 0.83920705 0.83876652 0.83964758 0.83964758 0.83964758\n",
      " 0.83964758 0.83964758 0.83964758 0.83964758 0.83920705 0.83876652\n",
      " 0.83964758 0.83964758 0.83964758 0.83964758 0.83964758 0.83964758\n",
      " 0.83524229 0.83127753 0.83171806 0.83436123 0.83480176 0.83480176\n",
      " 0.83920705 0.83876652 0.83876652 0.83524229 0.83127753 0.83171806\n",
      " 0.83436123 0.83480176 0.83480176 0.83920705 0.83876652 0.83876652\n",
      " 0.83524229 0.83127753 0.83171806 0.83436123 0.83480176 0.83480176\n",
      " 0.83920705 0.83876652 0.83876652        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84052863 0.84096916 0.84140969 0.83964758 0.84008811 0.84008811\n",
      " 0.83964758 0.83964758 0.83964758 0.84052863 0.84096916 0.84140969\n",
      " 0.83964758 0.84008811 0.84008811 0.83964758 0.83964758 0.83964758\n",
      " 0.84052863 0.84096916 0.84140969 0.83964758 0.84008811 0.84008811\n",
      " 0.83964758 0.83964758 0.83964758 0.84052863 0.84140969 0.84140969\n",
      " 0.83964758 0.83964758 0.83964758 0.83964758 0.83964758 0.83964758\n",
      " 0.84052863 0.84140969 0.84140969 0.83964758 0.83964758 0.83964758\n",
      " 0.83964758 0.83964758 0.83964758 0.84052863 0.84140969 0.84140969\n",
      " 0.83964758 0.83964758 0.83964758 0.83964758 0.83964758 0.83964758\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.83964758 0.83920705 0.83920705\n",
      " 0.83964758 0.83964758 0.83964758 0.83964758 0.83964758 0.83964758\n",
      " 0.83964758 0.83920705 0.83920705 0.83964758 0.83964758 0.83964758\n",
      " 0.83964758 0.83964758 0.83964758 0.83964758 0.83920705 0.83920705\n",
      " 0.83964758 0.83964758 0.83964758 0.83964758 0.83964758 0.83964758\n",
      " 0.83303965 0.830837   0.82819383 0.83656388 0.83480176 0.83524229\n",
      " 0.84008811 0.83964758 0.83964758 0.83303965 0.830837   0.82819383\n",
      " 0.83656388 0.83480176 0.83524229 0.84008811 0.83964758 0.83964758\n",
      " 0.83303965 0.830837   0.82819383 0.83656388 0.83480176 0.83524229\n",
      " 0.84008811 0.83964758 0.83964758        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84052863 0.84185022 0.84229075 0.84008811 0.83964758 0.84008811\n",
      " 0.83964758 0.83964758 0.83964758 0.84052863 0.84185022 0.84229075\n",
      " 0.84008811 0.83964758 0.84008811 0.83964758 0.83964758 0.83964758\n",
      " 0.84052863 0.84185022 0.84229075 0.84008811 0.83964758 0.84008811\n",
      " 0.83964758 0.83964758 0.83964758 0.84052863 0.84140969 0.84140969\n",
      " 0.83964758 0.83964758 0.83964758 0.83964758 0.83964758 0.83964758\n",
      " 0.84052863 0.84140969 0.84140969 0.83964758 0.83964758 0.83964758\n",
      " 0.83964758 0.83964758 0.83964758 0.84052863 0.84140969 0.84140969\n",
      " 0.83964758 0.83964758 0.83964758 0.83964758 0.83964758 0.83964758\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.83964758 0.83920705 0.83920705\n",
      " 0.83964758 0.83964758 0.83964758 0.83964758 0.83964758 0.83964758\n",
      " 0.83964758 0.83920705 0.83920705 0.83964758 0.83964758 0.83964758\n",
      " 0.83964758 0.83964758 0.83964758 0.83964758 0.83920705 0.83920705\n",
      " 0.83964758 0.83964758 0.83964758 0.83964758 0.83964758 0.83964758\n",
      " 0.68590308 0.68502203 0.68502203 0.64625551 0.64229075 0.63832599\n",
      " 0.59779736 0.59515419 0.59515419 0.68590308 0.68502203 0.68502203\n",
      " 0.64625551 0.64229075 0.63832599 0.59779736 0.59515419 0.59515419\n",
      " 0.68590308 0.68502203 0.68502203 0.64625551 0.64229075 0.63832599\n",
      " 0.59779736 0.59515419 0.59515419        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.69295154 0.69823789 0.69779736 0.64845815 0.65242291 0.65770925\n",
      " 0.60484581 0.60484581 0.60484581 0.69295154 0.69823789 0.69779736\n",
      " 0.64845815 0.65242291 0.65770925 0.60484581 0.60484581 0.60484581\n",
      " 0.69295154 0.69823789 0.69779736 0.64845815 0.65242291 0.65770925\n",
      " 0.60484581 0.60484581 0.60484581 0.69471366 0.69647577 0.7\n",
      " 0.65066079 0.65418502 0.65638767 0.59867841 0.60044053 0.60044053\n",
      " 0.69471366 0.69647577 0.7        0.65066079 0.65418502 0.65638767\n",
      " 0.59867841 0.60044053 0.60044053 0.69471366 0.69647577 0.7\n",
      " 0.65066079 0.65418502 0.65638767 0.59867841 0.60044053 0.60044053\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.68105727 0.68325991 0.68810573\n",
      " 0.63259912 0.64096916 0.64008811 0.59030837 0.58061674 0.58061674\n",
      " 0.68105727 0.68325991 0.68810573 0.63259912 0.64096916 0.64008811\n",
      " 0.59030837 0.58061674 0.58061674 0.68105727 0.68325991 0.68810573\n",
      " 0.63259912 0.64096916 0.64008811 0.59030837 0.58061674 0.58061674\n",
      " 0.67621145 0.68105727 0.68237885 0.64052863 0.64185022 0.63920705\n",
      " 0.59955947 0.59207048 0.59207048 0.67621145 0.68105727 0.68237885\n",
      " 0.64052863 0.64185022 0.63920705 0.59955947 0.59207048 0.59207048\n",
      " 0.67621145 0.68105727 0.68237885 0.64052863 0.64185022 0.63920705\n",
      " 0.59955947 0.59207048 0.59207048        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.69339207 0.69603524 0.70044053 0.65198238 0.65418502 0.65594714\n",
      " 0.6        0.6        0.6        0.69339207 0.69603524 0.70044053\n",
      " 0.65198238 0.65418502 0.65594714 0.6        0.6        0.6\n",
      " 0.69339207 0.69603524 0.70044053 0.65198238 0.65418502 0.65594714\n",
      " 0.6        0.6        0.6        0.69471366 0.69647577 0.7\n",
      " 0.65066079 0.65418502 0.65638767 0.59867841 0.60044053 0.60044053\n",
      " 0.69471366 0.69647577 0.7        0.65066079 0.65418502 0.65638767\n",
      " 0.59867841 0.60044053 0.60044053 0.69471366 0.69647577 0.7\n",
      " 0.65066079 0.65418502 0.65638767 0.59867841 0.60044053 0.60044053\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.68105727 0.68325991 0.68810573\n",
      " 0.63259912 0.64052863 0.64008811 0.59251101 0.5814978  0.5814978\n",
      " 0.68105727 0.68325991 0.68810573 0.63259912 0.64052863 0.64008811\n",
      " 0.59251101 0.5814978  0.5814978  0.68105727 0.68325991 0.68810573\n",
      " 0.63259912 0.64052863 0.64008811 0.59251101 0.5814978  0.5814978\n",
      " 0.67488987 0.67621145 0.67973568 0.63788546 0.63876652 0.63612335\n",
      " 0.60264317 0.58898678 0.58898678 0.67488987 0.67621145 0.67973568\n",
      " 0.63788546 0.63876652 0.63612335 0.60264317 0.58898678 0.58898678\n",
      " 0.67488987 0.67621145 0.67973568 0.63788546 0.63876652 0.63612335\n",
      " 0.60264317 0.58898678 0.58898678        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.6938326  0.69647577 0.7        0.65066079 0.65374449 0.65638767\n",
      " 0.59779736 0.60044053 0.60044053 0.6938326  0.69647577 0.7\n",
      " 0.65066079 0.65374449 0.65638767 0.59779736 0.60044053 0.60044053\n",
      " 0.6938326  0.69647577 0.7        0.65066079 0.65374449 0.65638767\n",
      " 0.59779736 0.60044053 0.60044053 0.69471366 0.69647577 0.7\n",
      " 0.65066079 0.65418502 0.65638767 0.59867841 0.60044053 0.60044053\n",
      " 0.69471366 0.69647577 0.7        0.65066079 0.65418502 0.65638767\n",
      " 0.59867841 0.60044053 0.60044053 0.69471366 0.69647577 0.7\n",
      " 0.65066079 0.65418502 0.65638767 0.59867841 0.60044053 0.60044053\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.68105727 0.68325991 0.68810573\n",
      " 0.63259912 0.64052863 0.64008811 0.59251101 0.5814978  0.5814978\n",
      " 0.68105727 0.68325991 0.68810573 0.63259912 0.64052863 0.64008811\n",
      " 0.59251101 0.5814978  0.5814978  0.68105727 0.68325991 0.68810573\n",
      " 0.63259912 0.64052863 0.64008811 0.59251101 0.5814978  0.5814978 ]\n",
      "  warnings.warn(\n",
      "/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 972 candidates, totalling 4860 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
      "1620 fits failed out of a total of 4860.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "810 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\", line 420, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1227, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py\", line 1221, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "810 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\", line 420, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:976: UserWarning: One or more of the test scores are non-finite: [0.83303965 0.83348018 0.83215859 0.83480176 0.83348018 0.83303965\n",
      " 0.83612335 0.83612335 0.83612335 0.83303965 0.83348018 0.83215859\n",
      " 0.83480176 0.83348018 0.83303965 0.83612335 0.83612335 0.83612335\n",
      " 0.83303965 0.83348018 0.83215859 0.83480176 0.83348018 0.83303965\n",
      " 0.83612335 0.83612335 0.83612335        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.83832599 0.83788546 0.83788546 0.83832599 0.83788546 0.83788546\n",
      " 0.83920705 0.83920705 0.83920705 0.83832599 0.83788546 0.83788546\n",
      " 0.83832599 0.83788546 0.83788546 0.83920705 0.83920705 0.83920705\n",
      " 0.83832599 0.83788546 0.83788546 0.83832599 0.83788546 0.83788546\n",
      " 0.83920705 0.83920705 0.83920705 0.83832599 0.83744493 0.83744493\n",
      " 0.83876652 0.83876652 0.83876652 0.83920705 0.83920705 0.83920705\n",
      " 0.83832599 0.83744493 0.83744493 0.83876652 0.83876652 0.83876652\n",
      " 0.83920705 0.83920705 0.83920705 0.83832599 0.83744493 0.83744493\n",
      " 0.83876652 0.83876652 0.83876652 0.83920705 0.83920705 0.83920705\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.83568282 0.83436123 0.83436123\n",
      " 0.83964758 0.83964758 0.83964758 0.83964758 0.83964758 0.83964758\n",
      " 0.83568282 0.83436123 0.83436123 0.83964758 0.83964758 0.83964758\n",
      " 0.83964758 0.83964758 0.83964758 0.83568282 0.83436123 0.83436123\n",
      " 0.83964758 0.83964758 0.83964758 0.83964758 0.83964758 0.83964758\n",
      " 0.82951542 0.830837   0.83171806 0.83127753 0.83215859 0.83215859\n",
      " 0.83744493 0.83744493 0.83744493 0.82951542 0.830837   0.83171806\n",
      " 0.83127753 0.83215859 0.83215859 0.83744493 0.83744493 0.83744493\n",
      " 0.82951542 0.830837   0.83171806 0.83127753 0.83215859 0.83215859\n",
      " 0.83744493 0.83744493 0.83744493        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.83788546 0.83788546 0.83832599 0.83876652 0.83876652 0.83876652\n",
      " 0.83920705 0.83920705 0.83920705 0.83788546 0.83788546 0.83832599\n",
      " 0.83876652 0.83876652 0.83876652 0.83920705 0.83920705 0.83920705\n",
      " 0.83788546 0.83788546 0.83832599 0.83876652 0.83876652 0.83876652\n",
      " 0.83920705 0.83920705 0.83920705 0.83832599 0.83744493 0.83744493\n",
      " 0.83876652 0.83876652 0.83876652 0.83920705 0.83920705 0.83920705\n",
      " 0.83832599 0.83744493 0.83744493 0.83876652 0.83876652 0.83876652\n",
      " 0.83920705 0.83920705 0.83920705 0.83832599 0.83744493 0.83744493\n",
      " 0.83876652 0.83876652 0.83876652 0.83920705 0.83920705 0.83920705\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.83568282 0.83436123 0.83436123\n",
      " 0.83964758 0.83964758 0.83964758 0.83964758 0.83964758 0.83964758\n",
      " 0.83568282 0.83436123 0.83436123 0.83964758 0.83964758 0.83964758\n",
      " 0.83964758 0.83964758 0.83964758 0.83568282 0.83436123 0.83436123\n",
      " 0.83964758 0.83964758 0.83964758 0.83964758 0.83964758 0.83964758\n",
      " 0.82863436 0.83039648 0.82995595 0.83436123 0.83039648 0.82995595\n",
      " 0.83700441 0.83700441 0.83700441 0.82863436 0.83039648 0.82995595\n",
      " 0.83436123 0.83039648 0.82995595 0.83700441 0.83700441 0.83700441\n",
      " 0.82863436 0.83039648 0.82995595 0.83436123 0.83039648 0.82995595\n",
      " 0.83700441 0.83700441 0.83700441        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.83788546 0.83744493 0.83744493 0.83876652 0.83876652 0.83876652\n",
      " 0.83920705 0.83920705 0.83920705 0.83788546 0.83744493 0.83744493\n",
      " 0.83876652 0.83876652 0.83876652 0.83920705 0.83920705 0.83920705\n",
      " 0.83788546 0.83744493 0.83744493 0.83876652 0.83876652 0.83876652\n",
      " 0.83920705 0.83920705 0.83920705 0.83832599 0.83744493 0.83744493\n",
      " 0.83876652 0.83876652 0.83876652 0.83920705 0.83920705 0.83920705\n",
      " 0.83832599 0.83744493 0.83744493 0.83876652 0.83876652 0.83876652\n",
      " 0.83920705 0.83920705 0.83920705 0.83832599 0.83744493 0.83744493\n",
      " 0.83876652 0.83876652 0.83876652 0.83920705 0.83920705 0.83920705\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.83568282 0.83436123 0.83436123\n",
      " 0.83964758 0.83964758 0.83964758 0.83964758 0.83964758 0.83964758\n",
      " 0.83568282 0.83436123 0.83436123 0.83964758 0.83964758 0.83964758\n",
      " 0.83964758 0.83964758 0.83964758 0.83568282 0.83436123 0.83436123\n",
      " 0.83964758 0.83964758 0.83964758 0.83964758 0.83964758 0.83964758\n",
      " 0.69515419 0.6969163  0.69427313 0.63700441 0.64052863 0.64317181\n",
      " 0.60792952 0.60881057 0.60881057 0.69515419 0.6969163  0.69427313\n",
      " 0.63700441 0.64052863 0.64317181 0.60792952 0.60881057 0.60881057\n",
      " 0.69515419 0.6969163  0.69427313 0.63700441 0.64052863 0.64317181\n",
      " 0.60792952 0.60881057 0.60881057        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.70440529 0.7092511  0.7061674  0.64185022 0.65022026 0.64977974\n",
      " 0.60881057 0.61409692 0.61409692 0.70440529 0.7092511  0.7061674\n",
      " 0.64185022 0.65022026 0.64977974 0.60881057 0.61409692 0.61409692\n",
      " 0.70440529 0.7092511  0.7061674  0.64185022 0.65022026 0.64977974\n",
      " 0.60881057 0.61409692 0.61409692 0.70088106 0.70704846 0.70704846\n",
      " 0.64185022 0.64889868 0.65110132 0.61189427 0.61365639 0.61365639\n",
      " 0.70088106 0.70704846 0.70704846 0.64185022 0.64889868 0.65110132\n",
      " 0.61189427 0.61365639 0.61365639 0.70088106 0.70704846 0.70704846\n",
      " 0.64185022 0.64889868 0.65110132 0.61189427 0.61365639 0.61365639\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.69162996 0.69251101 0.69339207\n",
      " 0.62863436 0.62863436 0.62863436 0.59779736 0.59427313 0.59427313\n",
      " 0.69162996 0.69251101 0.69339207 0.62863436 0.62863436 0.62863436\n",
      " 0.59779736 0.59427313 0.59427313 0.69162996 0.69251101 0.69339207\n",
      " 0.62863436 0.62863436 0.62863436 0.59779736 0.59427313 0.59427313\n",
      " 0.69339207 0.69295154 0.69251101 0.63436123 0.63348018 0.63612335\n",
      " 0.60484581 0.60792952 0.60792952 0.69339207 0.69295154 0.69251101\n",
      " 0.63436123 0.63348018 0.63612335 0.60484581 0.60792952 0.60792952\n",
      " 0.69339207 0.69295154 0.69251101 0.63436123 0.63348018 0.63612335\n",
      " 0.60484581 0.60792952 0.60792952        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.70264317 0.70704846 0.70704846 0.64096916 0.64845815 0.64889868\n",
      " 0.61057269 0.61365639 0.61365639 0.70264317 0.70704846 0.70704846\n",
      " 0.64096916 0.64845815 0.64889868 0.61057269 0.61365639 0.61365639\n",
      " 0.70264317 0.70704846 0.70704846 0.64096916 0.64845815 0.64889868\n",
      " 0.61057269 0.61365639 0.61365639 0.70088106 0.70704846 0.70704846\n",
      " 0.64185022 0.64889868 0.65110132 0.61189427 0.61365639 0.61365639\n",
      " 0.70088106 0.70704846 0.70704846 0.64185022 0.64889868 0.65110132\n",
      " 0.61189427 0.61365639 0.61365639 0.70088106 0.70704846 0.70704846\n",
      " 0.64185022 0.64889868 0.65110132 0.61189427 0.61365639 0.61365639\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.69162996 0.69339207 0.69339207\n",
      " 0.62819383 0.62863436 0.6277533  0.5969163  0.59251101 0.59251101\n",
      " 0.69162996 0.69339207 0.69339207 0.62819383 0.62863436 0.6277533\n",
      " 0.5969163  0.59251101 0.59251101 0.69162996 0.69339207 0.69339207\n",
      " 0.62819383 0.62863436 0.6277533  0.5969163  0.59251101 0.59251101\n",
      " 0.6938326  0.69603524 0.6969163  0.63656388 0.6339207  0.64008811\n",
      " 0.60528634 0.61145374 0.61145374 0.6938326  0.69603524 0.6969163\n",
      " 0.63656388 0.6339207  0.64008811 0.60528634 0.61145374 0.61145374\n",
      " 0.6938326  0.69603524 0.6969163  0.63656388 0.6339207  0.64008811\n",
      " 0.60528634 0.61145374 0.61145374        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.70132159 0.70704846 0.70704846 0.64317181 0.64801762 0.65022026\n",
      " 0.61057269 0.61365639 0.61365639 0.70132159 0.70704846 0.70704846\n",
      " 0.64317181 0.64801762 0.65022026 0.61057269 0.61365639 0.61365639\n",
      " 0.70132159 0.70704846 0.70704846 0.64317181 0.64801762 0.65022026\n",
      " 0.61057269 0.61365639 0.61365639 0.70088106 0.70704846 0.70704846\n",
      " 0.64185022 0.64889868 0.65110132 0.61189427 0.61365639 0.61365639\n",
      " 0.70088106 0.70704846 0.70704846 0.64185022 0.64889868 0.65110132\n",
      " 0.61189427 0.61365639 0.61365639 0.70088106 0.70704846 0.70704846\n",
      " 0.64185022 0.64889868 0.65110132 0.61189427 0.61365639 0.61365639\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.69162996 0.69339207 0.69339207\n",
      " 0.62819383 0.62863436 0.6277533  0.59647577 0.59251101 0.59251101\n",
      " 0.69162996 0.69339207 0.69339207 0.62819383 0.62863436 0.6277533\n",
      " 0.59647577 0.59251101 0.59251101 0.69162996 0.69339207 0.69339207\n",
      " 0.62819383 0.62863436 0.6277533  0.59647577 0.59251101 0.59251101]\n",
      "  warnings.warn(\n",
      "/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 972 candidates, totalling 4860 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
      "1620 fits failed out of a total of 4860.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "810 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\", line 420, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1227, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py\", line 1221, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "810 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\", line 420, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:976: UserWarning: One or more of the test scores are non-finite: [0.83259912 0.83215859 0.83127753 0.83744493 0.83656388 0.83656388\n",
      " 0.83920705 0.83964758 0.83964758 0.83259912 0.83215859 0.83127753\n",
      " 0.83744493 0.83656388 0.83656388 0.83920705 0.83964758 0.83964758\n",
      " 0.83259912 0.83215859 0.83127753 0.83744493 0.83656388 0.83656388\n",
      " 0.83920705 0.83964758 0.83964758        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84052863 0.84052863 0.84008811 0.84052863 0.84052863 0.84052863\n",
      " 0.83920705 0.83920705 0.83920705 0.84052863 0.84052863 0.84008811\n",
      " 0.84052863 0.84052863 0.84052863 0.83920705 0.83920705 0.83920705\n",
      " 0.84052863 0.84052863 0.84008811 0.84052863 0.84052863 0.84052863\n",
      " 0.83920705 0.83920705 0.83920705 0.84096916 0.84052863 0.84052863\n",
      " 0.84140969 0.84096916 0.84096916 0.83964758 0.83964758 0.83964758\n",
      " 0.84096916 0.84052863 0.84052863 0.84140969 0.84096916 0.84096916\n",
      " 0.83964758 0.83964758 0.83964758 0.84096916 0.84052863 0.84052863\n",
      " 0.84140969 0.84096916 0.84096916 0.83964758 0.83964758 0.83964758\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.84185022 0.83964758 0.84008811\n",
      " 0.84185022 0.84140969 0.84140969 0.83964758 0.83964758 0.83964758\n",
      " 0.84185022 0.83964758 0.84008811 0.84185022 0.84140969 0.84140969\n",
      " 0.83964758 0.83964758 0.83964758 0.84185022 0.83964758 0.84008811\n",
      " 0.84185022 0.84140969 0.84140969 0.83964758 0.83964758 0.83964758\n",
      " 0.82731278 0.82643172 0.82599119 0.83920705 0.83700441 0.83700441\n",
      " 0.83656388 0.83700441 0.83700441 0.82731278 0.82643172 0.82599119\n",
      " 0.83920705 0.83700441 0.83700441 0.83656388 0.83700441 0.83700441\n",
      " 0.82731278 0.82643172 0.82599119 0.83920705 0.83700441 0.83700441\n",
      " 0.83656388 0.83700441 0.83700441        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84052863 0.84096916 0.84052863 0.84096916 0.84052863 0.84052863\n",
      " 0.83964758 0.83964758 0.83964758 0.84052863 0.84096916 0.84052863\n",
      " 0.84096916 0.84052863 0.84052863 0.83964758 0.83964758 0.83964758\n",
      " 0.84052863 0.84096916 0.84052863 0.84096916 0.84052863 0.84052863\n",
      " 0.83964758 0.83964758 0.83964758 0.84096916 0.84052863 0.84052863\n",
      " 0.84140969 0.84096916 0.84096916 0.83964758 0.83964758 0.83964758\n",
      " 0.84096916 0.84052863 0.84052863 0.84140969 0.84096916 0.84096916\n",
      " 0.83964758 0.83964758 0.83964758 0.84096916 0.84052863 0.84052863\n",
      " 0.84140969 0.84096916 0.84096916 0.83964758 0.83964758 0.83964758\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.84140969 0.83964758 0.84008811\n",
      " 0.84185022 0.84140969 0.84140969 0.83964758 0.83964758 0.83964758\n",
      " 0.84140969 0.83964758 0.84008811 0.84185022 0.84140969 0.84140969\n",
      " 0.83964758 0.83964758 0.83964758 0.84140969 0.83964758 0.84008811\n",
      " 0.84185022 0.84140969 0.84140969 0.83964758 0.83964758 0.83964758\n",
      " 0.82731278 0.82863436 0.82731278 0.84052863 0.83832599 0.83832599\n",
      " 0.83832599 0.83876652 0.83876652 0.82731278 0.82863436 0.82731278\n",
      " 0.84052863 0.83832599 0.83832599 0.83832599 0.83876652 0.83876652\n",
      " 0.82731278 0.82863436 0.82731278 0.84052863 0.83832599 0.83832599\n",
      " 0.83832599 0.83876652 0.83876652        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84052863 0.84008811 0.84008811 0.84140969 0.84096916 0.84096916\n",
      " 0.83964758 0.83964758 0.83964758 0.84052863 0.84008811 0.84008811\n",
      " 0.84140969 0.84096916 0.84096916 0.83964758 0.83964758 0.83964758\n",
      " 0.84052863 0.84008811 0.84008811 0.84140969 0.84096916 0.84096916\n",
      " 0.83964758 0.83964758 0.83964758 0.84096916 0.84052863 0.84052863\n",
      " 0.84140969 0.84096916 0.84096916 0.83964758 0.83964758 0.83964758\n",
      " 0.84096916 0.84052863 0.84052863 0.84140969 0.84096916 0.84096916\n",
      " 0.83964758 0.83964758 0.83964758 0.84096916 0.84052863 0.84052863\n",
      " 0.84140969 0.84096916 0.84096916 0.83964758 0.83964758 0.83964758\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.84140969 0.83964758 0.84008811\n",
      " 0.84185022 0.84140969 0.84140969 0.83964758 0.83964758 0.83964758\n",
      " 0.84140969 0.83964758 0.84008811 0.84185022 0.84140969 0.84140969\n",
      " 0.83964758 0.83964758 0.83964758 0.84140969 0.83964758 0.84008811\n",
      " 0.84185022 0.84140969 0.84140969 0.83964758 0.83964758 0.83964758\n",
      " 0.69339207 0.69647577 0.69559471 0.63700441 0.63171806 0.62819383\n",
      " 0.59295154 0.58722467 0.58722467 0.69339207 0.69647577 0.69559471\n",
      " 0.63700441 0.63171806 0.62819383 0.59295154 0.58722467 0.58722467\n",
      " 0.69339207 0.69647577 0.69559471 0.63700441 0.63171806 0.62819383\n",
      " 0.59295154 0.58722467 0.58722467        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.69955947 0.70220264 0.70176211 0.64493392 0.64140969 0.64273128\n",
      " 0.60044053 0.59955947 0.59955947 0.69955947 0.70220264 0.70176211\n",
      " 0.64493392 0.64140969 0.64273128 0.60044053 0.59955947 0.59955947\n",
      " 0.69955947 0.70220264 0.70176211 0.64493392 0.64140969 0.64273128\n",
      " 0.60044053 0.59955947 0.59955947 0.70132159 0.70352423 0.70440529\n",
      " 0.64625551 0.64581498 0.64625551 0.60220264 0.60220264 0.60220264\n",
      " 0.70132159 0.70352423 0.70440529 0.64625551 0.64581498 0.64625551\n",
      " 0.60220264 0.60220264 0.60220264 0.70132159 0.70352423 0.70440529\n",
      " 0.64625551 0.64581498 0.64625551 0.60220264 0.60220264 0.60220264\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.67973568 0.68193833 0.67885463\n",
      " 0.62995595 0.62643172 0.6246696  0.5907489  0.58281938 0.58281938\n",
      " 0.67973568 0.68193833 0.67885463 0.62995595 0.62643172 0.6246696\n",
      " 0.5907489  0.58281938 0.58281938 0.67973568 0.68193833 0.67885463\n",
      " 0.62995595 0.62643172 0.6246696  0.5907489  0.58281938 0.58281938\n",
      " 0.69515419 0.69823789 0.70220264 0.64361233 0.63788546 0.63700441\n",
      " 0.60528634 0.59735683 0.59735683 0.69515419 0.69823789 0.70220264\n",
      " 0.64361233 0.63788546 0.63700441 0.60528634 0.59735683 0.59735683\n",
      " 0.69515419 0.69823789 0.70220264 0.64361233 0.63788546 0.63700441\n",
      " 0.60528634 0.59735683 0.59735683        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.70132159 0.70396476 0.70484581 0.64581498 0.64669604 0.64713656\n",
      " 0.60264317 0.60176211 0.60176211 0.70132159 0.70396476 0.70484581\n",
      " 0.64581498 0.64669604 0.64713656 0.60264317 0.60176211 0.60176211\n",
      " 0.70132159 0.70396476 0.70484581 0.64581498 0.64669604 0.64713656\n",
      " 0.60264317 0.60176211 0.60176211 0.70132159 0.70352423 0.70440529\n",
      " 0.64625551 0.64581498 0.64625551 0.60220264 0.60220264 0.60220264\n",
      " 0.70132159 0.70352423 0.70440529 0.64625551 0.64581498 0.64625551\n",
      " 0.60220264 0.60220264 0.60220264 0.70132159 0.70352423 0.70440529\n",
      " 0.64625551 0.64581498 0.64625551 0.60220264 0.60220264 0.60220264\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.68017621 0.68193833 0.6784141\n",
      " 0.63039648 0.62599119 0.62422907 0.59118943 0.58193833 0.58193833\n",
      " 0.68017621 0.68193833 0.6784141  0.63039648 0.62599119 0.62422907\n",
      " 0.59118943 0.58193833 0.58193833 0.68017621 0.68193833 0.6784141\n",
      " 0.63039648 0.62599119 0.62422907 0.59118943 0.58193833 0.58193833\n",
      " 0.69515419 0.69471366 0.69207048 0.64185022 0.63700441 0.63656388\n",
      " 0.60352423 0.59471366 0.59471366 0.69515419 0.69471366 0.69207048\n",
      " 0.64185022 0.63700441 0.63656388 0.60352423 0.59471366 0.59471366\n",
      " 0.69515419 0.69471366 0.69207048 0.64185022 0.63700441 0.63656388\n",
      " 0.60352423 0.59471366 0.59471366        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.70176211 0.70352423 0.70440529 0.64625551 0.64625551 0.64669604\n",
      " 0.60220264 0.60176211 0.60176211 0.70176211 0.70352423 0.70440529\n",
      " 0.64625551 0.64625551 0.64669604 0.60220264 0.60176211 0.60176211\n",
      " 0.70176211 0.70352423 0.70440529 0.64625551 0.64625551 0.64669604\n",
      " 0.60220264 0.60176211 0.60176211 0.70132159 0.70352423 0.70440529\n",
      " 0.64625551 0.64581498 0.64625551 0.60220264 0.60220264 0.60220264\n",
      " 0.70132159 0.70352423 0.70440529 0.64625551 0.64581498 0.64625551\n",
      " 0.60220264 0.60220264 0.60220264 0.70132159 0.70352423 0.70440529\n",
      " 0.64625551 0.64581498 0.64625551 0.60220264 0.60220264 0.60220264\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.68017621 0.68193833 0.6784141\n",
      " 0.63039648 0.62599119 0.62422907 0.59118943 0.58193833 0.58193833\n",
      " 0.68017621 0.68193833 0.6784141  0.63039648 0.62599119 0.62422907\n",
      " 0.59118943 0.58193833 0.58193833 0.68017621 0.68193833 0.6784141\n",
      " 0.63039648 0.62599119 0.62422907 0.59118943 0.58193833 0.58193833]\n",
      "  warnings.warn(\n",
      "/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 972 candidates, totalling 4860 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
      "1620 fits failed out of a total of 4860.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "810 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\", line 420, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1227, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py\", line 1221, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "810 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\", line 420, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:976: UserWarning: One or more of the test scores are non-finite: [0.8287002  0.82914363 0.82914363 0.83443094 0.83399041 0.83399041\n",
      " 0.83795614 0.83795614 0.83795614 0.8287002  0.82914363 0.82914363\n",
      " 0.83443094 0.83399041 0.83399041 0.83795614 0.83795614 0.83795614\n",
      " 0.8287002  0.82914363 0.82914363 0.83443094 0.83399041 0.83399041\n",
      " 0.83795614 0.83795614 0.83795614        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84059738 0.84059738 0.84059641 0.83883526 0.83839473 0.83839473\n",
      " 0.83971826 0.83971826 0.83971826 0.84059738 0.84059738 0.84059641\n",
      " 0.83883526 0.83839473 0.83839473 0.83971826 0.83971826 0.83971826\n",
      " 0.84059738 0.84059738 0.84059641 0.83883526 0.83839473 0.83839473\n",
      " 0.83971826 0.83971826 0.83971826 0.84280002 0.84191896 0.84191896\n",
      " 0.83927579 0.83883526 0.83883526 0.83971826 0.83971826 0.83971826\n",
      " 0.84280002 0.84191896 0.84191896 0.83927579 0.83883526 0.83883526\n",
      " 0.83971826 0.83971826 0.83971826 0.84280002 0.84191896 0.84191896\n",
      " 0.83927579 0.83883526 0.83883526 0.83971826 0.83971826 0.83971826\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.84280002 0.84235949 0.84235949\n",
      " 0.83927676 0.8388372  0.83927676 0.83971826 0.83971826 0.83971826\n",
      " 0.84280002 0.84235949 0.84235949 0.83927676 0.8388372  0.83927676\n",
      " 0.83971826 0.83971826 0.83971826 0.84280002 0.84235949 0.84235949\n",
      " 0.83927676 0.8388372  0.83927676 0.83971826 0.83971826 0.83971826\n",
      " 0.83002856 0.8295861  0.82562134 0.83310839 0.8317868  0.8317868\n",
      " 0.83399622 0.8357564  0.8357564  0.83002856 0.8295861  0.82562134\n",
      " 0.83310839 0.8317868  0.8317868  0.83399622 0.8357564  0.8357564\n",
      " 0.83002856 0.8295861  0.82562134 0.83310839 0.8317868  0.8317868\n",
      " 0.83399622 0.8357564  0.8357564         nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84280002 0.84280002 0.84280002 0.83883526 0.83883526 0.83883526\n",
      " 0.83971826 0.83971826 0.83971826 0.84280002 0.84280002 0.84280002\n",
      " 0.83883526 0.83883526 0.83883526 0.83971826 0.83971826 0.83971826\n",
      " 0.84280002 0.84280002 0.84280002 0.83883526 0.83883526 0.83883526\n",
      " 0.83971826 0.83971826 0.83971826 0.84280002 0.84191896 0.84191896\n",
      " 0.83927579 0.83883526 0.83883526 0.83971826 0.83971826 0.83971826\n",
      " 0.84280002 0.84191896 0.84191896 0.83927579 0.83883526 0.83883526\n",
      " 0.83971826 0.83971826 0.83971826 0.84280002 0.84191896 0.84191896\n",
      " 0.83927579 0.83883526 0.83883526 0.83971826 0.83971826 0.83971826\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.84280002 0.84235949 0.84235949\n",
      " 0.83927676 0.83927676 0.83927676 0.83971826 0.83971826 0.83971826\n",
      " 0.84280002 0.84235949 0.84235949 0.83927676 0.83927676 0.83927676\n",
      " 0.83971826 0.83971826 0.83971826 0.84280002 0.84235949 0.84235949\n",
      " 0.83927676 0.83927676 0.83927676 0.83971826 0.83971826 0.83971826\n",
      " 0.82826645 0.82782398 0.82871085 0.83266786 0.83134627 0.83134627\n",
      " 0.83575543 0.83487534 0.83487534 0.82826645 0.82782398 0.82871085\n",
      " 0.83266786 0.83134627 0.83134627 0.83575543 0.83487534 0.83487534\n",
      " 0.82826645 0.82782398 0.82871085 0.83266786 0.83134627 0.83134627\n",
      " 0.83575543 0.83487534 0.83487534        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84235949 0.8410379  0.8410379  0.83927579 0.83927579 0.83971632\n",
      " 0.83971826 0.83971826 0.83971826 0.84235949 0.8410379  0.8410379\n",
      " 0.83927579 0.83927579 0.83971632 0.83971826 0.83971826 0.83971826\n",
      " 0.84235949 0.8410379  0.8410379  0.83927579 0.83927579 0.83971632\n",
      " 0.83971826 0.83971826 0.83971826 0.84280002 0.84191896 0.84191896\n",
      " 0.83927579 0.83883526 0.83883526 0.83971826 0.83971826 0.83971826\n",
      " 0.84280002 0.84191896 0.84191896 0.83927579 0.83883526 0.83883526\n",
      " 0.83971826 0.83971826 0.83971826 0.84280002 0.84191896 0.84191896\n",
      " 0.83927579 0.83883526 0.83883526 0.83971826 0.83971826 0.83971826\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.84280002 0.84235949 0.84235949\n",
      " 0.83927676 0.83927676 0.83927676 0.83971826 0.83971826 0.83971826\n",
      " 0.84280002 0.84235949 0.84235949 0.83927676 0.83927676 0.83927676\n",
      " 0.83971826 0.83971826 0.83971826 0.84280002 0.84235949 0.84235949\n",
      " 0.83927676 0.83927676 0.83927676 0.83971826 0.83971826 0.83971826\n",
      " 0.69353246 0.68956383 0.68736215 0.67546594 0.66843201 0.66711333\n",
      " 0.59442901 0.59751852 0.59751852 0.69353246 0.68956383 0.68736215\n",
      " 0.67546594 0.66843201 0.66711333 0.59442901 0.59751852 0.59751852\n",
      " 0.69353246 0.68956383 0.68736215 0.67546594 0.66843201 0.66711333\n",
      " 0.59442901 0.59751852 0.59751852        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.69925449 0.7010166  0.69969405 0.67327008 0.67503219 0.67459263\n",
      " 0.61337174 0.60941085 0.60941085 0.69925449 0.7010166  0.69969405\n",
      " 0.67327008 0.67503219 0.67459263 0.61337174 0.60941085 0.60941085\n",
      " 0.69925449 0.7010166  0.69969405 0.67327008 0.67503219 0.67459263\n",
      " 0.61337174 0.60941085 0.60941085 0.69705475 0.69969502 0.69925255\n",
      " 0.67899695 0.67811686 0.67811783 0.61513482 0.61248971 0.61248971\n",
      " 0.69705475 0.69969502 0.69925255 0.67899695 0.67811686 0.67811783\n",
      " 0.61513482 0.61248971 0.61248971 0.69705475 0.69969502 0.69925255\n",
      " 0.67899695 0.67811686 0.67811783 0.61513482 0.61248971 0.61248971\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.68428717 0.67900373 0.67591906\n",
      " 0.66490584 0.66314954 0.66314954 0.605449   0.59752626 0.59752626\n",
      " 0.68428717 0.67900373 0.67591906 0.66490584 0.66314954 0.66314954\n",
      " 0.605449   0.59752626 0.59752626 0.68428717 0.67900373 0.67591906\n",
      " 0.66490584 0.66314954 0.66314954 0.605449   0.59752626 0.59752626\n",
      " 0.68737377 0.68736022 0.68780365 0.67635862 0.67503994 0.67371835\n",
      " 0.6050075  0.59708089 0.59708089 0.68737377 0.68736022 0.68780365\n",
      " 0.67635862 0.67503994 0.67371835 0.6050075  0.59708089 0.59708089\n",
      " 0.68737377 0.68736022 0.68780365 0.67635862 0.67503994 0.67371835\n",
      " 0.6050075  0.59708089 0.59708089        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.69793581 0.69969405 0.69969308 0.67767536 0.67899792 0.67811783\n",
      " 0.61601588 0.61072857 0.61072857 0.69793581 0.69969405 0.69969308\n",
      " 0.67767536 0.67899792 0.67811783 0.61601588 0.61072857 0.61072857\n",
      " 0.69793581 0.69969405 0.69969308 0.67767536 0.67899792 0.67811783\n",
      " 0.61601588 0.61072857 0.61072857 0.69705475 0.69969502 0.69925255\n",
      " 0.67899695 0.67811686 0.67811783 0.61513482 0.61248971 0.61248971\n",
      " 0.69705475 0.69969502 0.69925255 0.67899695 0.67811686 0.67811783\n",
      " 0.61513482 0.61248971 0.61248971 0.69705475 0.69969502 0.69925255\n",
      " 0.67899695 0.67811686 0.67811783 0.61513482 0.61248971 0.61248971\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.68428717 0.67944426 0.67547853\n",
      " 0.66534637 0.66270901 0.66314954 0.60588953 0.59840732 0.59840732\n",
      " 0.68428717 0.67944426 0.67547853 0.66534637 0.66270901 0.66314954\n",
      " 0.60588953 0.59840732 0.59840732 0.68428717 0.67944426 0.67547853\n",
      " 0.66534637 0.66270901 0.66314954 0.60588953 0.59840732 0.59840732\n",
      " 0.69441255 0.68252021 0.68560198 0.67547369 0.67195624 0.66931887\n",
      " 0.60544997 0.59796485 0.59796485 0.69441255 0.68252021 0.68560198\n",
      " 0.67547369 0.67195624 0.66931887 0.60544997 0.59796485 0.59796485\n",
      " 0.69441255 0.68252021 0.68560198 0.67547369 0.67195624 0.66931887\n",
      " 0.60544997 0.59796485 0.59796485        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.69705475 0.69969502 0.69925255 0.67899695 0.6772358  0.6776773\n",
      " 0.61557535 0.61160962 0.61160962 0.69705475 0.69969502 0.69925255\n",
      " 0.67899695 0.6772358  0.6776773  0.61557535 0.61160962 0.61160962\n",
      " 0.69705475 0.69969502 0.69925255 0.67899695 0.6772358  0.6776773\n",
      " 0.61557535 0.61160962 0.61160962 0.69705475 0.69969502 0.69925255\n",
      " 0.67899695 0.67811686 0.67811783 0.61513482 0.61248971 0.61248971\n",
      " 0.69705475 0.69969502 0.69925255 0.67899695 0.67811686 0.67811783\n",
      " 0.61513482 0.61248971 0.61248971 0.69705475 0.69969502 0.69925255\n",
      " 0.67899695 0.67811686 0.67811783 0.61513482 0.61248971 0.61248971\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.68428717 0.67944426 0.67547853\n",
      " 0.66534637 0.66270901 0.66314954 0.60544997 0.59840732 0.59840732\n",
      " 0.68428717 0.67944426 0.67547853 0.66534637 0.66270901 0.66314954\n",
      " 0.60544997 0.59840732 0.59840732 0.68428717 0.67944426 0.67547853\n",
      " 0.66534637 0.66270901 0.66314954 0.60544997 0.59840732 0.59840732]\n",
      "  warnings.warn(\n",
      "/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 972 candidates, totalling 4860 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
      "1620 fits failed out of a total of 4860.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "810 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\", line 420, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1227, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py\", line 1221, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "810 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\", line 420, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/michaelromanski/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:976: UserWarning: One or more of the test scores are non-finite: [0.83135112 0.83267173 0.83135015 0.8322283  0.8322283  0.8322283\n",
      " 0.83619403 0.83619403 0.83619403 0.83135112 0.83267173 0.83135015\n",
      " 0.8322283  0.8322283  0.8322283  0.83619403 0.83619403 0.83619403\n",
      " 0.83135112 0.83267173 0.83135015 0.8322283  0.8322283  0.8322283\n",
      " 0.83619403 0.83619403 0.83619403        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.8414794  0.84059834 0.83971826 0.83795614 0.83795614 0.83795614\n",
      " 0.83839667 0.83839667 0.83839667 0.8414794  0.84059834 0.83971826\n",
      " 0.83795614 0.83795614 0.83795614 0.83839667 0.83839667 0.83839667\n",
      " 0.8414794  0.84059834 0.83971826 0.83795614 0.83795614 0.83795614\n",
      " 0.83839667 0.83839667 0.83839667 0.84148037 0.84148037 0.84103984\n",
      " 0.83751464 0.83795517 0.8383957  0.83971826 0.83971826 0.83971826\n",
      " 0.84148037 0.84148037 0.84103984 0.83751464 0.83795517 0.8383957\n",
      " 0.83971826 0.83971826 0.83971826 0.84148037 0.84148037 0.84103984\n",
      " 0.83751464 0.83795517 0.8383957  0.83971826 0.83971826 0.83971826\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.84015878 0.84059834 0.84059834\n",
      " 0.83707508 0.8383957  0.8383957  0.83971826 0.83971826 0.83971826\n",
      " 0.84015878 0.84059834 0.84059834 0.83707508 0.8383957  0.8383957\n",
      " 0.83971826 0.83971826 0.83971826 0.84015878 0.84059834 0.84059834\n",
      " 0.83707508 0.8383957  0.8383957  0.83971826 0.83971826 0.83971826\n",
      " 0.8322312  0.82827129 0.82871182 0.83310936 0.83443288 0.83487341\n",
      " 0.83751561 0.83751561 0.83751561 0.8322312  0.82827129 0.82871182\n",
      " 0.83310936 0.83443288 0.83487341 0.83751561 0.83751561 0.83751561\n",
      " 0.8322312  0.82827129 0.82871182 0.83310936 0.83443288 0.83487341\n",
      " 0.83751561 0.83751561 0.83751561        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84103984 0.84103984 0.84103984 0.83751464 0.83795517 0.83795517\n",
      " 0.83971826 0.83971826 0.83971826 0.84103984 0.84103984 0.84103984\n",
      " 0.83751464 0.83795517 0.83795517 0.83971826 0.83971826 0.83971826\n",
      " 0.84103984 0.84103984 0.84103984 0.83751464 0.83795517 0.83795517\n",
      " 0.83971826 0.83971826 0.83971826 0.84148037 0.84148037 0.84103984\n",
      " 0.83751464 0.83795517 0.8383957  0.83971826 0.83971826 0.83971826\n",
      " 0.84148037 0.84148037 0.84103984 0.83751464 0.83795517 0.8383957\n",
      " 0.83971826 0.83971826 0.83971826 0.84148037 0.84148037 0.84103984\n",
      " 0.83751464 0.83795517 0.8383957  0.83971826 0.83971826 0.83971826\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.84015878 0.84059834 0.84059931\n",
      " 0.83707508 0.8383957  0.8383957  0.83971826 0.83971826 0.83971826\n",
      " 0.84015878 0.84059834 0.84059931 0.83707508 0.8383957  0.8383957\n",
      " 0.83971826 0.83971826 0.83971826 0.84015878 0.84059834 0.84059931\n",
      " 0.83707508 0.8383957  0.8383957  0.83971826 0.83971826 0.83971826\n",
      " 0.8300334  0.8273912  0.82871085 0.83266689 0.83486954 0.83575059\n",
      " 0.83795614 0.83795614 0.83795614 0.8300334  0.8273912  0.82871085\n",
      " 0.83266689 0.83486954 0.83575059 0.83795614 0.83795614 0.83795614\n",
      " 0.8300334  0.8273912  0.82871085 0.83266689 0.83486954 0.83575059\n",
      " 0.83795614 0.83795614 0.83795614        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84148037 0.84103984 0.84148037 0.83795517 0.8383957  0.8383957\n",
      " 0.83971826 0.83971826 0.83971826 0.84148037 0.84103984 0.84148037\n",
      " 0.83795517 0.8383957  0.8383957  0.83971826 0.83971826 0.83971826\n",
      " 0.84148037 0.84103984 0.84148037 0.83795517 0.8383957  0.8383957\n",
      " 0.83971826 0.83971826 0.83971826 0.84148037 0.84148037 0.84103984\n",
      " 0.83751464 0.83795517 0.8383957  0.83971826 0.83971826 0.83971826\n",
      " 0.84148037 0.84148037 0.84103984 0.83751464 0.83795517 0.8383957\n",
      " 0.83971826 0.83971826 0.83971826 0.84148037 0.84148037 0.84103984\n",
      " 0.83751464 0.83795517 0.8383957  0.83971826 0.83971826 0.83971826\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.84015878 0.84059834 0.84059931\n",
      " 0.83707508 0.8383957  0.8383957  0.83971826 0.83971826 0.83971826\n",
      " 0.84015878 0.84059834 0.84059931 0.83707508 0.8383957  0.8383957\n",
      " 0.83971826 0.83971826 0.83971826 0.84015878 0.84059834 0.84059931\n",
      " 0.83707508 0.8383957  0.8383957  0.83971826 0.83971826 0.83971826\n",
      " 0.68824515 0.69000629 0.6891262  0.64156751 0.64068548 0.64024495\n",
      " 0.58873021 0.58167595 0.58167595 0.68824515 0.69000629 0.6891262\n",
      " 0.64156751 0.64068548 0.64024495 0.58873021 0.58167595 0.58167595\n",
      " 0.68824515 0.69000629 0.6891262  0.64156751 0.64068548 0.64024495\n",
      " 0.58873021 0.58167595 0.58167595        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.69264366 0.69484339 0.69396234 0.64773684 0.64464734 0.64508786\n",
      " 0.59620758 0.59135886 0.59135886 0.69264366 0.69484339 0.69396234\n",
      " 0.64773684 0.64464734 0.64508786 0.59620758 0.59135886 0.59135886\n",
      " 0.69264366 0.69484339 0.69396234 0.64773684 0.64464734 0.64508786\n",
      " 0.59620758 0.59135886 0.59135886 0.69396524 0.69572058 0.69484243\n",
      " 0.64597473 0.64465121 0.64421165 0.59929612 0.59312582 0.59312582\n",
      " 0.69396524 0.69572058 0.69484243 0.64597473 0.64465121 0.64421165\n",
      " 0.59929612 0.59312582 0.59312582 0.69396524 0.69572058 0.69484243\n",
      " 0.64597473 0.64465121 0.64421165 0.59929612 0.59312582 0.59312582\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.68340224 0.68911943 0.68647626\n",
      " 0.62835552 0.62703684 0.62615772 0.58520308 0.57860386 0.57860386\n",
      " 0.68340224 0.68911943 0.68647626 0.62835552 0.62703684 0.62615772\n",
      " 0.58520308 0.57860386 0.57860386 0.68340224 0.68911943 0.68647626\n",
      " 0.62835552 0.62703684 0.62615772 0.58520308 0.57860386 0.57860386\n",
      " 0.68868858 0.68867987 0.68956383 0.64025367 0.6349712  0.63276952\n",
      " 0.58961321 0.58609285 0.58609285 0.68868858 0.68867987 0.68956383\n",
      " 0.64025367 0.6349712  0.63276952 0.58961321 0.58609285 0.58609285\n",
      " 0.68868858 0.68867987 0.68956383 0.64025367 0.6349712  0.63276952\n",
      " 0.58961321 0.58609285 0.58609285        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.69440577 0.69572058 0.6944019  0.64597473 0.64509174 0.64421262\n",
      " 0.59841507 0.59444643 0.59444643 0.69440577 0.69572058 0.6944019\n",
      " 0.64597473 0.64509174 0.64421262 0.59841507 0.59444643 0.59444643\n",
      " 0.69440577 0.69572058 0.6944019  0.64597473 0.64509174 0.64421262\n",
      " 0.59841507 0.59444643 0.59444643 0.69396524 0.69572058 0.69484243\n",
      " 0.64597473 0.64465121 0.64421165 0.59929612 0.59312582 0.59312582\n",
      " 0.69396524 0.69572058 0.69484243 0.64597473 0.64465121 0.64421165\n",
      " 0.59929612 0.59312582 0.59312582 0.69396524 0.69572058 0.69484243\n",
      " 0.64597473 0.64465121 0.64421165 0.59929612 0.59312582 0.59312582\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.68340224 0.68647626 0.68647626\n",
      " 0.6270349  0.62703684 0.62659825 0.58520308 0.57904439 0.57904439\n",
      " 0.68340224 0.68647626 0.68647626 0.6270349  0.62703684 0.62659825\n",
      " 0.58520308 0.57904439 0.57904439 0.68340224 0.68647626 0.68647626\n",
      " 0.6270349  0.62703684 0.62659825 0.58520308 0.57904439 0.57904439\n",
      " 0.69617273 0.68339836 0.67767827 0.64069226 0.63540785 0.62660018\n",
      " 0.59357409 0.59048942 0.59048942 0.69617273 0.68339836 0.67767827\n",
      " 0.64069226 0.63540785 0.62660018 0.59357409 0.59048942 0.59048942\n",
      " 0.69617273 0.68339836 0.67767827 0.64069226 0.63540785 0.62660018\n",
      " 0.59357409 0.59048942 0.59048942        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.69396524 0.69572058 0.6944019  0.64597473 0.64421068 0.64465218\n",
      " 0.59885559 0.59312582 0.59312582 0.69396524 0.69572058 0.6944019\n",
      " 0.64597473 0.64421068 0.64465218 0.59885559 0.59312582 0.59312582\n",
      " 0.69396524 0.69572058 0.6944019  0.64597473 0.64421068 0.64465218\n",
      " 0.59885559 0.59312582 0.59312582 0.69396524 0.69572058 0.69484243\n",
      " 0.64597473 0.64465121 0.64421165 0.59929612 0.59312582 0.59312582\n",
      " 0.69396524 0.69572058 0.69484243 0.64597473 0.64465121 0.64421165\n",
      " 0.59929612 0.59312582 0.59312582 0.69396524 0.69572058 0.69484243\n",
      " 0.64597473 0.64465121 0.64421165 0.59929612 0.59312582 0.59312582\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.68340224 0.68647626 0.68647626\n",
      " 0.62615578 0.62703684 0.62571913 0.58520308 0.57904439 0.57904439\n",
      " 0.68340224 0.68647626 0.68647626 0.62615578 0.62703684 0.62571913\n",
      " 0.58520308 0.57904439 0.57904439 0.68340224 0.68647626 0.68647626\n",
      " 0.62615578 0.62703684 0.62571913 0.58520308 0.57904439 0.57904439]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.83978873, 0.83978873, 0.84330986, 0.84479718, 0.84303351])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_score = cross_val_score(bin_log_grid, X_train, y_train, cv=5)\n",
    "log_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(min_df = .01, max_df = .9)\n",
    "\n",
    "X_train_tf = tf.fit_transform(X_train.values)\n",
    "X_train_tf = X_train_tf.toarray()\n",
    "X_test_tf = tf.transform(X_test.values)\n",
    "X_test_tf = X_test_tf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43661971830985913"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_model.fit(X_train_tf, y_train)\n",
    "\n",
    "y_pred_nb = nb_model.predict(X_test_tf)\n",
    "nb_test_acc = accuracy_score(y_test, y_pred_nb)\n",
    "nb_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fd75a154f40>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cfm_nb = confusion_matrix(y_test, y_pred_nb)\n",
    "ConfusionMatrixDisplay(cfm_nb).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.91      0.34       115\n",
      "           1       0.95      0.34      0.51       595\n",
      "\n",
      "    accuracy                           0.44       710\n",
      "   macro avg       0.58      0.63      0.43       710\n",
      "weighted avg       0.83      0.44      0.48       710\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_pipe = Pipeline([\n",
    "    ('nb', GaussianNB())\n",
    "])\n",
    "\n",
    "nb_param_grid = {\n",
    "    'nb__var_smoothing': [1e-8, 1e-9, 1e-10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=Pipeline(steps=[(&#x27;nb&#x27;, GaussianNB())]), n_jobs=2,\n",
       "             param_grid={&#x27;nb__var_smoothing&#x27;: [1e-08, 1e-09, 1e-10]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=Pipeline(steps=[(&#x27;nb&#x27;, GaussianNB())]), n_jobs=2,\n",
       "             param_grid={&#x27;nb__var_smoothing&#x27;: [1e-08, 1e-09, 1e-10]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;nb&#x27;, GaussianNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=Pipeline(steps=[('nb', GaussianNB())]), n_jobs=2,\n",
       "             param_grid={'nb__var_smoothing': [1e-08, 1e-09, 1e-10]})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_grid = GridSearchCV(nb_pipe, nb_param_grid, cv=5, n_jobs=2)\n",
    "nb_grid.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nb__var_smoothing': 1e-08}\n",
      "0.43517152296495015\n"
     ]
    }
   ],
   "source": [
    "print(nb_grid.best_params_)\n",
    "print(nb_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binary['list_tokens'] = lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unprocessed_tweet</th>\n",
       "      <th>product</th>\n",
       "      <th>emotion</th>\n",
       "      <th>processed_tweet</th>\n",
       "      <th>emotion_encoded</th>\n",
       "      <th>list_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>wesley iphone hr tweet riseaustin dead need up...</td>\n",
       "      <td>0</td>\n",
       "      <td>[wesley, iphone, hr, tweet, riseaustin, dead, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>jessedee know fludapp awesome ipadiphone app l...</td>\n",
       "      <td>1</td>\n",
       "      <td>[jessedee, know, fludapp, awesome, ipadiphone,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>swonderlin wait ipad also sale</td>\n",
       "      <td>1</td>\n",
       "      <td>[swonderlin, wait, ipad, also, sale]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>hope year festival crashy year iphone app</td>\n",
       "      <td>0</td>\n",
       "      <td>[hope, year, festival, crashy, year, iphone, app]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>sxtxstate great stuff fri marissa mayer google...</td>\n",
       "      <td>1</td>\n",
       "      <td>[sxtxstate, great, stuff, fri, marissa, mayer,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9077</th>\n",
       "      <td>@mention your PR guy just convinced me to swit...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>pr guy convince switch back iphone great cover...</td>\n",
       "      <td>1</td>\n",
       "      <td>[pr, guy, convince, switch, back, iphone, grea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9079</th>\n",
       "      <td>&amp;quot;papyrus...sort of like the ipad&amp;quot; - ...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>quotpapyrussort like ipadquot nice lol lavelle</td>\n",
       "      <td>1</td>\n",
       "      <td>[quotpapyrussort, like, ipadquot, nice, lol, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9080</th>\n",
       "      <td>Diller says Google TV &amp;quot;might be run over ...</td>\n",
       "      <td>Other Google product or service</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>diller say google tv quotmight run playstation...</td>\n",
       "      <td>0</td>\n",
       "      <td>[diller, say, google, tv, quotmight, run, play...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9085</th>\n",
       "      <td>I've always used Camera+ for my iPhone b/c it ...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>ive always use camera iphone bc image stabiliz...</td>\n",
       "      <td>1</td>\n",
       "      <td>[ive, always, use, camera, iphone, bc, image, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9088</th>\n",
       "      <td>Ipad everywhere. #SXSW {link}</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>ipad everywhere</td>\n",
       "      <td>1</td>\n",
       "      <td>[ipad, everywhere]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3548 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      unprocessed_tweet  \\\n",
       "0     .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1     @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2     @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3     @sxsw I hope this year's festival isn't as cra...   \n",
       "4     @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "...                                                 ...   \n",
       "9077  @mention your PR guy just convinced me to swit...   \n",
       "9079  &quot;papyrus...sort of like the ipad&quot; - ...   \n",
       "9080  Diller says Google TV &quot;might be run over ...   \n",
       "9085  I've always used Camera+ for my iPhone b/c it ...   \n",
       "9088                      Ipad everywhere. #SXSW {link}   \n",
       "\n",
       "                              product           emotion  \\\n",
       "0                              iPhone  Negative emotion   \n",
       "1                  iPad or iPhone App  Positive emotion   \n",
       "2                                iPad  Positive emotion   \n",
       "3                  iPad or iPhone App  Negative emotion   \n",
       "4                              Google  Positive emotion   \n",
       "...                               ...               ...   \n",
       "9077                           iPhone  Positive emotion   \n",
       "9079                             iPad  Positive emotion   \n",
       "9080  Other Google product or service  Negative emotion   \n",
       "9085               iPad or iPhone App  Positive emotion   \n",
       "9088                             iPad  Positive emotion   \n",
       "\n",
       "                                        processed_tweet  emotion_encoded  \\\n",
       "0     wesley iphone hr tweet riseaustin dead need up...                0   \n",
       "1     jessedee know fludapp awesome ipadiphone app l...                1   \n",
       "2                        swonderlin wait ipad also sale                1   \n",
       "3             hope year festival crashy year iphone app                0   \n",
       "4     sxtxstate great stuff fri marissa mayer google...                1   \n",
       "...                                                 ...              ...   \n",
       "9077  pr guy convince switch back iphone great cover...                1   \n",
       "9079     quotpapyrussort like ipadquot nice lol lavelle                1   \n",
       "9080  diller say google tv quotmight run playstation...                0   \n",
       "9085  ive always use camera iphone bc image stabiliz...                1   \n",
       "9088                                    ipad everywhere                1   \n",
       "\n",
       "                                            list_tokens  \n",
       "0     [wesley, iphone, hr, tweet, riseaustin, dead, ...  \n",
       "1     [jessedee, know, fludapp, awesome, ipadiphone,...  \n",
       "2                  [swonderlin, wait, ipad, also, sale]  \n",
       "3     [hope, year, festival, crashy, year, iphone, app]  \n",
       "4     [sxtxstate, great, stuff, fri, marissa, mayer,...  \n",
       "...                                                 ...  \n",
       "9077  [pr, guy, convince, switch, back, iphone, grea...  \n",
       "9079  [quotpapyrussort, like, ipadquot, nice, lol, l...  \n",
       "9080  [diller, say, google, tv, quotmight, run, play...  \n",
       "9085  [ive, always, use, camera, iphone, bc, image, ...  \n",
       "9088                                 [ipad, everywhere]  \n",
       "\n",
       "[3548 rows x 6 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=2, figsize=(12, 6))\n",
    "\n",
    "plotted_words_and_colors = {}\n",
    "\n",
    "color_palette = sns.color_palette('pastel', n_colors = 38)\n",
    "\n",
    "data_by_emotion = [y for _, y in df_binary.groupby('emotion_encoded', as_index=False)]\n",
    "for idx, emotion_df in enumerate(data_by_emotion):\n",
    "    all_words_in_emotion = emotion_df.list_tokens.explode()\n",
    "    top_10 = all_words_in_emotion.value_counts()[:10]\n",
    "    \n",
    "    colors = []\n",
    "    for word in top_10.index:\n",
    "        if word not in plotted_words_and_colors:\n",
    "             new_color = color_palette.pop(0)\n",
    "             plotted_words_and_colors[word] = new_color\n",
    "        colors.append(plotted_words_and_colors[word])\n",
    "            \n",
    "    ax = axes[idx]\n",
    "    ax.bar(top_10.index, top_10.values, color=colors)\n",
    "    ax.set_title(emotion_df.iloc[0].emotion.title())\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ipad      1196\n",
       "apple      881\n",
       "google     692\n",
       "store      549\n",
       "iphone     522\n",
       "app        395\n",
       "new        360\n",
       "get        301\n",
       "austin     290\n",
       "popup      218\n",
       "Name: list_tokens, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/processed_tweets.csv\", index_col=0)\n",
    "\n",
    "data[\"product\"].value_counts()\n",
    "\n",
    "data_branded = data.copy()\n",
    "\n",
    "mapping = {\n",
    "    'iPad': 'Apple',\n",
    "    'Apple': 'Apple',\n",
    "    'iPad or iPhone App': 'Apple',\n",
    "    'iPhone': 'Apple',\n",
    "    'Other Apple product or service': 'Apple',\n",
    "    'Google': 'Google',\n",
    "    'Other Google product or service': 'Google',\n",
    "    'Android App': 'Google',\n",
    "    'Android': 'Google'\n",
    "}\n",
    "\n",
    "data_branded[\"product\"] = data_branded[\"product\"].map(mapping)\n",
    "\n",
    "data_branded.isnull().sum()\n",
    "\n",
    "data_branded[\"product\"].value_counts()\n",
    "\n",
    "data_branded[data_branded['product'].isna()]\n",
    "\n",
    "apple_keywords = ['iPad', 'Apple', 'iPhone']\n",
    "google_keywords = ['Google', 'Android']\n",
    "\n",
    "for keyword in apple_keywords:\n",
    "    mask = (data_branded['product'].isna()) & (data_branded['unprocessed_tweet'].str.contains(keyword, case=False, na=False))\n",
    "    data_branded.loc[mask, 'product'] = 'Apple'\n",
    "\n",
    "for keyword in google_keywords:\n",
    "    mask = (data_branded['product'].isna()) & (data_branded['unprocessed_tweet'].str.contains(keyword, case=False, na=False))\n",
    "    data_branded.loc[mask, 'product'] = 'Google'\n",
    "\n",
    "data_branded.isnull().sum()\n",
    "\n",
    "data_branded.dropna()\n",
    "\n",
    "data_branded.dropna().to_csv(\"data/branded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No emotion toward brand or product    5389\n",
       "Positive emotion                      2978\n",
       "Negative emotion                       570\n",
       "I can't tell                           156\n",
       "Name: emotion, dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_branded['emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unprocessed_tweet      1\n",
       "product              762\n",
       "emotion                0\n",
       "processed_tweet        2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_branded.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_branded.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_data = data_branded[data_branded[\"emotion\"] == 'No emotion toward brand or product']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_test = neutral_data['processed_tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-81-ccd3b472fde9>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  neutral_test.dropna(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "neutral_test.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neutral_test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_neutral = bin_forest_grid.predict(neutral_test)\n",
    "y_pred_neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5       teachntech new ipad apps speechtherapy communi...\n",
       "16      holler gram ipad itunes app store httptcokfnfq...\n",
       "32      attn frineds register gdgtlive see cobra irada...\n",
       "33                              anyone want sell old ipad\n",
       "34                 anyone buy new ipad want sell old ipad\n",
       "                              ...                        \n",
       "9087           yup third app yet im android suggestion cc\n",
       "9089    wave buzz interrupt regularly schedule geek pr...\n",
       "9090    google zeiger physician never report potential...\n",
       "9091    verizon iphone customer complain time fell bac...\n",
       "9092                            google test checkin offer\n",
       "Name: processed_tweet, Length: 4647, dtype: object"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neutral_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>teachntech new ipad apps speechtherapy communi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>holler gram ipad itunes app store httptcokfnfq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>attn frineds register gdgtlive see cobra irada...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>anyone want sell old ipad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>anyone buy new ipad want sell old ipad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9087</th>\n",
       "      <td>yup third app yet im android suggestion cc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9089</th>\n",
       "      <td>wave buzz interrupt regularly schedule geek pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9090</th>\n",
       "      <td>google zeiger physician never report potential...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9091</th>\n",
       "      <td>verizon iphone customer complain time fell bac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9092</th>\n",
       "      <td>google test checkin offer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4647 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        processed_tweet\n",
       "5     teachntech new ipad apps speechtherapy communi...\n",
       "16    holler gram ipad itunes app store httptcokfnfq...\n",
       "32    attn frineds register gdgtlive see cobra irada...\n",
       "33                            anyone want sell old ipad\n",
       "34               anyone buy new ipad want sell old ipad\n",
       "...                                                 ...\n",
       "9087         yup third app yet im android suggestion cc\n",
       "9089  wave buzz interrupt regularly schedule geek pr...\n",
       "9090  google zeiger physician never report potential...\n",
       "9091  verizon iphone customer complain time fell bac...\n",
       "9092                          google test checkin offer\n",
       "\n",
       "[4647 rows x 1 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neutral_df = neutral_test.to_frame()\n",
    "neutral_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_tweet</th>\n",
       "      <th>emotion_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>teachntech new ipad apps speechtherapy communi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>holler gram ipad itunes app store httptcokfnfq...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>attn frineds register gdgtlive see cobra irada...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>anyone want sell old ipad</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>anyone buy new ipad want sell old ipad</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9087</th>\n",
       "      <td>yup third app yet im android suggestion cc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9089</th>\n",
       "      <td>wave buzz interrupt regularly schedule geek pr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9090</th>\n",
       "      <td>google zeiger physician never report potential...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9091</th>\n",
       "      <td>verizon iphone customer complain time fell bac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9092</th>\n",
       "      <td>google test checkin offer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4647 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        processed_tweet  emotion_encoded\n",
       "5     teachntech new ipad apps speechtherapy communi...                1\n",
       "16    holler gram ipad itunes app store httptcokfnfq...                1\n",
       "32    attn frineds register gdgtlive see cobra irada...                1\n",
       "33                            anyone want sell old ipad                1\n",
       "34               anyone buy new ipad want sell old ipad                1\n",
       "...                                                 ...              ...\n",
       "9087         yup third app yet im android suggestion cc                1\n",
       "9089  wave buzz interrupt regularly schedule geek pr...                1\n",
       "9090  google zeiger physician never report potential...                1\n",
       "9091  verizon iphone customer complain time fell bac...                1\n",
       "9092                          google test checkin offer                1\n",
       "\n",
       "[4647 rows x 2 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neutral_df['emotion_encoded'] = y_pred_neutral\n",
    "neutral_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    4407\n",
       "0     240\n",
       "Name: emotion_encoded, dtype: int64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neutral_df['emotion_encoded'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_tweet</th>\n",
       "      <th>emotion_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>funny austin trend matter minute point least a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>tweet google come circle platform today outsid...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>long line else apple store</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>austin classic miss mblogcom twitter iphone pm</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>dslr film discussion moderate guy kawasaki app...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8937</th>\n",
       "      <td>browser prefer internet explorer google chrome...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8953</th>\n",
       "      <td>google bloggersketchup party unexpectedly quie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9014</th>\n",
       "      <td>crazy think upgrade iphone right</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9016</th>\n",
       "      <td>smoked way much yesterday hahaha austin wakenb...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9084</th>\n",
       "      <td>google say future location location location cnn</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        processed_tweet  emotion_encoded\n",
       "123   funny austin trend matter minute point least a...                0\n",
       "312   tweet google come circle platform today outsid...                0\n",
       "330                          long line else apple store                0\n",
       "401      austin classic miss mblogcom twitter iphone pm                0\n",
       "496   dslr film discussion moderate guy kawasaki app...                0\n",
       "...                                                 ...              ...\n",
       "8937  browser prefer internet explorer google chrome...                0\n",
       "8953  google bloggersketchup party unexpectedly quie...                0\n",
       "9014                   crazy think upgrade iphone right                0\n",
       "9016  smoked way much yesterday hahaha austin wakenb...                0\n",
       "9084   google say future location location location cnn                0\n",
       "\n",
       "[240 rows x 2 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neutral_df[neutral_df['emotion_encoded'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "idk_data = data_branded[data_branded['emotion'] == \"I can't tell\"]\n",
    "idk_test = idk_data['processed_tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_idk = bin_forest_grid.predict(idk_test)\n",
    "y_pred_idk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_tweet</th>\n",
       "      <th>emotion_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>thanks publish news new medical apps sxswi con...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>quotapple open popup store austin nerd town ge...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>america need google launch major new social ne...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>queue apple store austin four block long crazy...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>hope well wave buzz google preview social netw...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9020</th>\n",
       "      <td>funny watching room full people hold ipad air ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9032</th>\n",
       "      <td>yeah google nothing u</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9037</th>\n",
       "      <td>yes google presentation exactly expect</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9058</th>\n",
       "      <td>quotdo know apple really good make feel bad xm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9066</th>\n",
       "      <td>much want bet apple disproportionately stock p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        processed_tweet  emotion_encoded\n",
       "90    thanks publish news new medical apps sxswi con...                1\n",
       "102   quotapple open popup store austin nerd town ge...                1\n",
       "237   america need google launch major new social ne...                0\n",
       "341   queue apple store austin four block long crazy...                0\n",
       "368   hope well wave buzz google preview social netw...                0\n",
       "...                                                 ...              ...\n",
       "9020  funny watching room full people hold ipad air ...                1\n",
       "9032                              yeah google nothing u                1\n",
       "9037             yes google presentation exactly expect                0\n",
       "9058  quotdo know apple really good make feel bad xm...                1\n",
       "9066  much want bet apple disproportionately stock p...                1\n",
       "\n",
       "[156 rows x 2 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idk_df = idk_test.to_frame()\n",
    "idk_df['emotion_encoded'] = y_pred_idk\n",
    "idk_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    136\n",
       "0     20\n",
       "Name: emotion_encoded, dtype: int64"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idk_df['emotion_encoded'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative Tweets about apple that are marked positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How much you want to bet Apple is disproportionately stocking the #SXSW pop-up store with iPad 2? The influencer/hipsters thank you'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['unprocessed_tweet'][9066]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'&quot;Do you know what Apple is really good at? Making you feel bad about your Xmas present!&quot; - Seth Meyers on iPad2 #sxsw #doyoureallyneedthat?'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['unprocessed_tweet'][9058]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x89ÛÏ@mention &quot;Apple has opened a pop-up store in Austin so the nerds in town for #SXSW can get their new iPads. {link} #wow'"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['unprocessed_tweet'][102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why Barry Diller thinks iPad only content is nuts @mention #SXSW {link}'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['unprocessed_tweet'][441]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative tweet about Google that was marked positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@mention yeah, we have @mention , Google has nothing on us :) #SXSW'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['unprocessed_tweet'][9032]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Properly labeled negative tweet about apple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apple is &quot;the classiest, fascist company in America,&quot; says @mention #sxsw'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['unprocessed_tweet'][1670]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['unprocessed_tweet'][4002]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive tweet about Google marked as negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@mention is biyt.ly for email, like google voice for email #loveit #sxsw #startupbus'"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['unprocessed_tweet'][3855]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive tweet about Apple marked as negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It is also limited in its abilities. Its a balance. RT @mention @mention An iPad is cheaper than most laptops. #newsapps #sxsw'"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['unprocessed_tweet'][4862]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possibly properly labeled negative google tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@mention Yes, the Google presentation was not exactly what I was expecting. #sxsw'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['unprocessed_tweet'][9037]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "properly labeled positive tweet about Apple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gave into extreme temptation at #SXSW and bought an iPad 2... #impulse'"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['unprocessed_tweet'][488]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tweet is interesting, both positive and negative depending on what brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Forgot my iPhone for #sxsw. Android only. Knife to a gun fight'"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['unprocessed_tweet'][705]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_tweet</th>\n",
       "      <th>emotion_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>thanks publish news new medical apps sxswi con...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>quotapple open popup store austin nerd town ge...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>syd crew iphone extra juice pod procure</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>barry diller think ipad content nut</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>give extreme temptation buy ipad impulse</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>catch mean ipad</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>forgot iphone android knife gun fight</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>google lanzara ningun producto en south southwest</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>comprando mi ipad en el apple store others</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>say google get drunk hckers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>ipad slow rest heartrate tapworthy gsdm leisur...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164</th>\n",
       "      <td>walk mobile apple store austin line insane</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1297</th>\n",
       "      <td>waze v google discussion one best see need com...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391</th>\n",
       "      <td>wow folk make sure iphone ringer turn way pres...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1422</th>\n",
       "      <td>let see google announce circle obama austin do...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>maybe entirely hd flip iphone xl footage certa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545</th>\n",
       "      <td>platformer ci di venue pake app sq apid cnth t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558</th>\n",
       "      <td>google quotcredit card co know accuracy yr go ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1575</th>\n",
       "      <td>sure check jukebox app run party music api</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>hijack iphone since mine doa muahahahahhaahaha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1716</th>\n",
       "      <td>id take hashtags scheen ipadipad tv sport twee...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1908</th>\n",
       "      <td>nonot google circle juwan howard</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1936</th>\n",
       "      <td>realize iphones gon na go nut foursquare gowal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>team android party cant find gowalla foursquar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2068</th>\n",
       "      <td>vai comear palestra exchief evangelist apple</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2095</th>\n",
       "      <td>man get arrest iphone</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2116</th>\n",
       "      <td>iphone crash front apple popup bestworstthingever</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2144</th>\n",
       "      <td>mom might call big power plant cnn google offc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2263</th>\n",
       "      <td>blackberry apps pls let knowcheerswill iphone</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2318</th>\n",
       "      <td>ipadi alacagimiz yer belli oldu apple open pop...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2334</th>\n",
       "      <td>saw bunch security guard play ipads apple stor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2448</th>\n",
       "      <td>agnerd confession use laptop ipad blackberry f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2472</th>\n",
       "      <td>im bum missed team android party oh well ctia ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2525</th>\n",
       "      <td>next designing interface ipad hit quotyour bra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2547</th>\n",
       "      <td>thank goodness two hour break turn iphone batt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2597</th>\n",
       "      <td>temp store apple store</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614</th>\n",
       "      <td>wish apple store employee cheer people line po...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2617</th>\n",
       "      <td>line buy ipad tomorrow notwinning</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2732</th>\n",
       "      <td>look like line apple popup store congress ave ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2739</th>\n",
       "      <td>good place test edgy enough google check offer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2774</th>\n",
       "      <td>hope there time leave poked liked retweeted go...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2902</th>\n",
       "      <td>trend time tsunamithe ipad though</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2954</th>\n",
       "      <td>uxdes glad standard ipad navigation tool might...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2974</th>\n",
       "      <td>think control identity facebook google becomes...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2986</th>\n",
       "      <td>peep awesome product video cant compete game cc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3058</th>\n",
       "      <td>google place value domain extension equal qagb</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3128</th>\n",
       "      <td>report introduce new social medium platform pr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3145</th>\n",
       "      <td>google facebookkiller quotcirclesquot unveil t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3197</th>\n",
       "      <td>liveblog indie iphone game development survive...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3221</th>\n",
       "      <td>walk around star iphone like everyone else</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3265</th>\n",
       "      <td>google might launch social network call circle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3298</th>\n",
       "      <td>quotgoogle product need condensedquot merissa ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3351</th>\n",
       "      <td>apple google intel others go gaga go game fast...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3352</th>\n",
       "      <td>bet googlebuzzlike fail people care privacy el...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3467</th>\n",
       "      <td>neither buying ipad today feel like geek cred ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3501</th>\n",
       "      <td>funny iphone correction dash learn thing two l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3539</th>\n",
       "      <td>diller would product ipad one form factor toda...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3762</th>\n",
       "      <td>petricone say google tv browser think thats co...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3921</th>\n",
       "      <td>agree concern microformats panelist condescend...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3939</th>\n",
       "      <td>would give apple stock like</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        processed_tweet  emotion_encoded\n",
       "90    thanks publish news new medical apps sxswi con...                1\n",
       "102   quotapple open popup store austin nerd town ge...                1\n",
       "413             syd crew iphone extra juice pod procure                1\n",
       "441                 barry diller think ipad content nut                1\n",
       "488            give extreme temptation buy ipad impulse                1\n",
       "640                                     catch mean ipad                1\n",
       "705               forgot iphone android knife gun fight                1\n",
       "749   google lanzara ningun producto en south southwest                1\n",
       "882          comprando mi ipad en el apple store others                1\n",
       "1069                        say google get drunk hckers                1\n",
       "1087  ipad slow rest heartrate tapworthy gsdm leisur...                1\n",
       "1164         walk mobile apple store austin line insane                1\n",
       "1297  waze v google discussion one best see need com...                1\n",
       "1391  wow folk make sure iphone ringer turn way pres...                1\n",
       "1422  let see google announce circle obama austin do...                1\n",
       "1437  maybe entirely hd flip iphone xl footage certa...                1\n",
       "1545  platformer ci di venue pake app sq apid cnth t...                1\n",
       "1558  google quotcredit card co know accuracy yr go ...                1\n",
       "1575         sure check jukebox app run party music api                1\n",
       "1599  hijack iphone since mine doa muahahahahhaahaha...                1\n",
       "1716  id take hashtags scheen ipadipad tv sport twee...                1\n",
       "1908                   nonot google circle juwan howard                1\n",
       "1936  realize iphones gon na go nut foursquare gowal...                1\n",
       "1961  team android party cant find gowalla foursquar...                1\n",
       "2068       vai comear palestra exchief evangelist apple                1\n",
       "2095                              man get arrest iphone                1\n",
       "2116  iphone crash front apple popup bestworstthingever                1\n",
       "2144  mom might call big power plant cnn google offc...                1\n",
       "2263      blackberry apps pls let knowcheerswill iphone                1\n",
       "2318  ipadi alacagimiz yer belli oldu apple open pop...                1\n",
       "2334  saw bunch security guard play ipads apple stor...                1\n",
       "2448  agnerd confession use laptop ipad blackberry f...                1\n",
       "2472  im bum missed team android party oh well ctia ...                1\n",
       "2525  next designing interface ipad hit quotyour bra...                1\n",
       "2547  thank goodness two hour break turn iphone batt...                1\n",
       "2597                             temp store apple store                1\n",
       "2614  wish apple store employee cheer people line po...                1\n",
       "2617                  line buy ipad tomorrow notwinning                1\n",
       "2732  look like line apple popup store congress ave ...                1\n",
       "2739  good place test edgy enough google check offer...                1\n",
       "2774  hope there time leave poked liked retweeted go...                1\n",
       "2902                  trend time tsunamithe ipad though                1\n",
       "2954  uxdes glad standard ipad navigation tool might...                1\n",
       "2974  think control identity facebook google becomes...                1\n",
       "2986    peep awesome product video cant compete game cc                1\n",
       "3058     google place value domain extension equal qagb                1\n",
       "3128  report introduce new social medium platform pr...                1\n",
       "3145  google facebookkiller quotcirclesquot unveil t...                1\n",
       "3197  liveblog indie iphone game development survive...                1\n",
       "3221         walk around star iphone like everyone else                1\n",
       "3265  google might launch social network call circle...                1\n",
       "3298  quotgoogle product need condensedquot merissa ...                1\n",
       "3351  apple google intel others go gaga go game fast...                1\n",
       "3352  bet googlebuzzlike fail people care privacy el...                1\n",
       "3467  neither buying ipad today feel like geek cred ...                1\n",
       "3501  funny iphone correction dash learn thing two l...                1\n",
       "3539  diller would product ipad one form factor toda...                1\n",
       "3762  petricone say google tv browser think thats co...                1\n",
       "3921  agree concern microformats panelist condescend...                1\n",
       "3939                        would give apple stock like                1"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idk_df[idk_df['emotion_encoded'] == 1].head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
